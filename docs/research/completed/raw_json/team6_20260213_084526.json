{
  "created_at": 1770990047,
  "id": "resp_47306b79-1ff0-4d29-b6fa-e7cb0a916f2f",
  "model": "anthropic/claude-opus-4-6",
  "object": "response",
  "output": [
    {
      "queries": [
        "Fireworks AI Kimi K2 API streaming function calling",
        "Fireworks AI LoRA fine-tuning deployment workflow",
        "Fireworks AI multi-LoRA serving"
      ],
      "results": [
        {
          "date": "2026-02-11",
          "id": 1,
          "last_updated": "2026-02-12T06:34:01",
          "snippet": "## Common use cases\n\n### Streaming responsesStream responses token-by-token for a better user experience:\n\n```\n\nfrom fireworks import Fireworks\n\nclient = Fireworks()\n\nstream = client.chat.completions.create(\n\nmodel=\"accounts/fireworks/models/deepseek-v3p1\",\n\nmessages=[{\"role\": \"user\", \"content\": \"Tell me a short story\"}],\n\nstream=True\n\n\n\nfor chunk in stream:\n\nif chunk.choices[0].delta.content:\n\nprint(chunk.choices[0].delta.content, end=\"\")\n\n```\n\n### Function callingConnect your models to external tools and APIs:\n\nLearn more about function calling \u2192\n\n```\n\nfrom fireworks import Fireworks\n\nclient = Fireworks()\n\nresponse = client.chat.completions.create(\n\nmodel=\"accounts/fireworks/models/kimi-k2-instruct-0905\",\n\nmessages=[\n\n{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}\n\n],\n\ntools=[\n\n\n\n\"type\": \"function\",",
          "source": "web",
          "title": "Function calling",
          "url": "https://docs.fireworks.ai/getting-started/quickstart"
        },
        {
          "date": "2026-01-22",
          "id": 2,
          "last_updated": "2026-01-23T06:49:20.975138",
          "snippet": "## Single-LoRA deploymentDeploy your LoRA fine-tuned model with a single command that delivers performance matching the base model. This streamlined approach, called live merge, eliminates the previous two-step process and provides better performance compared to multi-LoRA deployments.\n\n### Quick deploymentDeploy your LoRA fine-tuned model with one simple command:\n\n```\n\nfirectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\n```\n\nYour deployment will be ready to use once it completes, with performance that matches the base model.\n\n## Multi-LoRA deploymentIf you have multiple fine-tuned versions of the same base model (e.g., you\u2019ve fine-tuned the same model for different use cases, applications, or prototyping), you can share a single base model deployment across these LoRA models to achieve higher utilization.\n\n### Deploy with CLI\n\n1\n\nCreate base model deployment\n\nDeploy the base model with addons enabled:\n\n```\n\nfirectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n```\n\n2\n\nLoad LoRA addons\n\nOnce the deployment is ready, load your LoRA models onto the deployment:\n\nYou can load multiple LoRA models onto the same deployment by repeating this command with different model IDs.\n\n```\n\nfirectl load-lora <FINE_TUNED_MODEL_ID> --deployment <DEPLOYMENT_ID>\n\n```",
          "source": "web",
          "title": "Deploying Fine Tuned Models - Fireworks AI Docs",
          "url": "https://docs.fireworks.ai/fine-tuning/deploying-loras"
        },
        {
          "date": "2024-09-18",
          "id": 3,
          "last_updated": "2026-02-12T20:51:08",
          "snippet": "**Multi-LoRA benefits:**\n\n**Multi-LoRA is part of FireOptimizer**, our adaptation engine designed to customize and enhance AI model performance for your unique use cases and workload. FireOptimizer capabilities include Adaptive Speculative Execution, that enables up to 3x latency improvements, Customizable Quantization, to precisely balance speed and quality, and LoRA Fine-Tuning to customize and improve model performance.\n\n\u2026\n\n**Multi-LoRA** takes LoRA to the next level, by allowing you to serve hundreds of fine-tuned LoRAs on a single base model, simultaneously, at the same inference cost as a single base model. No matter what your deployment shape or configuration, you\u2019ll pay the same price as the base model, whether your deployment serves just one or hundreds of fine-tuned LoRAs.\n\n\u2026\n\nMulti-LoRA offers an elegant solution: teams can create LoRA fine-tunes for each experiment, benefiting from the compute efficiency of utilizing the same base model, increased velocity in training and deploying LoRA fine-tunes, and easier onboarding and offboarding of experimental fine-tuned models. Companies can periodically \u2018merge\u2019 the most successful experiments by combining datasets and hyperparameter findings.\n**Inference prices are the same as a single base model, even when serving hundreds of fine-tuned models**\n\nMulti-LoRA makes it feasible to serve hundreds of fine-tuned models at the same cost as base model inference.\n\nWe believe this speed and cost efficiency unlocks personalization at scale. If you have many users, user segments or use cases to personalize for, deploying each fine-tuned model on a separate GPU would be prohibitively expensive. With Multi-LoRA, you can personalize AI at scale, without scaling your costs.\nMulti-LoRA is also competitive in throughput and speed: you can serve hundreds of fine-tuned models, at inference speeds and throughput that approach 90% of base model performance.\n\nScott Kramer, former Head of ML at Amplitude, helps businesses leverage their proprietary data to fine-tune and deploy models using Fireworks. Scott's clients find it easier and more cost-effective to deploy with Fireworks, compared to other platforms like AWS. Scott also teaches a fine-tuning course for developers, leveraging Fireworks as a key platform for learning.\n\n\u2026\n\nAt Fireworks, we offer our own LoRA tuning service via Firectl CLI, with a REST API coming soon. However, you can also tune LoRAs using any tool or platform of your choice, and upload and deploy your LoRAs on Fireworks. Follow our documentation to fine-tune as well as upload and deploy models on Fireworks.\nOnce tuned, there are two main deployment options on Fireworks:\n\nAnd for even faster inference at high volumes, you can merge a LoRA into the base model and deploy the merged models on our on-demand or enterprise reserved deployments.\n\nAt Fireworks, we\u2019ve built first class support for LoRA serving in our proprietary FireAttention inference stack. We optimized Multi-LoRA serving through\n**Cross-Model Continuous Batching**, which enables processing requests from multiple LoRAs simultaneously on the same base model and dynamically adjusts batch sizes to maximize GPU utilization. We also use **Dynamic Loading** of LoRA adapters with caching, to ensure that the number of supported LoRAs is not constrained by GPU memory and new models can be added or taken down within seconds.",
          "source": "web",
          "title": "Multi-LoRA: Personalize AI at scale and deliver the best ...",
          "url": "https://fireworks.ai/blog/multi-lora"
        },
        {
          "date": "2026-01-27",
          "id": 4,
          "last_updated": "2026-02-12T11:38:58",
          "snippet": "}]\n\nresponse = client.chat.completions.create(\n\nmodel=\"accounts/fireworks/models/kimi-k2-instruct-0905\",\n\nmessages=[{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}],\n\ntools=tools,\n\ntemperature=0.1\n\n\n\nprint(response.choices[0].message.tool_calls)\n\n# Output: [ChatCompletionMessageToolCall(id='call_abc123', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function')]\n\n```\n\n\u2026\n\nmodel=\"accounts/fireworks/models/kimi-k2-instruct-0905\",\n\nmessages=messages,\n\ntools=tools,\n\ntemperature=0.1\n\n\n\n# Step 3: Check if model wants to call a tool\n\nif response.choices[0].message.tool_calls:\n\n# Step 4: Execute the tool\n\ntool_call = response.choices[0].message.tool_calls[0]\n\n\u2026\n\n```\n\n# Force a specific tool\n\nresponse = client.chat.completions.create(\n\nmodel=\"accounts/fireworks/models/kimi-k2-instruct-0905\",\n\nmessages=[{\"role\": \"user\", \"content\": \"What's the weather?\"}],\n\ntools=tools,\n\ntool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}},\n\ntemperature=0.1\n\n\n\n```\n\nSome models support parallel tool calling, where multiple tools can be called in a single response. Check the model\u2019s capabilities before relying on this feature.\n\n## Streaming\n\nUsing tool calls with streaming responses\n\nUsing tool calls with streaming responses\n\nTool calls work with streaming responses. Arguments are sent incrementally as the model generates them:\n\n\u2026\n\n},\n\n\"required\": [\"city\"]\n\n\n\n}]\n\nstream = client.chat.completions.create(\n\nmodel=\"accounts/fireworks/models/kimi-k2-instruct-0905\",\n\nmessages=[{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}],\n\ntools=tools,\n\nstream=True,\n\ntemperature=0.1",
          "source": "web",
          "title": "Tool Calling - Fireworks AI Docs",
          "url": "https://docs.fireworks.ai/guides/function-calling"
        },
        {
          "date": "2025-12-05",
          "id": 5,
          "last_updated": "2026-02-12T21:58:59",
          "snippet": "LoRA significantly reduces the computational cost of fine-tuning large models by updating only a small subset of parameters in a low-rank structure, making it particularly suitable for large models like LLaMA or DeepSeek.\n\nqLoRA (Quantized LoRA) further improves efficiency by enabling fine-tuning of 4-bit and 8-bit quantized models (dependent on model types) without sacrificing performance, reducing memory requirements even more.\nFireworks AI supports both LoRA and qLoRA tuning, allowing up to 100 LoRA adaptations to run simultaneously on a dedicated deployment without extra cost.\n\n**Step-by-Step Guide to Fine-Tuning with Fireworks AI**\n\nGo to fireworks.ai > Model Library > Filter \u201cTunable\u201d\n\nYou can also filter for \u201cServerless\u201d models if you are planning to run it serverless\nThis ensures that you are selecting models which allow LoRA-based tuning and deployments. These models support uploading LoRA adapters even if they were trained on another platform. You can also upload custom models that have the same architecture as the tunable ones in the list, and those models will also be tunable. For example, a deepseek distilled llama 8b model works just as well as a vanilla llama 8b model.\n\n\u2026\n\nOnce you run the fine-tuning job, you will see the job details on the \u201cFine-Tuning\u201d page.\n\n\n\nNote: The maximum number of tokens packed into each training batch. (default 32768)\n\nOnce the fine-tuning job is completed. You will see \u201cDeploy the LoRA\u201d option at the top right.\nSelect the LoRA model from the drop-down list.\n\nClick \u201cContinue\u201d\n\nThen select your deployment. You can either deploy the LoRA model \u201cserverless\u201d or via an \u201con-demand deployment\u201d.\n\nDeployment Options:\n\n\ud83d\udca1 PS: Use an existing on-demand deployment to load the LoRA model onto (instead of serverless)\nPS: As we are fine-tuning the DeepSeek R1 Distill Llama 70B, we will need to deploy the LoRA on -demand, rather than serverless. Use an existing on-demand deployment to load the LoRA model onto (instead of serverless)\n\nEnter the model display name and click \u201cSubmit\u201d\nBy following these steps, you can effectively adapt LLMs to your specific use case using Fireworks AI\u2019s fine-tuning pipeline with LoRA. This approach ensures lower costs, faster training, and scalable deployment for real-world applications.",
          "source": "web",
          "title": "Supervised Fine-Tuning (SFT) with LoRA on Fireworks AI",
          "url": "https://fireworks.ai/blog/supervised-fine-tuning-tutorial"
        },
        {
          "id": 6,
          "last_updated": "2025-11-01T08:49:50.380260",
          "snippet": "",
          "source": "web",
          "title": "Using multi-LoRA - Fireworks AI Docs - Introduction",
          "url": "https://fireworksai-docs.mintlify.app/fine-tuning/multi-lora"
        },
        {
          "date": "2008-01-01",
          "id": 7,
          "last_updated": "2026-02-03T20:16:32.554586",
          "snippet": "## How Kimi API is structured\n\nThink of the Kimi API as three layers:\n\n### Layer A - Transport + Auth\n\n\n\nHTTPS requests\n\n\n\nJSON request/response\n\n\n\nAuthorization via API key (Bearer token in the\n\n`Authorization`header)\n\n### Layer B - Core endpoints\n\n\n\n`/v1/chat/completions`(the main one)\n\n\n\n`/v1/models`(discover model IDs)\n\n\n\n`/v1/files`(upload and reference documents)\n\n### Layer C - Product behaviors\n\n\n\nTool calling / function calling\n\n\n\nLong context + document workflows\n\n\n\nMultimodal support (model-dependent)\n\n\n\nRate limits and quotas that shape production behavior",
          "source": "web",
          "title": "Kimi API - Kimi K2.5 API",
          "url": "https://kimik2ai.com/api/"
        },
        {
          "date": "2025-05-12",
          "id": 8,
          "last_updated": "2026-02-12T12:47:05",
          "snippet": "You have now fine-tuned a 7TB\n{ts:130} parameter LLM using Laura on Fireworks. Once the job is complete, you'll see an option to deploy this LoRa. Then you can\n{ts:137} choose the Laura model, create an ondemand deployment, and get started.",
          "source": "web",
          "title": "Supervised Fine Tuning on Fireworks AI",
          "url": "https://www.youtube.com/watch?v=FpRd6eFmjjs"
        },
        {
          "date": "2025-01-22",
          "id": 9,
          "last_updated": "2026-02-10T22:48:55",
          "snippet": "",
          "source": "web",
          "title": "Introduction | Fireworks",
          "url": "https://fireworks.gitbook.io/fireworks/"
        },
        {
          "date": "2025-11-10",
          "id": 10,
          "last_updated": "2026-02-13T11:27:44",
          "snippet": "",
          "source": "web",
          "title": "Error Codes",
          "url": "https://kimi-k2.ai/api-docs"
        },
        {
          "date": "2025-04-25",
          "id": 11,
          "last_updated": "2025-04-26T02:08:55.409614",
          "snippet": "If you're new to Laura fine-tuning, let me tell you that it's a gamecher when it comes to fine-tuning large language models. Instead of updating all the model parameters, it trains small low rank matrices making it cheaper and faster. Fireworks supports up to 100 Lora adapters per base model on dedicated deployments which means you can scale with zero added deployment cost.",
          "source": "web",
          "title": "Step-by-Step Tutorial for Supervised Fine Tuning (LoRA) on Fireworks AI",
          "url": "https://www.youtube.com/watch?v=xTYLEtkF4AI"
        },
        {
          "date": "2010-01-01",
          "id": 12,
          "last_updated": "2025-11-12T06:47:07.686167",
          "snippet": "",
          "source": "web",
          "title": "Load LoRA - Fireworks AI Docs",
          "url": "https://fireworksai-docs.mintlify.app/tools-sdks/firectl/commands/load-lora"
        },
        {
          "date": "2026-01-06",
          "id": 13,
          "last_updated": "2026-02-12T03:08:59",
          "snippet": "### Minimal call flow (high level)\n\n- Build a chat request (system + user messages).\n\n- Optionally include\n\n`tools`(a JSON array describing functions) to enable the model to autonomously call them.\n\n- Send the request to the chat/completions endpoint with model set to the K2 Thinking variant.\n\n- Stream and/or collect response chunks and assemble both\n\n`reasoning_content`and final\n\n`content`.\n\n- When the model requests a tool call, execute the tool on your side, return the result as a follow-up message (or via the provider\u2019s function-return protocol) and let the model continue.\n\n\u2026\n\n**Streaming:**enable streaming (\n\n`stream=True`) to render both reasoning and final content progressively.\n\n**Tool schema:**include a\n\n`tools`/\n\n`functions`array describing available functions; K2 will decide autonomously when to call them. Provide clear\n\n`description`and strict JSON schemas for arguments to avoid ambiguous calls.",
          "source": "web",
          "title": "How to Use Kimi K2 Thinking API\u2014 a practical guide - CometAPI",
          "url": "https://www.cometapi.com/how-to-use-kimi-k2-thinking-api-a-practical-guide/"
        },
        {
          "id": 14,
          "last_updated": "2025-11-28T22:07:08.643572",
          "snippet": "",
          "source": "web",
          "title": "Single-LoRA deployment with live merge - Fireworks AI Docs",
          "url": "https://fireworksai-docs.mintlify.app/fine-tuning/single-lora"
        },
        {
          "date": "2024-08-07",
          "id": 15,
          "last_updated": "2025-02-06T09:30:31.144605",
          "snippet": "More specifically, Multi-LoRA has benefits below: -- Fine-tune and serve hundreds of personalized LoRA models at the same cost as a single base model, which is just $0.2/1M tokens for Llama3.1 8B -- 100x cost-efficiency compared to serving 100\n\n\u2026\n\n\u26a1Cresta uses Multi-LoRA to personalize their Knowledge Assist feature for each individual customer on the Fireworks enterprise platform. \"Fireworks' Multi-LoRA capabilities align with Cresta's strategy to deploy custom AI through fine-tuning cutting-edge base models. It helps unleash the potential of AI on private enterprise data.\" - Tim Shi, Co-Founder and CTO of Cresta \u26a1Brainiac Labs helps businesses leverage their proprietary data to fine-tune and deploy models using Multi-LoRA on the Fireworks self-serve platform.",
          "source": "web",
          "title": "Fireworks AI | LinkedIn",
          "url": "https://kadmea.com/lander/kadmea.com/index.php?_=%2Fcompany%2Ffireworks-ai%236P%2BquF5Tp0W5nIfDc5Gz4tTDaIwBoIm2"
        }
      ],
      "type": "search_results"
    },
    {
      "queries": [
        "Supabase Edge Functions embedding pipeline RAG",
        "Supabase RPC similarity search pgvector",
        "Supabase Row Level Security multi-tenant RAG"
      ],
      "results": [
        {
          "date": "2024-05-22",
          "id": 16,
          "last_updated": "2025-07-12T16:50:19.391636",
          "snippet": "# LangChain in Supabase Edge Functions\n\nInstalling NPM Packages in Supabase Edge Functions\nIf you're familiar with Supabase Edge Functions, you may know that installing npm packages is not as straightforward as in a typical Node.js or Express application. However, there are several methods to include npm packages in Supabase Edge Functions. In this guide, I'll demonstrate how to use the import_map.json file to achieve this.\n\n\u2026\n\n### Step 3: Create a Supabase Edge Function\n\nOpen the directory in your favorite code editor and run the following command to create a new Supabase Edge Function:\n\n```\nnpx supabase functions new <your-function-name>\n```\n\nThis will create a folder with the name of your function inside the\n```\nfunctions\n```\ndirectory.\n\n\u2026\n\n```\n{\n  \"imports\": {\n    \"@supabase/supabase-js\": \"npm:@supabase/supabase-js@2.43.1\",\n    \"langchain/\": \"https://esm.sh/langchain@0.2.0/\",\n    \"@langchain/openai/\": \"https://esm.sh/@langchain/openai@0.0.28/\",\n    \"@langchain/core/\": \"https://esm.sh/@langchain/core@0.1.62/\",\n    \"@langchain/community/\": \"https://esm.sh/@langchain/community@0.0.56/\"\n  }\n}\n```\n\n\u2026\n\n## Deploy Your Edge Function to Production\n\nEnsure your project is linked with your Supabase cloud project.\n\nTo deploy your edge function to Supabase, run the following command:\n\n```\nnpx supabase functions deploy\n```\n\nThis command will deploy all functions.\n\nTo deploy a specific function, use:\n\n```\nnpx supabase functions deploy <function-name>\n```",
          "source": "web",
          "title": "GitHub - GreatHayat/supabase-edge-functions-rag: This is a repository to let you know the implementation of a basic RAG pipeline using LangChain in Supabase Edge Functions.",
          "url": "https://github.com/GreatHayat/supabase-edge-functions-rag"
        },
        {
          "date": "2026-02-13",
          "id": 17,
          "last_updated": "2026-02-13T13:32:04",
          "snippet": "",
          "source": "web",
          "title": "Storing a vector / embedding",
          "url": "https://supabase.com/docs/guides/database/extensions/pgvector"
        },
        {
          "date": "2026-02-12",
          "id": 18,
          "last_updated": "2026-02-13T00:29:50",
          "snippet": "",
          "source": "web",
          "title": "Row Level Security | Supabase Docs",
          "url": "https://supabase.com/docs/guides/database/postgres/row-level-security"
        },
        {
          "date": "2026-02-13",
          "id": 19,
          "last_updated": "2026-02-13T11:38:07",
          "snippet": "",
          "source": "web",
          "title": "Edge Functions | Supabase Docssupabase.com \u203a docs \u203a guides \u203a functions",
          "url": "https://supabase.com/docs/guides/functions"
        },
        {
          "date": "2026-02-12",
          "id": 20,
          "last_updated": "2026-02-13T07:12:03",
          "snippet": "",
          "source": "web",
          "title": "Similarity Metric",
          "url": "https://supabase.com/docs/guides/ai/semantic-search"
        },
        {
          "date": "2025-01-24",
          "id": 21,
          "last_updated": "2025-10-26T08:43:43.228838",
          "snippet": "Here\u2019s an example of how you can set this up:\n\n1. **Assume you have a\n   ```\n   profiles\n   ```\n   table** that stores the\n   ```\n   tenant_id\n   ```\n   for each user. You can create a policy that joins the\n   ```\n   profiles\n   ```\n   table to enforce the restriction:\n\n\u2026\n\n```\nALTER TABLE my_table ENABLE ROW LEVEL SECURITY;\n```\n\nFinally, test your setup by querying the table while authenticated as a user with a specific\n```\ntenant_id\n```\n. You should only see rows that match their\n```\ntenant_id\n```\n. Let me know if this helps or if you run into any other issues\ud83d\ude0a\n\n- \u2764\ufe0f 1\n\n1 reply\n\n- -",
          "source": "web",
          "title": "row-level security policies in Supabase for a multitenant ...",
          "url": "https://github.com/orgs/community/discussions/149922"
        },
        {
          "date": "2024-04-16",
          "id": 22,
          "last_updated": "2025-09-03T08:02:06.567227",
          "snippet": "We're making it super easy to run AI models within Supabase Edge Functions. A new built-in API is available within the Edge Runtime to run inference workloads in just a few lines of code:\n\n_10// Instantiate a new inference session_10const session = new Supabase.ai.Session('gte-small')_10_10// then use the session to run inference on a prompt_10const output = await session.run('Luke, I am your father')_10_10console.log(output)_10// [ -0.047715719789266586, -0.006132732145488262, ...]\n\n\u2026\n\nTo solve this, we built a native extension in Edge Runtime that enables using ONNX runtime via the Rust interface. This was made possible thanks to an excellent Rust wrapper called Ort:\n\nEmbedding generation is fairly lightweight compared to LLM workloads, so it can run on a CPU without hardware acceleration.\n\n### Availability: open source embeddings\n\nEmbeddings models are available on Edge Functions today. We currently support\n\n`gte-small` and we'll add more embeddings models based on user feedback.\n\nEmbedding generation via\n\n`Supabase.ai` API is available today for all Edge Functions users in both local, hosted, and self-hosted platforms.",
          "source": "web",
          "title": "Generating Text Embeddings...",
          "url": "https://supabase.com/blog/ai-inference-now-available-in-supabase-edge-functions"
        },
        {
          "date": "2023-09-04",
          "id": 23,
          "last_updated": "2026-02-13T09:53:02",
          "snippet": "## Building a pgvector similarity search\n\nI built all the code you're about to see using markdown programming with ChatGPT. You could optimize parts of the code by hand, but it's fine for the amount of data we've got.\n\nThe technique works in 3 steps:\n\n- Prep the Supabase DB for pgvector\n\n- Compute and store embeddings for every attendee's responses\n\n- Find similar attendees with a vector similarity search\n\nAll based on OpenAI embeddings. It cost me about $1 to run this for a few thousand rows of data.\n\n\u2026\n\n### Compute and store embeddings for every attendee\n\nWe'll need embedding vectors for each attendee to run similarity queries. Working with the assumption that giving similar answers to survey questions means you have similar interests.\n\nThe code first connects to the DB and inits an OpenAI client.\n\n```\n\nconst { Client } = require(\"pg\")\n\nconst OpenAI = require(\"openai\")\n\nconst db = new Client({\n\nconnectionString: process.env.SUPABASE_URL,\n\n})\n\nconst openai = new OpenAI(process.env.OPENAI_API_KEY)\n\nawait db.connect()\n\n```",
          "source": "web",
          "title": "Similarity search with pgvector and Supabase - Swizec Teller",
          "url": "https://swizec.com/blog/similarity-search-with-pgvector-and-supabase/"
        },
        {
          "date": "2026-02-13",
          "id": 24,
          "last_updated": "2026-02-13T13:28:59",
          "snippet": "",
          "source": "web",
          "title": "Documents owned by multiple...",
          "url": "https://supabase.com/docs/guides/ai/rag-with-permissions"
        },
        {
          "date": "2025-04-03",
          "id": 25,
          "last_updated": "2025-05-21T02:48:57.793224",
          "snippet": "# Supabase\n\nHow to run local embeddings model (other than Supabase/gte-small) on Edge Functions #34688\n\nlsli8888\n\nApr 3, 2025 \u00b7 0 comments\n\nReturn to top\n\n-\n-\n\n## lsli8888 Apr 3, 2025\n\nOriginal comment in English -\n\nSupabase has some nice documentation on how to generate embeddings using the Supabase/gte-small embedding model on Edge Functions. I easily got this to work by following the documentation. However, the gte-small embedding model doesn't perform well for me and I want to use another embedding model. I don't want to invoke a service that helps generate the embeddings as I want to do this locally on Supabase Edge Functions for performance and cost efficiency. Unfortunately, Supabase doesn't support anything but gte-small out of the box.\n\nFor example, I want to use all-MiniLM-L6-v2 as the embedding model. How can I code for this in my Edge Function?\n\nI tried the following code and deployed it to Supabase Edge Function, but I came back with a litany of runtime errors. I'm quite new to all this, especially Deno and transformers.js, so I may be doing things completely wrong:\n\n```\nimport { pipeline, env } from 'https://esm.sh@xenova/transformers@2.17.2';\nimport { split } from 'https://esm.sh/sentence-splitter@5.0.0';\n\n// Disable local model caching for edge functions\nenv.allowLocalModels = false;\n// env.backends.onnx.wasm.numThreads = 1;\n\n// Create a new pipeline for feature extraction\nlet extractor = await pipeline('feature-extraction', 'sentence-transformers/all-MiniLM-L6-v2');\n\nDeno.serve(async (req) => {\n  try {\n    // Extract input string from JSON body\n    const { input } = await req.json();\n\n    if (!input || typeof input !== 'string') {\n      return new Response(\n        JSON.stringify({ error: 'Invalid input. Expected a non-empty string.' }),\n        {\n          status: 400,\n          headers: { 'Content-Type': 'application/json' },\n        }\n      );\n    }\n\n    // Split the input text into sentences, normalize them, and filter out empty sentences\n    const sentences = split(input)\n      .map((sentence) => sentence.raw.trim().toLowerCase()) // Trim spaces and convert to lowercase\n      .filter((sentence) => sentence.length > 0); // Remove empty sentences\n\n    if (sentences.length === 0) {\n      return new Response(JSON.stringify({ error: 'No valid sentences found in input.' }), {\n        status: 400,\n        headers: { 'Content-Type': 'application/json' },\n      });\n    }\n\n    // Generate embeddings for each sentence\n    const output = await extractor(sentences, {\n      pooling: 'mean',\n      normalize: true,\n    });\n\n    const embeddings: any = Array.from(output.data);\n\n    // Calculate the mean embedding if there are multiple sentences\n    const meanEmbedding =\n      embeddings.length > 0\n        ? embeddings[0].map(\n            (_, i) =>\n              embeddings.reduce((sum, embedding) => sum + embedding[i], 0) / embeddings.length\n          )\n        : [];\n\n    // Return the sentences and the mean embedding\n    return new Response(JSON.stringify({ sentences, meanEmbedding }), {\n      headers: { 'Content-Type': 'application/json' },\n    });\n  } catch (error) {\n    return new Response(\n      JSON.stringify({ error: 'Unexpected error occurred.', details: error.message }),\n      { status: 500, headers: { 'Content-Type': 'application/json' } }\n    );\n  }\n});\n\n```\n\nRuntime errors include:\n\n- Failed to load model because protobuf parsing failed.\n- event loop error: Error: Can't create a session\n- Unable to add response to browser cache: Error: Web Cache is not available in this context.\n- Unable to determine content-length from response headers. Will expand buffer when needed.\n\nBeta Was this translation helpful? Give feedback.\n\nAll reactions\n\n## Replies: 0 comments\n\nSign up for free **to join this conversation on GitHub**. Already have an account? Sign in to comment\n\nCategory\n\nQuestions\n\nLabels\n\nNone yet\n\n1 participant",
          "source": "web",
          "title": "How to run local embeddings model (other than Supabase/gte-small) on Edge Functions \u00b7 supabase \u00b7 Discussion #34688",
          "url": "https://github.com/orgs/supabase/discussions/34688"
        },
        {
          "id": 26,
          "last_updated": "2026-02-03T06:29:59.209223",
          "snippet": "**Key Points:**\n\n`<=>`is pgvector's\n\n**cosine distance**operator\n\n- Distance ranges from 0 (identical) to 2 (opposite)\n\n- We convert to similarity with\n\n`1 - distance`\n\n- Results are ordered by distance (ascending = most similar first)\n\n\u2026\n\n## Summary\n\nIn this lesson, we built the search functionality that powers our RAG system:\n\n**Key Takeaways:**\n\n\n\n**RPC functions encapsulate search logic**: Cleaner code, better performance, atomic updates\n\n\n\n**pgvector's**: Convert distance to similarity with\n\n`<=>`operator enables cosine similarity\n\n`1 - distance`\n\n\n\n**Filtering improves relevance**: Source filters and similarity thresholds reduce noise\n\n\n\n**Multi-tenant search requires explicit user filtering**: Use\n\n`SECURITY DEFINER`with care\n\n\n\n**Test your search thoroughly**: Verify results, thresholds, and filters work as expected",
          "source": "web",
          "title": "Supabase Search Functionality (Postgres RPC)",
          "url": "https://freeacademy.ai/lessons/supabase-search-functionality"
        },
        {
          "date": "2024-12-23",
          "id": 27,
          "last_updated": "2026-01-31T13:14:47.175071",
          "snippet": "**Supabase**, which provided them with a fully managed database powered by PostgreSQL.They wanted to implement a multi-tenant architecture but were concerned about data security and isolation. The challenge was clear: how could they ensure strict data separation between tenants without the complexity and cost of maintaining separate databases for each one? This is where Row-Level Security (RLS) came into play, offering a way to solve the problem by providing fine-grained access control at the database level, making multi-tenancy secure, efficient, and scalable.\n\n\u2026\n\n**Fine-Grained Access Control:**RLS restricts database access at the row level, ensuring that users can only access specific rows that they are authorized to view based on predefined policies. This enhances data security by preventing unauthorized data access within the same table. **Security:**Rather than relying on application logic or external systems, RLS centralizes the security control within the database, making it easier to manage and enforce data access permissions.\n\n\u2026\n\n## Key Takeaways\n\nRow-Level Security (RLS) is a crucial component for ensuring secure and efficient multi-tenancy in modern applications. By enforcing fine-grained access control directly at the database level, RLS ensures that each tenant\u2019s data is isolated while sharing the same infrastructure. It simplifies security management, reduces the risk of unauthorized access, and improves scalability. Despite some challenges, such as performance overhead and policy complexity, RLS remains an essential tool for organizations looking to build secure, cost-effective, and scalable multi-tenant systems.",
          "source": "web",
          "title": "Multi-Tenant Applications with RLS on Supabase (Postgress)",
          "url": "https://www.antstack.com/blog/multi-tenant-applications-with-rls-on-supabase-postgress/"
        },
        {
          "id": 28,
          "last_updated": "2025-12-07T21:55:09.875030",
          "snippet": "",
          "source": "web",
          "title": "Edge Functions | Supabase",
          "url": "https://supabase.gofornextlevel.com/project/default/functions"
        },
        {
          "date": "2025-03-15",
          "id": 29,
          "last_updated": "2026-02-03T03:46:40.778124",
          "snippet": "",
          "source": "web",
          "title": "Generate OpenAI embeddings",
          "url": "https://cookbook.openai.com/examples/vector_databases/supabase/semantic-search"
        },
        {
          "date": "2025-10-17",
          "id": 30,
          "last_updated": "2026-02-13T04:58:33",
          "snippet": "## Supabase RLS Implementation Strategy\n\n### 1. Architecture Overview\n\nOur RLS implementation follows a hierarchical security model:\n\n```\nOrganization (Top Level)\n\u251c\u2500\u2500 Members (Role-based access)\n\u251c\u2500\u2500 Projects (Org-scoped)\n\u251c\u2500\u2500 Environments (Project-scoped)\n\u251c\u2500\u2500 Secrets (Environment-scoped)\n\u2514\u2500\u2500 Audit Logs (Org-scoped)\n\n```\n\n\u2026\n\n```\n-- Step 1: Enable RLS without policies (allows all access temporarily)\nALTER TABLE public.organization ENABLE ROW LEVEL SECURITY;\n\n-- Step 2: Create policies in a separate transaction\nBEGIN;\nCREATE POLICY \"test_policy\" ON public.organization FOR SELECT TO authenticated USING (true);\nCOMMIT;\n\n-- Step 3: Test thoroughly, then replace with restrictive policies\nBEGIN;\nDROP POLICY \"test_policy\" ON public.organization;\nCREATE POLICY \"restrictive_policy\" ON public.organization\n  FOR SELECT TO authenticated\n  USING (/* actual restriction logic */);\nCOMMIT;\n\n```",
          "source": "web",
          "title": "Enforcing Row Level Security in Supabase: A Deep Dive into ...",
          "url": "https://dev.to/blackie360/-enforcing-row-level-security-in-supabase-a-deep-dive-into-lockins-multi-tenant-architecture-4hd2"
        }
      ],
      "type": "search_results"
    },
    {
      "queries": [
        "PGVector HNSW vs IVFFlat index performance 2024",
        "text-embedding-3-small dimensions optimal RAG",
        "PGVector hybrid search keyword semantic filtering"
      ],
      "results": [
        {
          "date": "2024-04-24",
          "id": 31,
          "last_updated": "2026-02-10T04:40:14",
          "snippet": "The choice of index involves considering several factors, including the data type, dataset size, and the balance between search speed and accuracy.\n\nThe flat index provides a straightforward approach, with a focus on accuracy, making it suitable for smaller datasets or situations where we value precision over everything. On the other hand, the HNSW and IVFFlat indexes introduce approximations to improve search speed, offering solutions for larger datasets.\n\n\u2026\n\n**memory-intensive**. The benefit is that the index is **data agnostic**.\n\nLastly, the IVFFlat (Inverted File Flat) index, although slower and less accurate than HNSW, is more scalable due to its\n\n**low memory usage** and **fast build time**. It reduces the search scope through clustering, which makes it non-data-agnostic.",
          "source": "web",
          "title": "Choosing your Index with PGVector | PIXION Blog",
          "url": "https://pixion.co/blog/choosing-your-index-with-pg-vector-flat-vs-hnsw-vs-ivfflat"
        },
        {
          "id": 32,
          "last_updated": "2026-02-13T10:10:49",
          "snippet": "",
          "source": "web",
          "title": "Evaluating Open-Source vs. OpenAI Embeddings for RAG",
          "url": "https://www.tigerdata.com/blog/open-source-vs-openai-embeddings-for-rag"
        },
        {
          "id": 33,
          "last_updated": "2026-02-12T19:18:54",
          "snippet": "Therefore, the new approach is to combine full-text search for precise retrieval with semantic search for higher relevance. This paradigm is known as **hybrid search**, and it offers the best of both worlds. In this article, we'll explain how to implement hybrid search in PostgreSQL using **Cohere**, **pgvector**, and **pgai** (and a few more Timescale innovations). This will allow you to build a fast, relevant, and accurate search system using your existing PostgreSQL database.\n\n\u2026\n\n## Implementing Hybrid Search\n\nWe\u2019ll combine the results of both search functions to implement hybrid search. To sort the results by relevance, we\u2019ll use a Cohere reranker model, rerank-english-v3.0.\u00a0\n\nReranker models reorder the results by calculating a relevance score between the query and the data objects, then sorting them from most to least relevant. Since reranking is computationally expensive, we typically use it at the final stage after hybrid search has retrieved an initial set of results.\n\n\u2026\n\n## Next Steps\n\nIn this article, we discussed full-text search and semantic search and explained how they differ. We demonstrated how to combine them to implement a hybrid search that leverages the benefits of both approaches to deliver more precise and contextually relevant results.\n\nIn our implementation, we used:\n\n-   **Cohere** for the embedding model and the reranker.\n-   **pgai and pgvector** for augmenting PostgreSQL with vector handling and semantic search capabilities.",
          "source": "web",
          "title": "Implementing Keyword Search",
          "url": "https://www.tigerdata.com/blog/combining-semantic-search-and-full-text-search-in-postgresql-with-cohere-pgvector-and-pgai"
        },
        {
          "date": "2024-03-15",
          "id": 34,
          "last_updated": "2026-02-13T05:26:04",
          "snippet": "",
          "source": "web",
          "title": "Implementation And...",
          "url": "https://aws.amazon.com/blogs/database/optimize-generative-ai-applications-with-pgvector-indexing-a-deep-dive-into-ivfflat-and-hnsw-techniques/"
        },
        {
          "date": "2024-06-04",
          "id": 35,
          "last_updated": "2026-02-11T19:26:26",
          "snippet": "",
          "source": "web",
          "title": "Step-by-Step Guide to Choosing the Best Embedding Model for ...",
          "url": "https://weaviate.io/blog/how-to-choose-an-embedding-model"
        },
        {
          "date": "2025-12-23",
          "id": 36,
          "last_updated": "2026-02-13T12:36:31",
          "snippet": "|--|--|\nEach method has its strengths: full-text finds exact keyword matches, while vector search finds semantically similar results even without exact phrasing.\n\n**Performing hybrid search**\n\nTo combine results, use a two-part query: one subquery performs vector search, the other full-text. Then aggregate and re-rank using Reciprocal Rank Fusion (RRF):",
          "source": "web",
          "title": "pgvector Hybrid Search: Benefits, Use Cases & Quick Tutorial",
          "url": "https://www.instaclustr.com/education/vector-database/pgvector-hybrid-search-benefits-use-cases-and-quick-tutorial/"
        },
        {
          "date": "2025-02-25",
          "id": 37,
          "last_updated": "2025-10-03T21:51:44.101698",
          "snippet": "",
          "source": "web",
          "title": "A Guide to pgvector, IVFFlat, and HNSW",
          "url": "https://dev.to/cubesoft/vector-search-demystified-a-guide-to-pgvector-ivfflat-and-hnsw-36hf"
        },
        {
          "date": "2025-12-22",
          "id": 38,
          "last_updated": "2025-12-23T03:18:59.885655",
          "snippet": "OpenAI's text-embedding-3 models introduced a useful capability: native dimension reduction. You can generate 3072-dimensional embeddings but instruct the API to return only the first 256, 512, 1024, or 1536 dimensions. This Matryoshka-style approach preserves semantic ordering in early dimensions, so truncated vectors retain most of the full model's accuracy.\n|Model|Dimensions|Use Case Fit|\n|--|--|--|\n|all-MiniLM-L6-v2|384|Cost-sensitive, general search|\n|text-embedding-3-small|1536|Balanced accuracy and cost|\n|text-embedding-3-large|3072|Maximum semantic resolution|\n|embed-english-v3.0|1024|Search-optimized retrieval|\n|BGE-large|1024|Open-source, high accuracy|\n|Voyage-2|1024|RAG-specific optimization|",
          "source": "web",
          "title": "How Many Dimensions Should Your Embeddings Have?",
          "url": "https://particula.tech/blog/embedding-dimensions-rag-vector-search"
        },
        {
          "id": 39,
          "last_updated": "2026-02-13T08:02:04",
          "snippet": "In this article, we'll combine **keyword** and [**semantic search**](https://www.tigerdata.com/learn/vector-search-vs-semantic-search) to achieve better search results. Keyword search matches words directly or to synonyms, while semantic search seeks to match the meaning behind the words in the query. We'll leverage **Cohere** and **pgvector** to implement hybrid search on PostgreSQL and see how they make our work easier.\n\n\u2026\n\n-   **Precision:** Keyword search enables exact matches to the query, leaving no room for ambiguity.\u00a0\n-   **Context:** Semantic search allows algorithms to understand the query's intent. If no keywords are matched, semantic search analyzes the context and meaning behind the query, ensuring that relevant results are still provided and covering any gaps in keyword-based matching.\n-   **Relevance:** Together, both techniques improve result relevance for unseen queries.\n|  | Precision | Context | Relevance |\n| Keyword | Matches words exactly | Ignores context | Results contain exact phrase |\n| Semantic | Matches context and meaning | Favors context over keyword | Results contain similar meanings |\n| Hybrid | Ranks word matches by context relevance | Incorporates context as a factor | Results contain exact phrases and relevant meanings |\n\n\u2026\n\n**2.\u00a0Embedding generation**:\n\na. **Semantic search using Cohere**: Documents are processed using Cohere to create dense embeddings, which capture the semantic meaning of the text.\n\nb. **Keyword search using PostgreSQL**: Documents are processed using PostgreSQL to create sparse embeddings focusing on the text's specific keywords.\n\n\u2026\n\ncursor.execute(sql, params)\nconn.commit()\n```\n\nNow, let\u2019s write our functions for searching the articles.\u00a0\n\n## Functions for Keyword Search and Semantic Search\n\nThis section will discuss and implement the core functions of our hybrid search engine, keyword, and semantic search functions.\n\n### Keyword search\n\nText search, or full-text searching, enables the identification of natural-language documents matching a query and potentially sorting them by relevance. Traditional database textual search operators like `~`, `~*`, `LIKE` and `ILIKE` lack crucial properties needed for modern information systems:",
          "source": "web",
          "title": "Implementing Hybrid Search...",
          "url": "https://www.tigerdata.com/blog/postgresql-hybrid-search-using-pgvector-and-cohere"
        },
        {
          "date": "2026-01-03",
          "id": 40,
          "last_updated": "2026-02-04T16:41:44.203817",
          "snippet": "Learn how vector indexes dramatically improve semantic search speed, what trade-offs exist between IVFFlat and HNSW, and how to tune your PostgreSQL setup for AI-scale workloads.\n\n\u2699\ufe0f Topics Covered:\n- PGVector performance overview\n- Building IVFFlat and HNSW indexes\n- Running query-time benchmarks\n- Choosing the right index for your use case\n-Scaling PostgreSQL for vector search\n\n\u2026\n\n{ts:964} faster  compared  to  HNSW.  But  I  would \n recommend  if  you  are  trying  both  first\n{ts:969} \n and  select  than  one  for  your  scenarios. \n\n{ts:972} Now  before  I  started  the  tutorial,  so \n initially  I  tried  this  graph  with  uh\n{ts:977} \n 100k  entries  also.  So  currently  we  have\n\n\u2026\n\n{ts:1006} section  we  are  comparing  both  the \n indexes  IVF  flat  and  HNSW  the  pros  and\n{ts:1011} \n cons  cons  of  both.  So  you  can  decide \n\n{ts:1014} which  one  to  use.  So  algorithm  wise  the \n IVF  flat  is  based  on  clustering  as  I\n{ts:1019} \n said  and  HNSW  is  based  on  graph.  speed.",
          "source": "web",
          "title": "Speed vs Accuracy - IVFFlat vs HNSW Benchmark in PGVector (PostgreSQL) | Python Tutorial",
          "url": "https://www.youtube.com/watch?v=UV0R6yG42I4"
        },
        {
          "date": "2025-11-03",
          "id": 41,
          "last_updated": "2025-11-20T14:00:52.414196",
          "snippet": "# text-embedding-3-small: High-Quality Embeddings at Scale\n\nOpenAI pulled off an impressive feat: they made embeddings both better AND 5\u00d7 cheaper, a model that outperforms its predecessor by 13% while costing just $0.02 per million tokens. This breakthrough, known as text-embedding-3-small, transforms text into 1536-dimensional vectors for semantic search, clustering, and RAG applications, an exponential increase in cost-efficiency for production AI systems that makes high-quality embeddings accessible at scale.\n\n\u2026\n\n## The Leap from ada-002 to 3-small\n\nMoving from\n\n__ada-002__ to text-embedding-3-small introduced a new economic reality for embeddings:\n\n- On multilingual retrieval tasks\n\n**(** **), performance jumped from 31.4% to 44.0%, a remarkable 13-point gain that makes the model significantly more effective for global applications** __MIRACL benchmark__\n\n\u2026\n\nThis flexibility offers practical benefits for real-world applications. Need faster similarity searches? Request 512-dimensional embeddings instead of the full 1536. Running low on vector database storage? Trim to 1024 dimensions while maintaining competitive accuracy. The remarkable aspect is that even a trimmed 1024D embedding from the large model beats ada-002's full output, demonstrating the superiority of the new architecture. This feature enables developers to dynamically balance accuracy, speed, and storage costs based on their specific use case requirements.\n\n\u2026\n\nFor most teams building semantic search or RAG systems, the decision is straightforward: text-embedding-3-small is the new baseline. Unless you're operating in highly specialized domains or need absolute maximum accuracy regardless of cost, this model offers the rare combination of performance, flexibility, and economics that makes it the obvious starting point. The next time someone claims AI advancement means escalating costs, point them here this is what democratization looks like.",
          "source": "web",
          "title": "text-embedding-3-small: High-Quality Embeddings at Scale",
          "url": "https://blog.promptlayer.com/text-embedding-3-small-high-quality-embeddings-at-scale/"
        },
        {
          "date": "2025-12-11",
          "id": 42,
          "last_updated": "2026-02-13T10:38:18",
          "snippet": "## Combining Semantic and Traditional Search\n\nVector search plays nicely with regular SQL filtering and even full-text search. Often, you want to find results that match certain metadata or keywords\n\n*and* are semantically relevant. This hybrid approach delivers very precise results.\n\nFor example, suppose you\u2019re searching for recent sci-fi articles about space. You might write:",
          "source": "web",
          "title": "Practical Use Cases of...",
          "url": "https://www.red-gate.com/simple-talk/databases/postgresql/how-to-build-an-ai-powered-semantic-search-in-postgresql-with-pgvector/"
        },
        {
          "date": "2023-12-01",
          "id": 43,
          "last_updated": "2025-10-08T03:50:21.876008",
          "snippet": "",
          "source": "web",
          "title": "Faster similarity search performance with pgvector indexes",
          "url": "https://cloud.google.com/blog/products/databases/faster-similarity-search-performance-with-pgvector-indexes"
        },
        {
          "date": "2025-10-16",
          "id": 44,
          "last_updated": "2026-02-13T11:56:16",
          "snippet": "Example:\n\n- OpenAI\u2019s text-embedding-3-large (3072 dims) offers excellent accuracy but higher storage overhead.\n\n- BGE-base (768 dims) or E5-small (384 dims) deliver good trade-offs for enterprise-scale deployments.\n\nYour choice should align with your infrastructure\u2019s capacity and your precision requirements.\n\n\u2026\n\n### OpenAI Text-Embedding 3 Series\n\n**Type:** Proprietary API **Best for: **Production-ready RAG with stable infrastructure\n\nThe text-embedding-3-small and text-embedding-3-large models provide state-of-the-art performance for English and multilingual retrieval. Despite being closed-source, their reliability, scalability, and consistent latency make them one of the most popular choices in production systems.",
          "source": "web",
          "title": "5 Best Embedding Models for RAG: How to Choose the Right One",
          "url": "https://greennode.ai/blog/best-embedding-models-for-rag"
        },
        {
          "date": "2026-02-13",
          "id": 45,
          "last_updated": "2026-02-13T13:30:06",
          "snippet": "",
          "source": "web",
          "title": "When to consider hybrid search",
          "url": "https://supabase.com/docs/guides/ai/hybrid-search"
        }
      ],
      "type": "search_results"
    },
    {
      "contents": [
        {
          "snippet": "Hybrid search\n\nCombine keyword search with semantic search.\n\nHybrid search combines full text search (searching by keyword) with semantic search (searching by meaning) to identify results that are both directly and contextually relevant to the user's query.\n\n## Use cases for hybrid search\n\nSometimes a single search method doesn't quite capture what a user is really looking for. For example, if a user searches for \"Italian recipes with tomato sauce\" on a cooking app, a keyword search would pull up recipes that specifically mention \"Italian,\" \"recipes,\" and \"tomato sauce\" in the text. However, it might miss out on dishes that are quintessentially Italian and use tomato sauce but don't explicitly label themselves with these words, or use variations like \"pasta sauce\" or \"marinara.\" On the other hand, a semantic search might understand the culinary context and find recipes that match the intent, such as a traditional \"Spaghetti Marinara,\" even if they don't match the exact keyword phrase. However, it could also suggest recipes that are contextually related but not what the user is looking for, like a \"Mexican salsa\" recipe, because it understands the context to be broadly about tomato-based sauces.\n\nHybrid search combines the strengths of both these methods. It would ensure that recipes explicitly mentioning the keywords are prioritized, thus capturing direct hits that satisfy the keyword criteria. At the same time, it would include recipes identified through semantic understanding as being related in meaning or context, like different Italian dishes that traditionally use tomato sauce but might not have been tagged explicitly with the user's search terms. It identifies results that are both directly and contextually relevant to the user's query while ideally minimizing misses and irrelevant suggestions.\n\n## When to consider hybrid search\n\nThe decision to use hybrid search depends on what your users are looking for in your app. For a code repository where developers need to find exact lines of code or error messages, keyword search is likely ideal because it matches specific terms. In a mental health forum where users search for advice or experiences related to their feelings, semantic search may be better because it finds results based on the meaning of a query, not just specific words. For a shopping app where customers might search for specific product names yet also be open to related suggestions, hybrid search combines the best of both worlds - finding exact matches while also uncovering similar products based on the shopping context.\n\n## How to combine search methods\n\nHybrid search merges keyword search and semantic search, but how does this process work?\n\nFirst, each search method is executed separately. Keyword search, which involves searching by specific words or phrases present in the content, will yield its own set of results. Similarly, semantic search, which involves understanding the context or meaning behind the search query rather than the specific words used, will generate its own unique results.\n\nNow with these separate result lists available, the next step is to combine them into a single, unified list. This is achieved through a process known as \u201cfusion\u201d. Fusion takes the results from both search methods and merges them together based on a certain ranking or scoring system. This system may prioritize certain results based on factors like their relevance to the search query, their ranking in the individual lists, or other criteria. The result is a final list that integrates the strengths of both keyword and semantic search methods.\n\n## Reciprocal Ranked Fusion (RRF)\n\nOne of the most common fusion methods is Reciprocal Ranked Fusion (RRF). The key idea behind RRF is to give more weight to the top-ranked items in each individual result list when building the final combined list.\n\nIn RRF, we iterate over each record and assign a score (noting that each record could exist in one or both lists). The score is calculated as 1 divided by that record's rank in each list, summed together between both lists. For example, if a record with an ID of\n\n`123` was ranked third in the keyword search and ninth in semantic search, it would receive a score of . If the record was found in only one list and not the other, it would receive a score of 0 for the other list. The records are then sorted by this score to create the final list. The items with the highest scores are ranked first, and lowest scores ranked last.\n\nThis method ensures that items that are ranked high in multiple lists are given a high rank in the final list. It also ensures that items that are ranked high in only a few lists but low in others are not given a high rank in the final list. Placing the rank in the denominator when calculating score helps penalize the low ranking records.\n\n### Smoothing constant\n\n`k`\n\nTo prevent extremely high scores for items that are ranked first (since we're dividing by the rank), a\n\n`k` constant is often added to the denominator to smooth the score:\n\nThis constant can be any positive number, but is typically small. A constant of 1 would mean that a record ranked first would have a score of instead of . This adjustment can help balance the influence of items that are ranked very high in individual lists when creating the final combined list.\n\n## Hybrid search in Postgres\n\nLet's implement hybrid search in Postgres using\n\n`tsvector` (keyword search) and\n\n`pgvector` (semantic search).\n\nFirst we'll create a\n\n`documents` table to store the documents that we will search over. This is just an example - adjust this to match the structure of your application.\n\n`1create table documents (2 id bigint primary key generated always as identity,3 content text,4 fts tsvector generated always as (to_tsvector('english', content)) stored,5 embedding extensions.vector(512)6);`\n\nThe table contains 4 columns:\n\n`id`is an auto-generated unique ID for the record. We'll use this later to match records when performing RRF.\n\n`content`contains the actual text we will be searching over.\n\n`fts`is an auto-generated\n\n`tsvector`column that is generated using the text in\n\n`content`. We will use this for full text search (search by keyword).\n\n`embedding`is a vector column that stores the vector generated from our embedding model. We will use this for semantic search (search by meaning). We chose 512 dimensions for this example, but adjust this to match the size of the embedding vectors generated from your preferred model.\n\nNext we'll create indexes on the\n\n`fts` and\n\n`embedding` columns so that their individual queries will remain fast at scale:\n\n`1-- Create an index for the full-text search2create index on documents using gin(fts);34-- Create an index for the semantic vector search5create index on documents using hnsw (embedding vector_ip_ops);`\n\nFor full text search we use a generalized inverted (GIN) index which is designed for handling composite values like those stored in a\n\n`tsvector`.\n\nFor semantic vector search we use an HNSW index, which is a high performing approximate nearest neighbor (ANN) search algorithm. Note that we are using the\n\n`vector_ip_ops` (inner product) operator with this index because we plan on using the inner product (\n\n`<#>`) operator later in our query. If you plan to use a different operator like cosine distance (\n\n`<=>`), be sure to update the index accordingly. For more information, see distance operators.\n\nFinally we'll create our\n\n`hybrid_search` function:\n\n`1create or replace function hybrid_search(2 query_text text,3 query_embedding extensions.vector(512),4 match_count int,5 full_text_weight float = 1,6 semantic_weight float = 1,7 rrf_k int = 508)9returns setof documents10language sql11as $$12with full_text as (13 select14 id,15 -- Note: ts_rank_cd is not indexable but will only rank matches of the where clause16 -- which shouldn't be too big17 row_number() over(order by ts_rank_cd(fts, websearch_to_tsquery(query_text)) desc) as rank_ix18 from19 documents20 where21 fts @@ websearch_to_tsquery(query_text)22 order by rank_ix23 limit least(match_count, 30) * 224),25semantic as (26 select27 id,28 row_number() over (order by embedding <#> query_embedding) as rank_ix29 from30 documents31 order by rank_ix32 limit least(match_count, 30) * 233)34select35 documents.*36from37 full_text38 full outer join semantic39 on full_text.id = semantic.id40 join documents41 on coalesce(full_text.id, semantic.id) = documents.id42order by43 coalesce(1.0 / (rrf_k + full_text.rank_ix), 0.0) * full_text_weight +44 coalesce(1.0 / (rrf_k + semantic.rank_ix), 0.0) * semantic_weight45 desc46limit47 least(match_count, 30)48$$;`\n\nLet's break this down:\n\n\n\n**Parameters:**The function accepts quite a few parameters, but the main (required) ones are\n\n`query_text`,\n\n`query_embedding`, and\n\n`match_count`.\n\n`query_text`is the user's query text (more on this shortly)\n\n`query_embedding`is the vector representation of the user's query produced by the embedding model. We chose 512 dimensions for this example, but adjust this to match the size of the embedding vectors generated from your preferred model. This must match the size of the\n\n`embedding`vector on the\n\n`documents`table (and use the same model).\n\n`match_count`is the number of records returned in the\n\n`limit`clause.\n\nThe other parameters are optional, but give more control over the fusion process.\n\n`full_text_weight`and\n\n`semantic_weight`decide how much weight each search method gets in the final score. These are both 1 by default which means they both equally contribute towards the final rank. A\n\n`full_text_weight`of 2 and\n\n`semantic_weight`of 1 would give full-text search twice as much weight as semantic search.\n\n`rrf_k`is the\n\n`k`smoothing constant added to the reciprocal rank. The default is 50.\n\n\n\n**Return type:**The function returns a set of records from our\n\n`documents`table.\n\n\n\n**CTE:**We create two common table expressions (CTE), one for full-text search and one for semantic search. These perform each query individually prior to joining them.\n\n\n\n**RRF:**The final query combines the results from the two CTEs using reciprocal rank fusion (RRF).\n\n## Running hybrid search\n\nTo use this function in SQL, we can run:\n\n`1select2 *3from4 hybrid_search(5 'Italian recipes with tomato sauce', -- user query6 '[...]'::extensions.vector(512), -- embedding generated from user query7 108 );`\n\nIn practice, you will likely be calling this from the Supabase client or through a custom backend layer. Here is a quick example of how you might call this from an Edge Function using JavaScript:\n\n`1import { createClient } from 'npm:@supabase/supabase-js@2'2import OpenAI from 'npm:openai'34const supabaseUrl = Deno.env.get('SUPABASE_URL')!5const supabaseServiceRoleKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!6const openaiApiKey = Deno.env.get('OPENAI_API_KEY')!78Deno.serve(async (req) => {9 // Grab the user's query from the JSON payload10 const { query } = await req.json()1112 // Instantiate OpenAI client13 const openai = new OpenAI({ apiKey: openaiApiKey })1415 // Generate a one-time embedding for the user's query16 const embeddingResponse = await openai.embeddings.create({17 model: 'text-embedding-3-large',18 input: query,19 dimensions: 512,20 })2122 const [{ embedding }] = embeddingResponse.data2324 // Instantiate the Supabase client25 // (replace service role key with user's JWT if using Supabase auth and RLS)26 const supabase = createClient(supabaseUrl, supabaseServiceRoleKey)2728 // Call hybrid_search Postgres function via RPC29 const { data: documents } = await supabase.rpc('hybrid_search', {30 query_text: query,31 query_embedding: embedding,32 match_count: 10,33 })3435 return new Response(JSON.stringify(documents), {36 headers: { 'Content-Type': 'application/json' },37 })38})`\n\nThis uses OpenAI's\n\n`text-embedding-3-large` model to generate embeddings (shortened to 512 dimensions for faster retrieval). Swap in your preferred embedding model (and dimension size) accordingly.\n\nTo test this, make a\n\n`POST` request to the function's endpoint while passing in a JSON payload containing the user's query. Here is an example\n\n`POST` request using cURL:\n\n`1curl -i --location --request POST \\2 'http://127.0.0.1:54321/functions/v1/hybrid-search' \\3 --header 'Authorization: Bearer <anonymous key>' \\4 --header 'Content-Type: application/json' \\5 --data '{\"query\":\"Italian recipes with tomato sauce\"}'`\n\nFor more information on how to create, test, and deploy edge functions, see Getting started.\n\n",
          "title": "When to consider hybrid search",
          "url": "https://supabase.com/docs/guides/ai/hybrid-search"
        },
        {
          "snippet": "RAG with Permissions\n\nFine-grain access control with Retrieval Augmented Generation.\n\nSince pgvector is built on top of Postgres, you can implement fine-grain access control on your vector database using Row Level Security (RLS). This means you can restrict which documents are returned during a vector similarity search to users that have access to them. Supabase also supports Foreign Data Wrappers (FDW) which means you can use an external database or data source to determine these permissions if your user data doesn't exist in Supabase.\n\nUse this guide to learn how to restrict access to documents when performing retrieval augmented generation (RAG).\n\n## Example\n\nIn a typical RAG setup, your documents are chunked into small subsections and similarity is performed over those sections:\n\n`1-- Track documents/pages/files/etc2create table documents (3 id bigint primary key generated always as identity,4 name text not null,5 owner_id uuid not null references auth.users (id) default auth.uid(),6 created_at timestamp with time zone not null default now()7);89-- Store the content and embedding vector for each section in the document10-- with a reference to original document (one-to-many)11create table document_sections (12 id bigint primary key generated always as identity,13 document_id bigint not null references documents (id),14 content text not null,15 embedding extensions.vector (384)16);`\n\nNotice how we record the\n\n`owner_id` on each document. Let's create an RLS policy that restricts access to\n\n`document_sections` based on whether or not they own the linked document:\n\n`1-- enable row level security2alter table document_sections enable row level security;34-- setup RLS for select operations5create policy \"Users can query their own document sections\"6on document_sections for select to authenticated using (7 document_id in (8 select id9 from documents10 where (owner_id = (select auth.uid()))11 )12);`\n\nIn this example, the current user is determined using the built-in\n\n`auth.uid()` function when the query is executed through your project's auto-generated REST API. If you are connecting to your Supabase database through a direct Postgres connection, see Direct Postgres Connection below for directions on how to achieve the same access control.\n\nNow every\n\n`select` query executed on\n\n`document_sections` will implicitly filter the returned sections based on whether or not the current user has access to them.\n\nFor example, executing:\n\n`1select * from document_sections;`\n\nas an authenticated user will only return rows that they are the owner of (as determined by the linked document). More importantly, semantic search over these sections (or any additional filtering for that matter) will continue to respect these RLS policies:\n\n`1-- Perform inner product similarity based on a match_threshold2select *3from document_sections4where document_sections.embedding <#> embedding < -match_threshold5order by document_sections.embedding <#> embedding;`\n\nThe above example only configures\n\n`select` access to users. If you wanted, you could create more RLS policies for inserts, updates, and deletes in order to apply the same permission logic for those other operations. See Row Level Security for a more in-depth guide on RLS policies.\n\n## Alternative scenarios\n\nEvery app has its own unique requirements and may differ from the above example. Here are some alternative scenarios we often see and how they are implemented in Supabase.\n\n### Documents owned by multiple people\n\nInstead of a one-to-many relationship between\n\n`users` and\n\n`documents`, you may require a many-to-many relationship so that multiple people can access the same document. Let's reimplement this using a join table:\n\n`1create table document_owners (2 id bigint primary key generated always as identity,3 owner_id uuid not null references auth.users (id) default auth.uid(),4 document_id bigint not null references documents (id)5);`\n\nThen your RLS policy would change to:\n\n`1create policy \"Users can query their own document sections\"2on document_sections for select to authenticated using (3 document_id in (4 select document_id5 from document_owners6 where (owner_id = (select auth.uid()))7 )8);`\n\nInstead of directly querying the\n\n`documents` table, we query the join table.\n\n### User and document data live outside of Supabase\n\nYou may have an existing system that stores users, documents, and their permissions in a separate database. Let's explore the scenario where this data exists in another Postgres database. We'll use a foreign data wrapper (FDW) to connect to the external DB from within your Supabase DB:\n\nRLS is latency-sensitive, so extra caution should be taken before implementing this method. Use the query plan analyzer to measure execution times for your queries to ensure they are within expected ranges. For enterprise applications, contact enterprise@supabase.io.\n\nFor data sources other than Postgres, see Foreign Data Wrappers for a list of external sources supported today. If your data lives in a source not provided in the list, contact support and we'll be happy to discuss your use case.\n\nLet's assume your external DB contains a\n\n`users` and\n\n`documents` table like this:\n\n`1create table public.users (2 id bigint primary key generated always as identity,3 email text not null,4 created_at timestamp with time zone not null default now()5);67create table public.documents (8 id bigint primary key generated always as identity,9 name text not null,10 owner_id bigint not null references public.users (id),11 created_at timestamp with time zone not null default now()12);`\n\nIn your Supabase DB, let's create foreign tables that link to the above tables:\n\n`1create schema external;2create extension postgres_fdw with schema extensions;34-- Setup the foreign server5create server foreign_server6 foreign data wrapper postgres_fdw7 options (host '<db-host>', port '<db-port>', dbname '<db-name>');89-- Map local 'authenticated' role to external 'postgres' user10create user mapping for authenticated11 server foreign_server12 options (user 'postgres', password '<user-password>');1314-- Import foreign 'users' and 'documents' tables into 'external' schema15import foreign schema public limit to (users, documents)16 from server foreign_server into external;`\n\nThis example maps the\n\n`authenticated` role in Supabase to the\n\n`postgres` user in the external DB. In production, it's best to create a custom user on the external DB that has the minimum permissions necessary to access the information you need.\n\nOn the Supabase DB, we use the built-in\n\n`authenticated` role which is automatically used when end users make authenticated requests over your auto-generated REST API. If you plan to connect to your Supabase DB over a direct Postgres connection instead of the REST API, you can change this to any user you like. See Direct Postgres Connection for more info.\n\nWe'll store\n\n`document_sections` and their embeddings in Supabase so that we can perform similarity search over them via pgvector.\n\n`1create table document_sections (2 id bigint primary key generated always as identity,3 document_id bigint not null,4 content text not null,5 embedding extensions.vector (384)6);`\n\nWe maintain a reference to the foreign document via\n\n`document_id`, but without a foreign key reference since foreign keys can only be added to local tables. Be sure to use the same ID data type that you use on your external documents table.\n\nSince we're managing users and authentication outside of Supabase, we have two options:\n\n- Make a direct Postgres connection to the Supabase DB and set the current user every request\n\n- Issue a custom JWT from your system and use it to authenticate with the REST API\n\n#### Direct Postgres connection\n\nYou can directly connect to your Supabase Postgres DB using the connection info on a project page. To use RLS with this method, we use a custom session variable that contains the current user's ID:\n\n`1-- enable row level security2alter table document_sections enable row level security;34-- setup RLS for select operations5create policy \"Users can query their own document sections\"6on document_sections for select to authenticated using (7 document_id in (8 select id9 from external.documents10 where owner_id = current_setting('app.current_user_id')::bigint11 )12);`\n\nThe session variable is accessed through the\n\n`current_setting()` function. We name the variable\n\n`app.current_user_id` here, but you can modify this to any name you like. We also cast it to a\n\n`bigint` since that was the data type of the\n\n`user.id` column. Change this to whatever data type you use for your ID.\n\nNow for every request, we set the user's ID at the beginning of the session:\n\n`1set app.current_user_id = '<current-user-id>';`\n\nThen all subsequent queries will inherit the permission of that user:\n\n`1-- Only document sections owned by the user are returned2select *3from document_sections4where document_sections.embedding <#> embedding < -match_threshold5order by document_sections.embedding <#> embedding;`\n\nYou might be tempted to discard RLS completely and simply filter by user within the\n\n`where` clause. Though this will work, we recommend RLS as a general best practice since RLS is always applied even as new queries and application logic is introduced in the future.\n\n#### Custom JWT with REST API\n\nIf you would like to use the auto-generated REST API to query your Supabase database using JWTs from an external auth provider, you can get your auth provider to issue a custom JWT for Supabase.\n\nSee the Clerk Supabase docs for an example of how this can be done. Modify the instructions to work with your own auth provider as needed.\n\nNow we can use the same RLS policy from our first example:\n\n`1-- enable row level security2alter table document_sections enable row level security;34-- setup RLS for select operations5create policy \"Users can query their own document sections\"6on document_sections for select to authenticated using (7 document_id in (8 select id9 from documents10 where (owner_id = (select auth.uid()))11 )12);`\n\nUnder the hood,\n\n`auth.uid()` references\n\n`current_setting('request.jwt.claim.sub')` which corresponds to the JWT's\n\n`sub` (subject) claim. This setting is automatically set at the beginning of each request to the REST API.\n\nAll subsequent queries will inherit the permission of that user:\n\n`1-- Only document sections owned by the user are returned2select *3from document_sections4where document_sections.embedding <#> embedding < -match_threshold5order by document_sections.embedding <#> embedding;`\n\n### Other scenarios\n\nThere are endless approaches to this problem based on the complexities of each system. Luckily Postgres comes with all the primitives needed to provide access control in the way that works best for your project.\n\nIf the examples above didn't fit your use case or you need to adjust them slightly to better fit your existing system, feel free to reach out to support and we'll be happy to assist you.\n\n",
          "title": "Documents owned by multiple...",
          "url": "https://supabase.com/docs/guides/ai/rag-with-permissions"
        }
      ],
      "type": "fetch_url_results"
    },
    {
      "queries": [
        "RAG chunking strategies fitness coaching content best practices",
        "conversation memory RAG summarization across sessions",
        "Fireworks AI LoRA JSONL training data format requirements"
      ],
      "results": [
        {
          "date": "2025-09-04",
          "id": 1,
          "last_updated": "2026-02-13T08:52:17",
          "snippet": "### LLM-Based Chunking\n\nLLM-based chunking uses a\n\n**large language model (LLM)** to decide how to split the text. Instead of relying on fixed rules or vector-based similarity scores, the LLM processes the document and generates semantically coherent chunks, often also adding additional context, summaries, or other information. This can be done by: **Identifying propositions**(breaking text into clear, logical statements) **Summarizing sections**into smaller, meaning-preserving chunks **Highlighting key points**to ensure the most relevant information is captured\n\n\u2026\n\n### Adaptive Chunking\n\nAdaptive chunking techniques dynamically adjust key\n\n**parameters** (like chunk size and overlap) based on the document's content.\n\nInstead of applying a single, fixed rule for an entire document, this method treats the text as a varied landscape. It might use machine learning models to analyze the semantic density and structure of different sections. For example, it could automatically create smaller, more granular chunks for a complex, information-rich paragraph to capture fine-grained details, while using larger chunks for a more general, introductory section.\n\n\u2026\n\n**When to use:** Documents with varied and inconsistent internal structures. Think of a long report that contains dense, technical paragraphs alongside sparse, narrative sections. An adaptive strategy excels here because it avoids the \"one-size-fits-all\" problem. It can create small, granular chunks for the complex parts to capture every detail, and larger chunks for the simpler text to preserve context, all within the same document.\n\n\u2026\n\n|Recursive|Splits text by repeatedly dividing it until it fits the desired chunk size, often preserving some structure.|Medium|Documents where some structure should be maintained but speed is still important|Research articles, product guides, short reports|\n|Document-Based|Treats each document as a single chunk or splits only at document boundaries.|Low|Collections of short, standalone documents|News articles, customer support tickets, short contracts|\n|Semantic|Splits text at natural meaning boundaries (topics, ideas).|Medium-High|Technical, academic, or narrative documents|Scientific papers, textbooks, novels, whitepapers|\n|LLM-Based|Uses a language model to decide chunk boundaries based on context, meaning, or task needs.|High|Complex text where meaning-aware chunking improves downstream tasks like summarization or Q&A|Long reports, legal opinions, medical records|\n\n\u2026\n\n|Hierarchical|Breaks text into multiple levels (sections \u2192 paragraphs \u2192 sentences). Keeps structure intact.|Medium|Large, structured docs like manuals, reports, or contracts|Employee handbooks, government regulations, software documentation|\n|Adaptive|Adjusts chunk size and overlap dynamically with ML or heuristics.|High|Mixed datasets with varying structures and lengths|Data from multiple sources: blogs, PDFs, emails, technical docs|",
          "source": "web",
          "title": "Chunking Strategies to Improve Your RAG Performance",
          "url": "https://weaviate.io/blog/chunking-strategies-for-rag"
        },
        {
          "date": "2025-09-22",
          "id": 2,
          "last_updated": "2025-12-04T07:11:02.852478",
          "snippet": "",
          "source": "web",
          "title": "Why Your RAG System Needs Memory: Building Stateful ...",
          "url": "https://ragaboutit.com/why-your-rag-system-needs-memory-building-stateful-conversational-ai-with-langchain-and-chromadb/"
        },
        {
          "date": "2026-02-11",
          "id": 3,
          "last_updated": "2026-02-13T12:18:18",
          "snippet": "Prepare a dataset\n\nDatasets must be in JSONL format, where each line represents a complete JSON-formatted training example. Make sure your data conforms to the following restrictions:\n\n**Minimum examples:**3 **Maximum examples:**3 million per dataset **File format:**\n\n`.jsonl`\n\n**Message schema:**Each training sample must include a messages array, where each message is an object with two fields:",
          "source": "web",
          "title": "Supervised Fine Tuning - Text",
          "url": "https://docs.fireworks.ai/fine-tuning/fine-tuning-models"
        },
        {
          "date": "2025-04-18",
          "id": 4,
          "last_updated": "2026-02-12T18:11:53",
          "snippet": "",
          "source": "web",
          "title": "Mastering Chunking Strategies for RAG - Databricks Community",
          "url": "https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chunking-strategies-for-rag-applications/ba-p/113089"
        },
        {
          "date": "2024-08-26",
          "id": 5,
          "last_updated": "2026-02-12T10:02:31",
          "snippet": "After that, we ask the LLM to continue updating and generate a new summary/ memory by combining the previous memory and subsequent dialogues. Finally, we encourage the LLM to respond using the latest memory as the primary reference to engage the ongoing dialogue. Given that the generated summaries are much shorter than the full dialogues, our proposed schema not only models long-term conversation memory but also serves as a potential solution to enable current LLMs to handle extremely long contexts (across multiple dialogue sessions) without expensively expanding the maximum length setting.\n\n\u2026\n\nFormally, each dialogue can be written as D=\\{S,C_{t},r_{t}\\}. Here, S=\\{S_{1},S_{2},...,S_{N}\\} represents N past sessions and each of the sessions consists of multiple utterances between two speakers. r_{t} is the ground truth response to C_{t} with background sessions S. C_{t}=\\{u_{1},r_{1},...,u_{t}\\} denotes the dialogue context of the current session at t step, where u and r represent the utterances from the user and the chatbot, respectively.\n\n\u2026\n\nwhere P_{r} is a prompt of the generator. Especially, taking the generated memory M_{N} and current session C_{t}, we ask the LLM again to generate a response. In Figure 3, the LLM considers the newest memory from Session 2 and provides a high consistent response to the user, i.e., \u201c more flexibility with a 24-hour gym membership\u201d.\n\n\u2026\n\nAs seen, the generated memories of both models gain considerable F1 scores (+25%), explaining the reliability of using dialogue summaries as memory. Besides, ChatGPT-Rsum (Ours) achieves higher overall performances on memory, suggesting that recursively summarizing can obtain more complete and long-term information than ChatGPT-MemoryBank. Finally, given the response performances of session 5 in Table 3 (Ours >ChatGPT-MemoryBank), we suppose that the accuracy of memory prediction is positively correlated with the quality of response. We also believe advanced memory mechanisms can achieve further improvement, which might be investigated in future works.",
          "source": "web",
          "title": "Recursively Summarizing Enables Long-Term Dialogue Memory in ...",
          "url": "https://arxiv.org/html/2308.15022v3"
        },
        {
          "date": "2026-02-03",
          "id": 6,
          "last_updated": "2026-02-05T06:06:49.979111",
          "snippet": "## Step-by-Step Guide to Fine-Tuning with Fireworks AI\n\n### 1. Preparing the DatasetDatasets must adhere strictly to the JSONL format, where each line represents a complete JSON-formatted training example.\n\n**Minimum Requirements:** **Minimum examples needed:**3 **Maximum examples:**Up to 3 million examples per dataset **File format:**JSONL (each line is a valid JSON object) **Message Schema:**Each training sample must include a\n\n\u2026\n\nWhile all of the above approaches should work,\n\n```\n\n// Upload JSONL file\n\nconst urlUpload = `${BASE_URL}/datasets/${DATASET_ID}:upload`;\n\nconst files = new FormData();\n\nfiles.append(\"file\", localFileInput.files[0]);\n\nconst uploadResponse = await fetch(urlUpload, {\n\nmethod: \"POST\",\n\nheaders: HEADERS,\n\nbody: files\n\n});\n\n```\n\n`UI` is more suitable for smaller datasets\n\n`< 500MB` while\n\n`firectl` might work better for bigger datasets.",
          "source": "web",
          "title": "Supervised Fine-Tuning (SFT) with LoRA on Fireworks AI",
          "url": "https://docs.fireworks.ai/fine-tuning/supervised-fine-tuning"
        },
        {
          "date": "2025-11-21",
          "id": 7,
          "last_updated": "2026-02-11T08:42:02",
          "snippet": "",
          "source": "web",
          "title": "Develop a RAG Solution - Chunking Phase - Microsoft Learn",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-chunking-phase"
        },
        {
          "date": "2025-06-02",
          "id": 8,
          "last_updated": "2025-10-16T06:56:07.652243",
          "snippet": "",
          "source": "web",
          "title": "Saving Conversations Across Sessions (AAIDC-Week3- ...",
          "url": "https://app.readytensor.ai/lessons/persistent-memory-saving-conversations-across-sessions-aaidc-week3-lecture-3b-Ax1KnCSIt9QI"
        },
        {
          "id": 9,
          "last_updated": "2025-11-14T09:29:16.745047",
          "snippet": "2\n\nPrepare a dataset\n\nDatasets must be in JSONL format, where each line represents a complete JSON-formatted training example. Make sure your data conforms to the following restrictions:\n\n**Minimum examples:**3 **Maximum examples:**3 million per dataset **File format:**\n\n`.jsonl`\n\n**Message schema:**Each training sample must include a messages array, where each message is an object with two fields:",
          "source": "web",
          "title": "Supervised fine-tuning for text (SFT) - Fireworks AI Docs",
          "url": "https://fireworksai-docs.mintlify.app/fine-tuning/fine-tuning-models"
        },
        {
          "date": "2025-06-07",
          "id": 10,
          "last_updated": "2026-02-13T10:49:48",
          "snippet": "- \u201cBasic\u201d chunking strategy: This method allows you to combine sequential elements to maximally fill each chunk while respecting the maximum chunk size limit. If a single isolated element exceeds the hard-max, it will be divided into two or more chunks.\n\n- \u201cBy title\u201d chunking strategy: This strategy leverages the document element types identified during partitioning to understand the document structure, and preserves section boundaries. This means that a single chunk will never contain text that occurred in two different sections, ensuring that topics remain self-contained for enhanced retrieval precision.",
          "source": "web",
          "title": "Chunking for RAG: best practices - Unstructured",
          "url": "https://unstructured.io/blog/chunking-for-rag-best-practices"
        },
        {
          "date": "2026-02-06",
          "id": 11,
          "last_updated": "2026-02-06T07:15:09.688274",
          "snippet": "",
          "source": "web",
          "title": "Mastering Conversation Summary Memory: A 2025 Guide - Sparkco",
          "url": "https://sparkco.ai/blog/mastering-conversation-summary-memory-a-2025-guide"
        },
        {
          "date": "2026-01-27",
          "id": 12,
          "last_updated": "2026-02-01T01:22:52.122790",
          "snippet": "## Fine-tuning a VLM using LoRA\n\nPrepare your vision dataset\n\nvision datasets must be in JSONL format in OpenAI-compatible chat format. Each line represents a complete training example.\n\n**Dataset Requirements:** **Format**:\n\n`.jsonl`file\n\n**Minimum examples**: 3 **Maximum examples**: 3 million per dataset **Images**: Must be base64 encoded with proper MIME type prefixes **Supported image formats**: PNG, JPG, JPEG **Message Schema:**Each training example must include a",
          "source": "web",
          "title": "Supervised Fine Tuning - Vision - Fireworks AI Docs",
          "url": "https://fireworksai-docs.mintlify.app/fine-tuning/fine-tuning-vlm"
        },
        {
          "date": "2025-10-10",
          "id": 13,
          "last_updated": "2026-02-12T21:33:10",
          "snippet": "|Budget is tight, need speed|Size-based chunking (details below)|Fastest to implement, no computational overhead|\n|Processing code files|Recursive with code separators (details below)|Respects function/class boundaries|\n|Short-form content (tweets, Q&A)|Sentence-based chunking (details below)|Preserves complete thoughts|\n|High-value content, experimental|LLM-based chunking (details below)|Context-aware, adapts to structure dynamically|\n**Many production systems use hybrid approaches:** route PDFs to page-level chunking, web pages to recursive splitting, and code to code-aware separators based on file type or content analysis.",
          "source": "web",
          "title": "Best Chunking Strategies for RAG in 2025 - Firecrawl",
          "url": "https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025"
        },
        {
          "date": "2025-09-10",
          "id": 14,
          "last_updated": "2026-02-12T19:06:44",
          "snippet": "",
          "source": "web",
          "title": "LLM Chat History Summarization Guide October 2025",
          "url": "https://mem0.ai/blog/llm-chat-history-summarization-guide-2025"
        },
        {
          "date": "2025-09-29",
          "id": 15,
          "last_updated": "2026-01-15T01:32:48.799258",
          "snippet": "`JSON`format.\n\nThe OCI Generative AI service accepts a\n\n`JSONL`file for fine-tuning custom models in the following format:\n\n`{\"prompt\": \"<first prompt>\", \"completion\": \"<expected completion given first prompt>\"} {\"prompt\": \"<second prompt>\", \"completion\": \"<expected completion given second prompt>\"} . . .`\n\n\u2026\n\n`JSONL` dataset file that you create for Generative AI has the following properties:\n\n- The file is\n\n`UTF-8`encoded.\n\n- Each line item contains a valid\n\n`JSON`object.\n\n- Each\n\n`JSON`object has two properties:\n\n`\"prompt\"`and\n\n`\"completion\"`.",
          "source": "web",
          "title": "Training Data in Generative AI - Oracle Help Center",
          "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/training-data-requirements.htm"
        }
      ],
      "type": "search_results"
    },
    {
      "contents": [
        {
          "snippet": "## Fine-tuning a model using SFT\n\nConfirm model support for fine-tuning\n\nYou can confirm that a base model is available to fine-tune by looking for the\n\n`Tunnable` tag in the model library or by using:\n\nAnd looking for\n\n```\n\nfirectl model get -a fireworks <MODEL-ID>\n\n```\n\n`Tunable: true`.\n\nSome base models cannot be tuned on Fireworks (\n\n`Tunable: false`) but still list support for LoRA (\n\n`Supports Lora: true`). This means that users can tune a LoRA for this base model on a separate platform and upload it to Fireworks for inference. Consult importing fine-tuned models for more information.\n\nPrepare a dataset\n\nDatasets must be in JSONL format, where each line represents a complete JSON-formatted training example. Make sure your data conforms to the following restrictions:\n\n**Minimum examples:**3 **Maximum examples:**3 million per dataset **File format:**\n\n`.jsonl`\n\n**Message schema:**Each training sample must include a messages array, where each message is an object with two fields:\n\n`role`: one of\n\n`system`,\n\n`user`, or\n\n`assistant`. A message with the\n\n`system`role is optional, but if specified, it must be the first message of the conversation\n\n`content`: a string representing the message content\n\n`weight`: optional key with value to be configured in either 0 or 1. message will be skipped if value is set to 0\n\n\n\n**Sample weight:**Optional key\n\n`weight`at the root of the JSON object. It can be any floating point number (positive, negative, or 0) and is used as a loss multiplier for tokens in that sample. If used, this field must be present in all samples in the dataset.\n\nHere is an example conversation dataset with sample weights:\n\n```\n\n\n\n\"messages\": [\n\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n\n{\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n\n{\"role\": \"assistant\", \"content\": \"Paris.\"}\n\n\n\n\"messages\": [\n\n{\"role\": \"user\", \"content\": \"What is 1+1?\"},\n\n{\"role\": \"assistant\", \"content\": \"2\", \"weight\": 0},\n\n{\"role\": \"user\", \"content\": \"Now what is 2+2?\"},\n\n{\"role\": \"assistant\", \"content\": \"4\"}\n\n\n\n```\n\nWe also support function calling dataset with a list of tools. An example would look like:\n\n```\n\n\n\n\"messages\": [\n\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n\n{\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n\n{\"role\": \"assistant\", \"content\": \"Paris.\"}\n\n],\n\n\"weight\": 0.5\n\n\n\n\"messages\": [\n\n{\"role\": \"user\", \"content\": \"What is 1+1?\"},\n\n{\"role\": \"assistant\", \"content\": \"2\", \"weight\": 0},\n\n{\"role\": \"user\", \"content\": \"Now what is 2+2?\"},\n\n{\"role\": \"assistant\", \"content\": \"4\"}\n\n],\n\n\"weight\": 1.0\n\n\n\n```\n\nFor the subset of models that supports thinking (e.g. DeepSeek R1, GPT OSS models and Qwen3 thinking models), we also support fine tuning with thinking traces. If you wish to fine tune with thinking traces, the dataset could also include thinking traces for assistant turns. Though optional, ideally each assistant turn includes a thinking trace. For example:\n\n```\n\n\n\n\"tools\": [\n\n\n\n\"type\": \"function\",\n\n\"function\": {\n\n\"name\": \"get_car_specs\",\n\n\"description\": \"Fetches detailed specifications for a car based on the given trim ID.\",\n\n\"parameters\": {\n\n\"trimid\": {\n\n\"description\": \"The trim ID of the car for which to retrieve specifications.\",\n\n\"type\": \"int\",\n\n\"default\": \"\"\n\n\n\n},\n\n],\n\n\"messages\": [\n\n\n\n\"role\": \"user\",\n\n\"content\": \"What is the specs of the car with trim 121?\"\n\n},\n\n\n\n\"role\": \"assistant\",\n\n\"tool_calls\": [\n\n\n\n\"type\": \"function\",\n\n\"function\": {\n\n\"name\": \"get_car_specs\",\n\n\"arguments\": \"{\\\"trimid\\\": 121}\"\n\n\n\n```\n\nNote that when fine tuning with intermediate thinking traces, the number of total tuned tokens could exceed the number of total tokens in the dataset. This is because we perform preprocessing and expand the dataset to ensure train-inference consistency.\n\n```\n\n\n\n\"messages\": [\n\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n\n{\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n\n{\"role\": \"assistant\", \"content\": \"Paris.\", \"reasoning_content\": \"The user is asking about the capital city of France, it should be Paris.\"}\n\n\n\n\"messages\": [\n\n{\"role\": \"user\", \"content\": \"What is 1+1?\"},\n\n{\"role\": \"assistant\", \"content\": \"2\", \"weight\": 0, \"reasoning_content\": \"The user is asking about the result of 1+1, the answer is 2.\"},\n\n{\"role\": \"user\", \"content\": \"Now what is 2+2?\"},\n\n{\"role\": \"assistant\", \"content\": \"4\", \"reasoning_content\": \"The user is asking about the result of 2+2, the answer should be 4.\"}\n\n\n\n```\n\nCreate and upload a dataset\n\nThere are a couple ways to upload the dataset to Fireworks platform for fine tuning:\n\n`firectl`,\n\n`Restful API` ,\n\n`builder SDK` or\n\n`UI`.\n\nWhile all of the above approaches should work,\n\n\n\nYou can simply navigate to the dataset tab, click\n\n`Create Dataset`and follow the wizard.\n\n`UI` is more suitable for smaller datasets\n\n`< 500MB` while\n\n`firectl` might work better for bigger datasets.Ensure the dataset ID conforms to the resource id restrictions.\n\nLaunch a fine-tuning job\n\nThere are also a couple ways to launch the fine-tuning jobs. We highly recommend creating supervised fine tuning jobs via\n\n`UI` .\n\nWith\n\nSimply navigate to the\n\n`Fine-Tuning` tab, click\n\n`Fine-Tune a Model` and follow the wizard from there. You can even pick a LoRA model to start the fine-tuning for continued training.\n\n`UI`, once the job is created, it will show in the list of jobs. Clicking to view the job details to monitor the job progress.With\n\n`firectl`, you can monitor the progress of the tuning job by running\n\nOnce the job successfully completes, you will see the new LoRA model in your model list\n\n```\n\nfirectl sftj get <DATASET_ID>\n\n```\n\n```\n\nfirectl model list\n\n```\n\n## Deploying a fine-tuned modelAfter fine-tuning completes, deploy your model to make it available for inference:\n\nThis creates a dedicated deployment with performance matching the base model.\n\n```\n\nfirectl deployment create <FINE_TUNED_MODEL_ID>\n\n```\n\n## Additional SFT job settingsAdditional tuning settings are available when starting a fine-tuning job. All of the below settings are optional and will have reasonable defaults if not specified. For settings that affect tuning quality like\n\n`epochs` and\n\n`learning rate`, we recommend using default settings and only changing hyperparameters if results are not as desired.\n\nEvaluation\n\nEvaluation\n\nBy default, the fine-tuning job will run evaluation by running the fine-tuned model against an evaluation set that\u2019s created by automatically carving out a portion of your training set. You have the option to explicitly specify a separate evaluation dataset to use instead of carving out training data.\n\n`evaluation_dataset`: The ID of a separate dataset to use for evaluation. Must be pre-uploaded via firectl\n\n```\n\nfirectl sftj create \\\n\n--evaluation-dataset my-eval-set \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset \\\n\n--output-model my-tuned-model\n\n```\n\nMax Context Length\n\nMax Context Length\n\nDepending on the size of the model, the default context size will be different. For most models, the default context size is >= 32768. Training examples will be cut-off at 32768 tokens. Usually you do not need to set the max context length unless out of memory error is encountered with higher lora rank and large max context length.\n\n```\n\nfirectl sftj create \\\n\n--max-context-length 65536 \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset \\\n\n--output-model my-tuned-model\n\n```\n\nBatch Size\n\nBatch Size\n\nBatch size is the number of tokens packed into one forward step during training. One batch could consist of multiple training samples. We do sequence packing on the training samples, and batch size controls how many total tokens will be packed into each batch.\n\n```\n\nfirectl sftj create \\\n\n--batch-size 65536 \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset \\\n\n--output-model my-tuned-model\n\n```\n\nEpochs\n\nEpochs\n\nEpochs are the number of passes over the training data. Our default value is 1. If the model does not follow the training data as much as expected, increase the number of epochs by 1 or 2. Non-integer values are supported.\n\n**Note: we set a max value of 3 million dataset examples \u00d7 epochs**\n\n```\n\nfirectl sftj create \\\n\n--epochs 2.0 \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset \\\n\n--output-model my-tuned-model\n\n```\n\nLearning rate\n\nLearning rate\n\nLearning rate controls how fast the model updates from data. We generally do not recommend changing learning rate. The default value is automatically based on your selected model.\n\n```\n\nfirectl sftj create \\\n\n--learning-rate 0.0001 \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset \\\n\n--output-model my-tuned-model\n\n```\n\nLearning rate warmup steps\n\nLearning rate warmup steps\n\nLearning rate warmup steps controls the number of training steps during which the learning rate will be linearly ramped up to the set learning rate.\n\n```\n\nfirectl sftj create \\\n\n--learning-rate 0.0001 \\\n\n--learning-rate-warmup-steps 200 \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset \\\n\n--output-model my-tuned-model\n\n```\n\nGradient accumlation steps\n\nGradient accumlation steps\n\nGradient accumulation steps controls the number of forward steps and backward steps to take (gradients are accumulated) before optimizer.step() is taken. Gradient accumulation steps > 1 increases effective batch size.\n\n```\n\nfirectl sftj create \\\n\n--gradient-accumulation-steps 4 \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset \\\n\n--output-model my-tuned-model\n\n```\n\nLoRA Rank\n\nLoRA Rank\n\nLoRA rank refers to the number of parameters that will be tuned in your LoRA add-on. Higher LoRA rank increases the amount of information that can be captured while tuning. LoRA rank must be a power of 2 up to 64. Our default value is 8.\n\n```\n\nfirectl sftj create \\\n\n--lora-rank 16 \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset \\\n\n--output-model my-tuned-model\n\n```\n\nTraining progress and monitoring\n\nTraining progress and monitoring\n\nThe fine-tuning service integrates with Weights & Biases to provide observability into the tuning process. To use this feature, you must have a Weights & Biases account and have provisioned an API key.\n\n```\n\nfirectl sftj create \\\n\n--wandb-entity my-org \\\n\n--wandb-api-key xxx \\\n\n--wandb-project \"My Project\" \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset \\\n\n--output-model my-tuned-model\n\n```\n\nModel ID\n\nModel ID\n\nBy default, the fine-tuning job will generate a random unique ID for the model. This ID is used to refer to the model at inference time. You can optionally specify a custom ID, within ID constraints.\n\n```\n\nfirectl sftj create \\\n\n--output-model my-model \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset\n\n```\n\nJob ID\n\nJob ID\n\nBy default, the fine-tuning job will generate a random unique ID for the fine-tuning job. You can optionally choose a custom ID.\n\n```\n\nfirectl sftj create \\\n\n--job-id my-fine-tuning-job \\\n\n--base-model MY_BASE_MODEL \\\n\n--dataset cancerset \\\n\n--output-model my-tuned-model\n\n```\n\n## Appendix\n\n`Python SDK`references\n\n`Restful API`references\n\n`firectl`references\n\n- Complete Python SDK workflow example for a code-only implementation\n\n",
          "title": "Supervised Fine Tuning - Text",
          "url": "https://docs.fireworks.ai/fine-tuning/fine-tuning-models"
        },
        {
          "snippet": "**How it works:**\n\n- Define tools using JSON Schema (name, description, parameters)\n\n- Model analyzes the query and decides whether to call a tool\n\n- If needed, model returns structured tool calls with parameters\n\n- You execute the tool and send results back for the final response\n\n## Quick exampleDefine tools and send a request - the model will return structured tool calls when needed: Initialize the client:\n\nDefine the tools and make the request:\n\n```\n\nfrom fireworks import Fireworks\n\nclient = Fireworks()\n\n```\n\n```\n\ntools = [{\n\n\"type\": \"function\",\n\n\"function\": {\n\n\"name\": \"get_weather\",\n\n\"description\": \"Get the current weather for a location\",\n\n\"parameters\": {\n\n\"type\": \"object\",\n\n\"properties\": {\n\n\"location\": {\"type\": \"string\", \"description\": \"City name\"}\n\n},\n\n\"required\": [\"location\"]\n\n\n\n}]\n\nresponse = client.chat.completions.create(\n\nmodel=\"accounts/fireworks/models/kimi-k2-instruct-0905\",\n\nmessages=[{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}],\n\ntools=tools,\n\ntemperature=0.1\n\n\n\nprint(response.choices[0].message.tool_calls)\n\n# Output: [ChatCompletionMessageToolCall(id='call_abc123', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function')]\n\n```\n\nComplete workflow: Execute tools and get final response\n\nComplete workflow: Execute tools and get final response\n\n```\n\nimport json\n\n# Step 1: Define your tools\n\ntools = [{\n\n\"type\": \"function\",\n\n\"function\": {\n\n\"name\": \"get_weather\",\n\n\"description\": \"Get the current weather for a location\",\n\n\"parameters\": {\n\n\"type\": \"object\",\n\n\"properties\": {\n\n\"location\": {\"type\": \"string\", \"description\": \"City name\"},\n\n\"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n\n},\n\n\"required\": [\"location\"]\n\n\n\n}]\n\n# Step 2: Send initial request\n\nmessages = [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]\n\nresponse = client.chat.completions.create(\n\nmodel=\"accounts/fireworks/models/kimi-k2-instruct-0905\",\n\nmessages=messages,\n\ntools=tools,\n\ntemperature=0.1\n\n\n\n# Step 3: Check if model wants to call a tool\n\nif response.choices[0].message.tool_calls:\n\n# Step 4: Execute the tool\n\ntool_call = response.choices[0].message.tool_calls[0]\n\n# Your actual tool implementation\n\ndef get_weather(location, unit=\"celsius\"):\n\n# In production, call your weather API here\n\nreturn {\"temperature\": 72, \"condition\": \"sunny\", \"unit\": unit}\n\n# Parse arguments and call your function\n\nfunction_args = json.loads(tool_call.function.arguments)\n\nfunction_response = get_weather(**function_args)\n\n# Step 5: Send tool response back to model\n\nmessages.append(response.choices[0].message) # Add assistant's tool call\n\nmessages.append({\n\n\"role\": \"tool\",\n\n\"tool_call_id\": tool_call.id,\n\n\"content\": json.dumps(function_response)\n\n})\n\n# Step 6: Get final response\n\nfinal_response = client.chat.completions.create(\n\nmodel=\"accounts/fireworks/models/kimi-k2-instruct-0905\",\n\nmessages=messages,\n\ntools=tools,\n\ntemperature=0.1\n\n\n\nprint(final_response.choices[0].message.content)\n\n# Output: \"It's currently 72\u00b0F and sunny in San Francisco.\"\n\n```\n\n## Defining toolsTools are defined using JSON Schema format. Each tool requires:\n\n**name**: Function identifier (a-z, A-Z, 0-9, underscores, dashes; max 64 characters) **description**: Clear explanation of what the function does (used by the model to decide when to call it) **parameters**: JSON Schema object describing the function\u2019s parameters\n\n### Parameter typesJSON Schema supports:\n\n`string`,\n\n`number`,\n\n`integer`,\n\n`object`,\n\n`array`,\n\n`boolean`, and\n\n`null`. You can also:\n\n- Use\n\n`enum`to restrict values to specific options\n\n- Mark parameters as\n\n`required`or optional\n\n- Provide descriptions for each parameter\n\nExample: Defining multiple tools\n\nExample: Defining multiple tools\n\n```\n\ntools = [\n\n\n\n\"type\": \"function\",\n\n\"function\": {\n\n\"name\": \"get_weather\",\n\n\"description\": \"Get current weather for a location\",\n\n\"parameters\": {\n\n\"type\": \"object\",\n\n\"properties\": {\n\n\"location\": {\n\n\"type\": \"string\",\n\n\"description\": \"City name, e.g. San Francisco\"\n\n},\n\n\"unit\": {\n\n\"type\": \"string\",\n\n\"enum\": [\"celsius\", \"fahrenheit\"],\n\n\"description\": \"Temperature unit\"\n\n\n\n},\n\n\"required\": [\"location\"]\n\n\n\n},\n\n\n\n\"type\": \"function\",\n\n\"function\": {\n\n\"name\": \"search_restaurants\",\n\n\"description\": \"Search for restaurants by cuisine type\",\n\n\"parameters\": {\n\n\"type\": \"object\",\n\n\"properties\": {\n\n\"cuisine\": {\n\n\"type\": \"string\",\n\n\"description\": \"Type of cuisine (e.g., Italian, Mexican)\"\n\n},\n\n\"location\": {\n\n\"type\": \"string\",\n\n\"description\": \"City or neighborhood\"\n\n},\n\n\"price_range\": {\n\n\"type\": \"string\",\n\n\"enum\": [\"$\", \"$$\", \"$$$\", \"$$$$\"]\n\n\n\n},\n\n\"required\": [\"cuisine\", \"location\"]\n\n\n\n```\n\n## Additional configurations\n\n### tool_choiceThe\n\n`tool_choice` parameter controls how the model uses tools:\n\n**(default): Model decides whether to call a tool or respond directly**\n\n`auto`\n\n**: Model will not call any tools**\n\n`none`\n\n**: Model must call at least one tool**\n\n`required`\n\n**Specific function**: Force the model to call a particular function\n\n```\n\n# Force a specific tool\n\nresponse = client.chat.completions.create(\n\nmodel=\"accounts/fireworks/models/kimi-k2-instruct-0905\",\n\nmessages=[{\"role\": \"user\", \"content\": \"What's the weather?\"}],\n\ntools=tools,\n\ntool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}},\n\ntemperature=0.1\n\n\n\n```\n\nSome models support parallel tool calling, where multiple tools can be called in a single response. Check the model\u2019s capabilities before relying on this feature.\n\n## Streaming\n\nUsing tool calls with streaming responses\n\nUsing tool calls with streaming responses\n\nTool calls work with streaming responses. Arguments are sent incrementally as the model generates them:\n\n```\n\nimport json\n\ntools = [{\n\n\"type\": \"function\",\n\n\"function\": {\n\n\"name\": \"get_weather\",\n\n\"description\": \"Get the current weather for a city\",\n\n\"parameters\": {\n\n\"type\": \"object\",\n\n\"properties\": {\n\n\"city\": {\"type\": \"string\", \"description\": \"City name\"},\n\n\"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n\n},\n\n\"required\": [\"city\"]\n\n\n\n}]\n\nstream = client.chat.completions.create(\n\nmodel=\"accounts/fireworks/models/kimi-k2-instruct-0905\",\n\nmessages=[{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}],\n\ntools=tools,\n\nstream=True,\n\ntemperature=0.1\n\n\n\n# Accumulate tool call data\n\ntool_calls = {}\n\nfor chunk in stream:\n\nif chunk.choices[0].delta.tool_calls:\n\nfor tool_call in chunk.choices[0].delta.tool_calls:\n\nindex = tool_call.index\n\nif index not in tool_calls:\n\ntool_calls[index] = {\"id\": \"\", \"name\": \"\", \"arguments\": \"\"}\n\nif tool_call.id:\n\ntool_calls[index][\"id\"] = tool_call.id\n\nif tool_call.function and tool_call.function.name:\n\ntool_calls[index][\"name\"] = tool_call.function.name\n\nif tool_call.function and tool_call.function.arguments:\n\ntool_calls[index][\"arguments\"] += tool_call.function.arguments\n\nif chunk.choices[0].finish_reason == \"tool_calls\":\n\nfor tool_call in tool_calls.values():\n\nargs = json.loads(tool_call[\"arguments\"])\n\nprint(f\"Calling {tool_call['name']} with {args}\")\n\nbreak\n\n```\n\n## Troubleshooting\n\nModel isn't calling tools when expected\n\nModel isn't calling tools when expected\n\n- Check that your tool descriptions are clear and detailed\n\n- Ensure the user query clearly indicates a need for the tool\n\n- Try using\n\n`tool_choice=\"required\"`to force tool usage\n\n- Verify your model supports tool calling (check\n\n`supportsTools`field)\n\nTool arguments are incorrect or malformed\n\nTool arguments are incorrect or malformed\n\n- Add more detailed parameter descriptions\n\n- Use lower temperature (0.0-0.3) for more deterministic outputs\n\n- Provide examples in parameter descriptions\n\n- Use\n\n`enum`to constrain values to specific options\n\nGetting JSON parsing errors\n\nGetting JSON parsing errors\n\n- Always validate tool call arguments before parsing\n\n- Handle partial or malformed JSON gracefully in production\n\n- Use try-catch blocks when parsing\n\n`tool_call.function.arguments`\n\n",
          "title": "tool_choice",
          "url": "https://docs.fireworks.ai/guides/function-calling"
        }
      ],
      "type": "fetch_url_results"
    },
    {
      "queries": [
        "Kimi K2 model ID Fireworks AI accounts/fireworks/models",
        "Hyrox race format workout stations distances",
        "RAG system prompt architecture coaching persona best practices"
      ],
      "results": [
        {
          "date": "2025-07-14",
          "id": 1,
          "last_updated": "2026-02-07T18:49:13",
          "snippet": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\n|## Fine-tuningDocs|Kimi K2 Instruct can be customized with your data to improve responses. Fireworks uses LoRA to efficiently train and deploy your personalized model|\n|--|--|\n|## On-demand DeploymentDocs|On-demand deployments give you dedicated GPUs for Kimi K2 Instruct using Fireworks' reliable, high-performance system with no rate limits.|",
          "source": "web",
          "title": "Kimi K2 Instruct API & Playground - Fireworks AI",
          "url": "https://fireworks.ai/models/fireworks/kimi-k2-instruct"
        },
        {
          "id": 2,
          "last_updated": "2025-10-02T06:44:56.353824",
          "snippet": "",
          "source": "web",
          "title": "rulebook single",
          "url": "https://hyrox.com/wp-content/uploads/2022/12/EN_R5_SINGLE_RULEBOOK_EN_SEASON-22_23-1.pdf"
        },
        {
          "date": "2024-02-10",
          "id": 3,
          "last_updated": "2026-02-12T05:57:32",
          "snippet": "",
          "source": "web",
          "title": "Prompt engineering for RAG - OpenAI Developer Community",
          "url": "https://community.openai.com/t/prompt-engineering-for-rag/621495"
        },
        {
          "date": "2025-09-24",
          "id": 4,
          "last_updated": "2026-02-11T14:15:02",
          "snippet": "The Kimi Series models are built by Moonshot AI, an Alibaba backed company founded by Zhilin Yang. They are known for pioneering open agentic intelligence with state-of-the-art performance in coding, document intelligence, and multi-turn agents. Their most capable models, including the latest multimodal Kimi K2.5, are available for developers to start building with on Fireworks with a single click today.\nExplore top models from Moonshot AI, including the Kimi K2.5 and Kimi K2 Thinking, that deliver state-of-the-art performance in complex agents, coding, and document intelligence.\n\nKimi K2.5 Achieves SOTA quality in complex multi-agent reasoning and visual tasks,\n\n**outperforming closed-source models like Opus 4.5 and Gemini 3 Pro**\nKimi K2.5 elevates your Agent use cases by delivering Moonshot\u2019s multimodal power. It integrates image and video understanding with transparent 'thinking' modes, ensuring unmatched trust and visibility in every agentic workflow.\n\nThe Kimi K2 Series incorporate a Mixture-of-Experts (MoE) architecture that provides superior efficiency and higher performance. It also provides a large context window that maintains remarkable coherence for complex analysis and multi-turn flows, making it the optimized choice for long-range agentic workflows.",
          "source": "web",
          "title": "Kimi on Fireworks - Explore All Models Available via API ...",
          "url": "https://fireworks.ai/kimi"
        },
        {
          "id": 5,
          "last_updated": "2025-10-02T06:08:55.095422",
          "snippet": "",
          "source": "web",
          "title": "rulebook doubles",
          "url": "https://hyrox.com/wp-content/uploads/2022/12/EN_R5_DOUBLES_RULEBOOK_EN_SEASON-22_23-1.pdf"
        },
        {
          "date": "2025-07-28",
          "id": 6,
          "last_updated": "2025-10-22T15:05:33.919762",
          "snippet": "",
          "source": "web",
          "title": "Best practices - GenAI Prompt skill - Azure AI Search",
          "url": "https://learn.microsoft.com/en-us/azure/search/responsible-ai-best-practices-genai-prompt-skill"
        },
        {
          "date": "2026-02-09",
          "id": 7,
          "last_updated": "2026-02-10T15:20:32",
          "snippet": "## Specifications\n\n|Model Key|`fireworks_ai/accounts/fireworks/models/kimi-k2-thinking`|\n|--|--|\n|Provider|Fireworks AI|\n|Provider ID|fireworks_ai|\n|Mode|Text|\n|Canonical Name|kimi-k2-thinking|\n|Context Window|262K tokens|\n|Max Output|262K tokens|",
          "source": "web",
          "title": "Kimi K2 Thinking Pricing & Specs | AI Models | CloudPrice",
          "url": "https://cloudprice.net/models/fireworks_ai%2Faccounts%2Ffireworks%2Fmodels%2Fkimi-k2-thinking"
        },
        {
          "id": 8,
          "last_updated": "2025-10-01T07:02:01.344279",
          "snippet": "",
          "source": "web",
          "title": "rulebook single",
          "url": "https://hyrox.com/wp-content/uploads/2024/09/SINGLE_RULEBOOK_EN_24_25-.pdf"
        },
        {
          "date": "2026-02-10",
          "id": 9,
          "last_updated": "2026-02-13T11:07:36",
          "snippet": "",
          "source": "web",
          "title": "The Complete Guide to Prompt Engineering for Retrieval ...",
          "url": "https://www.stack-ai.com/blog/prompt-engineering-for-rag-pipelines-the-complete-guide-to-prompt-engineering-for-retrieval-augmented-generation"
        },
        {
          "date": "2025-04-09",
          "id": 10,
          "last_updated": "2026-02-13T11:44:02",
          "snippet": "Release Date: 9/4/2025\n\nKimi K2 0905 is an updated version of Kimi K2, a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Kimi K2 0905 has improved coding abilities, a longer context window, and agentic tool use, and a longer (262K) context window.\n\nAvg. Accuracy\n\n37.50%\n\nLatency\n\n149.81s\n\nCost (In/Out)\n\nContext Window\n\n256k\n\nMax Output Tokens\n\n256k\n\nInput Modality",
          "source": "web",
          "title": "Kimi K2 Instruct 0905 - Vals AI",
          "url": "https://www.vals.ai/models/fireworks_kimi-k2-instruct-0905"
        },
        {
          "date": "2025-11-02",
          "id": 11,
          "last_updated": "2026-01-20T09:06:15.557436",
          "snippet": "By the end, you\u2019ll have covered\n\n**8 kilometers of running** and completed **8 functional workout stations**. The entire race is done indoors, usually in a large convention center, so you don\u2019t have to worry about weather, terrain, or route changes.\n\nEach station tests a different part of your fitness:",
          "source": "web",
          "title": "HYROX beginner guide: Format, stations, times, and first-race tips ...",
          "url": "https://www.rezerv.co/blogs/hyrox-beginner-guide-format-stations-times-and-first-race-tips-2026"
        },
        {
          "date": "2025-11-25",
          "id": 12,
          "last_updated": "2026-02-13T07:30:06",
          "snippet": "As highlighted in the\n\n**Nuclia RAG Academy** session featuring the Nuclia Lead Machine Learning Engineer and \u201cprompting master,\u201d Carmen Iniesta, prompting sits at a critical juncture in the **RAG** pipeline. After the user asks a question and the relevant context is retrieved from your data source (like NucliaDB), both the user\u2019s question and the retrieved context are sent to the LLM. The prompt acts as the instruction manual for the LLM, guiding how it should use the provided context to generate the final answer.",
          "source": "web",
          "title": "Mastering the Art of Prompting LLMs for RAG",
          "url": "https://www.progress.com/blogs/mastering-the-art-of-prompting-llms-for-rag"
        },
        {
          "date": "2026-02-11",
          "id": 13,
          "last_updated": "2026-02-11T14:41:23",
          "snippet": "## Specifications\n\n|Model Key|`fireworks_ai/accounts/fireworks/models/kimi-k2-instruct`|\n|--|--|\n|Provider|Fireworks AI|\n|Provider ID|fireworks_ai|\n|Mode|Text|\n|Canonical Name|kimi-k2|\n|Context Window|131K tokens|\n|Max Output|16K tokens|\n\n## Capabilities\u2717 Vision\u2713 Function Calling\u2717 Reasoning\u2713 JSON Schema\u2717 System Messages\u2717 Web Search\u2717 Prompt Caching\u2717 Audio Input\u2717 Audio Output\n\n## Pricing\n\n|Type|Per 1K Tokens|Per 1M Tokens|\n|--|--|--|\n|Input Tokens|0.000600|0.600|\n|Output Tokens|0.0025|2.50|\n\n\u2026\n\n|Fireworks AI|fireworks_ai/accounts/fireworks/models/kimi-k2-instruct|0.600|2.50|\n|Hyperbolic|hyperbolic/moonshotai/Kimi-K2-Instruct|2.00|2.00|\n|Novita|novita/moonshotai/kimi-k2-instruct|0.570|2.30|",
          "source": "web",
          "title": "Kimi K2 Instruct Pricing & Specs | AI Models",
          "url": "https://cloudprice.net/models/fireworks_ai%2Faccounts%2Ffireworks%2Fmodels%2Fkimi-k2-instruct"
        },
        {
          "id": 14,
          "last_updated": "2025-10-01T01:55:43.894481",
          "snippet": "workout station 2, etc.] until they have completed a \ntotal of 8 km running and 8 stations.\nWhilst the repetition range and/or weights differ \nbetween the divisions, the running distance remains \nthe same across all divisions: each racer must run 1 \nkm before each, i.e. a total distance of 8 km.\n\n\u2026\n\ninfringement.\nAll HYROX courses comprise the same distances \nand combine the Run Course, Roxzone and workout \nstation distances.\nThe running splits are always around 1,000 meters; \ndepending on the venue, this may be split between \n1-5 running laps.\nIn some cases, the first/last running lap may be less",
          "source": "web",
          "title": "SINGLE",
          "url": "https://hyrox.com/wp-content/uploads/2025/07/25_26_HYROX_RulebookSingles_EN.pdf"
        },
        {
          "date": "2025-04-07",
          "id": 15,
          "last_updated": "2026-02-13T10:20:52",
          "snippet": "The effectiveness of a Retrieve-Augmented Generation (RAG) system doesn't just depend on retrieving the right information; it significantly relies on\n\n*how* that information is presented to the Large Language Model (LLM). The prompt acts as the bridge between the retrieved context and the generation process. Crafting this prompt well is essential for guiding the LLM to produce accurate, relevant, and contextually grounded responses.\nThink of the prompt as the instruction manual you provide to the LLM. It needs to clearly state the task, present the relevant evidence (the retrieved context), and specify how that evidence should be used to fulfill the user's original request. A poorly structured prompt can lead the LLM astray, even if the retrieval step was successful. It might ignore the context, misinterpret the user's query, or fail to synthesize the information effectively.",
          "source": "web",
          "title": "Structuring Prompts for RAG Systems",
          "url": "https://apxml.com/courses/getting-started-rag/chapter-4-rag-generation-augmentation/structuring-rag-prompts"
        }
      ],
      "type": "search_results"
    },
    {
      "contents": [
        {
          "snippet": "# HYROX beginner guide: Format, stations, times, and first-race tips (2026)\n\nNew to HYROX? Learn the format, all 8 stations, time goals, pacing, and a race-day checklist so you can finish strong in 2026.HYROXHYROX raceHYROX beginner\n\nSo, you\u2019ve heard about HYROX and you\u2019re wondering what the hype\u2019s all about. Maybe you saw the videos on TikTok: people running, pushing sleds, throwing wall balls, and thought,\n\n*\u201cThat looks intense... but kind of fun.\u201d* Well, you\u2019re right on both counts.\n\nHYROX is a global fitness race designed for\n\n*everyone*, from everyday gym-goers to seasoned athletes. It blends running and functional workouts in a structured format that tests endurance, strength, and grit, all in one event. Think of it as the perfect bridge between traditional endurance races and high-intensity gym training.\n\nIf you\u2019re new, the setup might sound intimidating at first. Eight 1-kilometer runs. Eight workout stations in between. Thousands of people cheering around you.\n\nBut HYROX is incredibly beginner-friendly. The race is standardized worldwide, so no matter where you compete (New York, Singapore, or Berlin) it\u2019s the same format, same challenge, same community vibe.\n\nIn this beginner\u2019s guide, you\u2019ll learn exactly how HYROX works:\n\n- The race format and station breakdown\n\n- Typical finish times and pacing goals\n\n- A simple 4-week training plan\n\n- Race-day checklists and practical tips\n\nUse this page as your playbook. Skim the station cards for quick technique cues, check the pacing table to set a goal, then follow the starter plan to build confidence week by week. Bookmark it, share it with your training buddy, and come back before race day.\n\nSource: Unsplash\n\n## What is HYROX?\n\nHYROX is a standardized indoor fitness race that combines endurance running with functional workouts. Each race follows the same format worldwide, so every athlete, from first-timers to pros, faces an identical challenge.\n\nThat\u2019s part of what makes HYROX so addictive: you can train, track your progress, and directly compare your performance with thousands of other racers around the globe.\n\nHere\u2019s how it works in simple terms:\n\n- You\u2019ll\n\n**run 1 kilometer**, then complete a **workout station**.\n\n- You\u2019ll repeat that sequence\n\n**eight times**\u2014making it **8 runs + 8 workouts**in total.\n\n- You finish with the infamous\n\n**wall balls**, which test every ounce of energy you\u2019ve got left.\n\nThe workouts focus on real-life movements\u2014pushing, pulling, carrying, rowing\u2014so you don\u2019t need to be a CrossFitter or a marathoner to join in. You just need consistency, basic fitness, and the willingness to challenge yourself.\n\nAnother reason HYROX is growing fast? HYROX is more than a race, it\u2019s an experience. Events are held in massive indoor arenas, with DJ sets, bright lights, and cheering crowds that make it feel more like a sporting festival than a competition. Whether you join solo, with a partner, or in a relay team, the atmosphere is electric and motivating from start to finish.\n\n## Race format at a glance\n\nEvery HYROX event follows the same structure worldwide, no surprises, no hidden twists. That\u2019s part of the magic. You know exactly what\u2019s coming, and you can train specifically for it.\n\nHere\u2019s the simple breakdown:\n\n**Run 1 km \u2192 Workout Station 1 \u2192 Run 1 km \u2192 Workout Station 2 \u2026 all the way to Station 8.**\n\nBy the end, you\u2019ll have covered\n\n**8 kilometers of running** and completed **8 functional workout stations**. The entire race is done indoors, usually in a large convention center, so you don\u2019t have to worry about weather, terrain, or route changes.\n\nEach station tests a different part of your fitness:\n\n**Strength**(sled push and pull) **Power**(wall balls, burpee broad jumps) **Endurance**(row and SkiErg) **Stability and coordination**(farmer\u2019s carry, lunges)\n\nBetween every workout, you\u2019ll return to the\n\n**running track**, your recovery and reset time before the next challenge.\n\nRight in the middle of all this action is the\n\n**Roxzone.**\n\nRoxzine is the transition area connecting your run to the workout stations. Managing this zone efficiently is key to a smooth race. It\u2019s where you\u2019ll catch your breath, pick up equipment, and move between workouts. Think of it as your pit stop: fast, focused, and efficient.\n\nNo matter the location, London, Sydney, or Chicago, the HYROX course layout, order of stations, and distance remain identical. That\u2019s what makes it the first truly global fitness race. Your time in one city can be directly compared to someone racing halfway across the world.\n\nSource: Unsplash\n\n## HYROX categories explained\n\nOne of the coolest things about HYROX is that it\u2019s designed for\n\n*everyone*. You get to pick the level that matches your current fitness and goals. There\u2019s no need to \u201cqualify\u201d for any category, and you can always move up when you\u2019re ready.\n\nHere\u2019s a quick breakdown of the four main HYROX categories:\n\n#### 1. Open\n\nThis is the standard and most popular division for first-timers.\n\n- Moderate weights for sled push/pull, lunges, and wall balls.\n\n- Designed for accessibility, so anyone with a decent fitness base can finish.\n\n- Perfect for your first race or if you\u2019re still working on technique and pacing.\n\n**Goal:** Complete and enjoy the experience while setting your personal benchmark.\n\n#### 2. Pro\n\nThis is where things get heavier and faster.\n\n- The format is the same, but weights increase significantly.\n\n- Usually chosen by experienced athletes, CrossFitters, or competitive racers.\n\n- Expect a tougher challenge, but also the thrill of pushing your limits.\n\n**Goal:** Compete for rankings or improve on your Open time with added resistance.\n\n#### 3. Doubles\n\nRace with a partner and share the work!\n\n- Both teammates run the full 8 km together.\n\n- The workout stations are split however you like using the\n\n**\u201cYou Go, I Go\u201d (YGIG)**rule.\n\n- Great for friends, couples, or training buddies who want to experience HYROX together without the full individual load.\n\n**Goal:** Divide and conquer, teamwork and pacing make all the difference.\n\n#### 4. Relay\n\nIdeal for teams of four.\n\n- Each member completes 2 runs and 2 workout stations before tagging the next teammate.\n\n- It\u2019s fast, loud, and super fun, perfect for groups or corporate challenges.\n\n**Goal:** Go all out in your section and experience the HYROX energy without doing the entire race solo.\n\nSource: Unsplash\n\n## The 8 HYROX workout stations explained\n\nEvery HYROX race features the\n\n**same eight workout stations**, always in the **same order**. This consistency makes training easier because you can prepare for exactly what\u2019s coming. Each station challenges a different aspect of fitness: cardio, strength, power, endurance, and stability.\n\nLet\u2019s break them down one by one:\n\n#### 1. SkiErg (1,000 meters)\n\nThe first station hits your upper body and lungs hard.\n\n**Focus:**Pulling power and rhythm. **Tip:**Keep strokes long and controlled, don\u2019t sprint too early. **Common mistake:**Hunching over or using only your arms instead of driving from the hips.\n\n#### 2. Sled Push (50 meters)\n\nA full-body power test that hits your legs and core.\n\n**Focus:**Strong drive through your quads and glutes. **Tip:**Keep a low body angle, short steps, and constant tension. **Common mistake:**Pushing with just your arms or standing too upright, which kills momentum.\n\n#### 3. Sled Pull (50 meters)\n\nNow the pull to balance out the push.\n\n**Focus:**Grip strength and posterior chain. **Tip:**Use a hand-over-hand motion, lean back slightly, and walk backward steadily. **Common mistake:**Jerky pulls that waste ener, smooth is fast here.\n\n#### 4. Burpee Broad Jumps (80 meters)\n\nIt\u2019s as brutal as it sounds, but pacing is key.\n\n**Focus:**Explosiveness and endurance. **Tip:**Land softly, keep jumps consistent, and rest in the standing position if you need to catch your breath. **Common mistake:**Jumping too far and burning out early.\n\n#### 5. Row (1,000 meters)\n\nA cardio reset that still taxes your legs and core.\n\n**Focus:**Efficient stroke rhythm, don\u2019t rush it. **Tip:**Drive with your legs first, then hinge back, then pull with your arms. **Common mistake:**Over-pulling or neglecting leg drive.\n\n#### 6. Farmer\u2019s Carry (200 meters)\n\nSimple but deceiving. Grip and core control are everything.\n\n**Focus:**Stability and breathing under load. **Tip:**Keep shoulders back, walk tall, and take measured steps. **Common mistake:**Leaning or swinging weights, which drains your grip faster.\n\n#### 7. Sandbag Lunges (100 meters)\n\nYour legs will hate you here, but it\u2019s almost the end.\n\n**Focus:**Balance, endurance, and leg power. **Tip:**Keep your core tight and knees at 90 degrees with every step. **Common mistake:**Short steps or skipping the knee touch, can cost penalties.\n\n#### 8. Wall Balls (75 reps for men / 50 for women)\n\nThe grand finale, your heart rate spikes, but the crowd carries you through.\n\n**Focus:**Coordination and consistent breathing. **Tip:**Stay calm, use leg power, and aim for smooth sets of 10\u201315 reps. **Common mistake:**Dropping the ball too often, small, steady sets beat big burnout sets.\n\nSource: Unsplash\n\n## How long does HYROX take?\n\nOne of the most common questions beginners ask is:\n\n*\u201cHow long will it take me to finish HYROX?\u201d*\n\nThe short answer, somewhere between\n\n**60 and 120 minutes**, depending on your fitness level, pacing, and category.\n\nBut let\u2019s break it down a little more clearly.\n\n#### Average finishing times\n\n**Elite athletes (Pro Men/Women):**55\u201375 minutes **Intermediate athletes (Open):**80\u2013100 minutes **First-timers:**100\u2013120 minutes\n\nRemember: there\u2019s no time limit. You can take as long as you need to finish, and the crowd will still cheer just as loud when you cross that line. HYROX is about personal challenge, not just competition.\n\n#### Typical time breakdown\n\n#### \u201cGood\u201d benchmark goals\n\nIf you\u2019re training for your first HYROX and want a rough benchmark:\n\n**Under 100 mins:**Strong first-timer, well-balanced fitness. **Around 110 mins:**Solid finish with steady pacing. **Under 90 mins:**You\u2019ve officially caught the HYROX bug, and trained well!\n\n#### Pacing strategy\n\nThe biggest mistake beginners make? Starting too fast.\n\nThat first 1 km run feels easy, but it\u2019s a trap. Aim to finish your first two runs feeling like you could go faster, not gasping for air. That controlled start will help you maintain energy for the final three stations, where most people hit the wall.\n\n**Tip:** Treat the run segments as active recovery. Slow down slightly between heavy stations, regulate your breathing, and save your push for the final two kilometers.\n\n## Race-day checklist\n\nDial in the simple stuff so you can focus on moving well. Use this list the night before.\n\n#### Must-bring items\n\n- Photo ID, registration QR, waiver\n\n- Race wristband if collected at packet pick-up\n\n- Shoes with good indoor grip, spare laces\n\n- Socks (bring a backup pair)\n\n- Outfit: top, shorts/tights, sports bra (for women)\n\n- Anti-chafe stick, tape for hot spots\n\n- Small towel\n\n- Electrolytes, bottle, light carbs (bananas, gels, chews)\n\n- Post-race snack, recovery drink\n\n- Cash/card for venue food and merch\n\n#### Nice-to-have gear\n\n- Knee sleeves for lunges\n\n- Thin gloves for sled pull rope\n\n- Wrist strap for timing chip if provided\n\n- Portable massage ball or mini band\n\n#### What to wear\n\n- Lightweight, breathable layers\n\n- Secure pockets or race belt for gels\n\n- Shoes with stable midsole and good forefoot traction\n\n#### Warm-up (10\u201315 minutes)\n\n- Easy jog or brisk walk 5 minutes\n\n- Mobility: hips, calves, thoracic spine\n\n- Activation: bodyweight squats, lunges, plank 30 seconds\n\n- Two short strides or SkiErg/rower pickups at comfortable effort\n\n#### Pre-start fueling\n\n- Night before: normal dinner with extra carbs and water\n\n- 2\u20133 hours before: simple carb-heavy meal you trust\n\n- 15\u201320 minutes before: sip electrolytes, optional small carb bite\n\n#### Timing at the venue\n\n- Arrive 60\u201390 minutes before your wave\n\n- Bag drop and restroom first\n\n- Walk the Roxzone entry and exit paths\n\n- Locate start pen early\n\n#### Roxzone habits that save minutes\n\n- Keep moving with small steps, breathe through the nose on entry\n\n- Set equipment once, avoid re-adjusting mid-station\n\n- Exit decisively, settle into run pace in the first 50\u2013100 meters\n\n#### Mental checklist before the gun\n\n- First 2 km controlled pace\n\n- Smooth reps over big sets\n\n- Sip fluids between stations as needed\n\n- One cue per station: posture, breath, rhythm\n\nPack it all the night before, lay out your outfit, and set two alarms. Calm morning, clean execution.\n\n## How to pace your first HYROX\n\nPacing can make or break your HYROX race. Go out too fast, and you\u2019ll hit the wall by station three. Go too slow, and you\u2019ll finish with too much left in the tank. The key is finding that middle ground: steady, controlled effort from start to finish.\n\n#### 1. Start slower than you think\n\nThe first 1 km run always feels easy, especially with adrenaline and music pumping. But don\u2019t get carried away. Keep your effort around\n\n**70\u201375%** of what feels \u201chard.\u201d You should be breathing steadily, not gasping. Remember, you\u2019ve got eight rounds to go.\n\n#### 2. Use the runs as active recovery\n\nHYROX isn\u2019t eight sprints, it\u2019s a rhythm race. The running sections are your chance to bring your heart rate down between stations. Use shorter, efficient strides and relax your shoulders. Think of it as your reset before the next big effort.\n\n#### 3. Control your transitions\n\nThe\n\n**Roxzone** (your transition area) is where many athletes lose time. Don\u2019t stop to rest here. Slow down your walk, grab your equipment smoothly, and start the movement. Aim for **no more than 10\u201315 seconds** between run and station.\n\n#### 4. Station strategy\n\nEach workout station has its own pacing trick:\n\n**SkiErg / Row:**Keep strokes smooth and steady. Save power for the finish. **Sled Push / Pull:**Use short bursts, then rest 3\u20135 seconds instead of grinding nonstop. **Burpees / Wall Balls:**Break reps into manageable sets early\u201410s or 15s\u2014and stick to your plan. **Farmer\u2019s Carry / Lunges:**Focus on posture and breathing; these stations are about control, not speed.\n\n#### 5. Negative split your race\n\nIf you can, aim to finish stronger than you started. That means holding back in the first half and gradually increasing pace from station five onward. It\u2019s far more rewarding to overtake people later than to get passed in the final stretch.\n\n#### 6. Watch the clock, but don\u2019t chase it\n\nUse your watch or on-site timer to stay aware of your total time, but don\u2019t obsess. HYROX is about rhythm and flow. Stick to your strategy, manage your energy, and you\u2019ll likely finish faster than expected.\n\n## Common rookie mistakes (and how to fix them)\n\nEvery HYROX athlete makes mistakes at their first race, it\u2019s practically a rite of passage. But the good news? Most of them are easy to avoid once you know what to look out for. Here are the most common slip-ups beginners make and how to fix them fast.\n\n#### 1. Starting too fast\n\n**The mistake:** Sprinting the first run like it\u2019s a 5K.\n\n**The fix:** Treat the first 2 km as a warm-up. Keep your breathing under control and aim for a pace you can maintain for 90+ minutes. You\u2019ll thank yourself later.\n\n#### 2. Ignoring transitions\n\n**The mistake:** Standing still between runs and stations to \u201ccatch your breath.\u201d\n\n**The fix:** Keep moving. Slow your walk, focus on deep breathing, and start the next movement before your mind talks you out of it. Time disappears quickly in the Roxzone.\n\n#### 3. Poor sled technique\n\n**The mistake:** Standing upright and pushing with arms only, or jerking the rope on the pull.\n\n**The fix:** Stay low, drive from your legs, and maintain constant pressure on the push. On the pull, use a smooth hand-over-hand motion while leaning slightly back.\n\n#### 4. Skipping recovery and hydration\n\n**The mistake:** Not drinking enough or relying solely on adrenaline.\n\n**The fix:** Sip electrolytes before and during the event. A small carb snack 30\u201345 minutes before your heat keeps energy steady through the later stations.\n\n#### 5. Overestimating wall balls\n\n**The mistake:** Trying to hit all 75 (or 50) reps unbroken.\n\n**The fix:** Break early and strategically. Sets of 10\u201315 reps with short pauses keep your form clean and prevent burnout. Smooth and steady wins here.\n\n#### 6. Wearing the wrong shoes\n\n**The mistake:** Showing up in thick-soled running shoes with no grip.\n\n**The fix:** Choose a stable, low-profile trainer with good traction on indoor flooring. Slipping on sleds or lunges can cost time and confidence.\n\n#### 7. Neglecting strength in training\n\n**The mistake:** Only running to prepare for HYROX.\n\n**The fix:** Mix in functional strength days: lunges, sled pushes, carries. The race rewards balanced conditioning, not just cardio.\n\n#### 8. Forgetting to enjoy it\n\n**The mistake:** Treating HYROX like a punishment instead of an event.\n\n**The fix:** Look up, smile, and soak in the energy. The crowd, music, and community vibe are part of the reason people keep coming back.\n\n## Gear that actually helps\n\nYou don\u2019t need fancy gear to crush your first HYROX, but a few smart choices can make the experience way smoother (and save you from painful blisters or wasted energy).\n\nHere\u2019s what really matters:\n\n#### 1. Shoes: Grip over gimmicks\n\nYour shoes can make or break your sled stations. You\u2019ll be on polished indoor floors, not outdoor tracks, so\n\n**traction and stability** are everything.\n\n\u2705 Look for flat, firm soles with good grip.\n\n\u2705 Avoid squishy running shoes, they absorb force instead of transferring it.\n\n\u2705 Training shoes like cross-trainers or hybrid running/strength models usually perform best.\n\n**Tip:** Try your shoes on wet or smooth flooring before race day. If you slip while pushing a sled, it\u2019ll cost time and energy.\n\n#### 2. Gloves: Optional but useful\n\nSome athletes swear by gloves for the sled pull and farmer\u2019s carry, others hate them.\n\n- Use\n\n**thin grip gloves**or chalk if you tend to get sweaty hands.\n\n- Skip thick gloves, they\u2019ll make gripping the rope harder.\n\n- Test them during training so there are no surprises on race day.\n\n#### 3. Knee sleeves or pads\n\nLunges and burpees can get tough on hard floors. A light pair of\n\n**neoprene knee sleeves** adds support and protects from bruising.\n\n- Choose flexible, breathable sleeves, not powerlifting-style thick ones.\n\n- Bonus: they keep your knees warm, helping mobility early in the race.\n\n#### 4. Apparel\n\nYou\u2019ll sweat, a lot, so pick\n\n**moisture-wicking fabrics**.\n\n- Tank or fitted top: avoids fabric snagging on sleds.\n\n- Compression shorts or leggings: reduce chafing and keep things secure.\n\n- Sports bra with strong support (for women).\n\n- Avoid cotton, it gets heavy and stays wet.\n\n#### 5. Accessories\n\n**Timing chip strap:**make sure it\u2019s secure before the start. **Small towel:**for quick wipes in transition areas. **Lightweight belt or pocket:**to hold gels or small fuel if needed. **Water bottle or electrolyte pack:**optional if the venue provides hydration zones, but always good to have.\n\n## Gear that actually helps\n\nBefore you step onto the starting line, it helps to know the key terms you\u2019ll hear tossed around at HYROX events and in training videos. Here\u2019s a quick glossary to get you sounding (and feeling) like a pro:\n\n#### 1. Roxzone\n\nThe central transition area that connects your runs to each workout station. You\u2019ll enter and exit the Roxzone eight times during the race. Smooth transitions here can shave off minutes from your total time.\n\n#### 2. YGIG (You Go, I Go)\n\nThe rule used in\n\n*Doubles* races. One partner works while the other rests, and you can switch anytime. The strategy is to divide the work efficiently and communicate clearly.\n\n#### 3. Open\n\nThe standard HYROX category designed for most participants. Weights and distances are balanced for accessibility, making it perfect for first-timers.\n\n#### 4. Pro\n\nThe advanced division featuring heavier weights and faster pacing. Usually chosen by experienced or competitive athletes aiming for top rankings.\n\n#### 5. Doubles\n\nA two-person race where both partners run every kilometer together but split the workout stations under the YGIG rule. Great for couples, training buddies, or anyone who wants to share the load.\n\n#### 6. Relay\n\nA four-person team format. Each athlete completes two runs and two workout stations before tagging the next teammate. It\u2019s fast, fun, and loud, ideal for groups or corporate teams.\n\n#### 7. Wall Balls\n\nThe final station of every HYROX. You\u2019ll squat with a medicine ball and throw it to a target on the wall\u201475 reps for men, 50 for women. It\u2019s a mental battle as much as a physical one.\n\n#### 8. SkiErg / RowErg\n\nThe cardio machines used in two of the eight stations. Both mimic endurance movements\u2014skiing and rowing\u2014that test upper-body stamina and coordination.\n\n#### 9. Functional Fitness\n\nThe core principle of HYROX. It means exercises that mimic real-life movements (pushing, pulling, carrying, and lunging) to build practical, usable strength.\n\n#### 10. Transition Time\n\nThe seconds between finishing a run and starting the next workout. Efficient transitions can save up to 3\u20135 minutes over the course of a race.\n\n## Conclusion\n\nIf you\u2019ve made it this far, you\u2019re already halfway to the start line. HYROX might look intimidating from the outside, but once you understand the format and break your training into simple, consistent steps, it becomes something incredible, a challenge that\u2019s equal parts physical grind and community celebration.\n\nYour first race isn\u2019t about speed or rankings; it\u2019s about finishing strong and learning how your body responds under pressure. Every run, every sled push, every wall ball teaches you something about pacing, resilience, and mental grit.\n\nSo here\u2019s your action plan:\n\n- Pick a HYROX category that feels right (hint:\n\n*Open*or *Doubles*are perfect starts).\n\n- Follow the 4-week plan above and practice transitions whenever you can.\n\n- Show up on race day with the mindset to\n\n*enjoy the experience*, not just survive it.\n\nAnd once you cross that finish line, no matter your time, you\u2019ll understand why so many\n\npeople call HYROX\n\n*addictive*. It\u2019s you versus you, but surrounded by a community that pushes you to keep going.\n\nReady to take the leap?\n\nCheck the official HYROX race calendar, find an event near you, and start training. Your first finish line is waiting.\n\n",
          "title": "HYROX beginner guide: Format, stations, times, and first-race tips ...",
          "url": "https://www.rezerv.co/blogs/hyrox-beginner-guide-format-stations-times-and-first-race-tips-2026"
        },
        {
          "snippet": "You can also upload and deploy LoRA models fine-tuned outside of Fireworks. See importing fine-tuned models for details.\n\n## Single-LoRA deploymentDeploy your LoRA fine-tuned model with a single command that delivers performance matching the base model. This streamlined approach, called live merge, eliminates the previous two-step process and provides better performance compared to multi-LoRA deployments.\n\n### Quick deploymentDeploy your LoRA fine-tuned model with one simple command:\n\n```\n\nfirectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\n```\n\nYour deployment will be ready to use once it completes, with performance that matches the base model.\n\n## Multi-LoRA deploymentIf you have multiple fine-tuned versions of the same base model (e.g., you\u2019ve fine-tuned the same model for different use cases, applications, or prototyping), you can share a single base model deployment across these LoRA models to achieve higher utilization.\n\n### Deploy with CLI\n\n1\n\nCreate base model deployment\n\nDeploy the base model with addons enabled:\n\n```\n\nfirectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n```\n\n2\n\nLoad LoRA addons\n\nOnce the deployment is ready, load your LoRA models onto the deployment:\n\nYou can load multiple LoRA models onto the same deployment by repeating this command with different model IDs.\n\n```\n\nfirectl load-lora <FINE_TUNED_MODEL_ID> --deployment <DEPLOYMENT_ID>\n\n```\n\n### When to use multi-LoRA deploymentUse multi-LoRA deployment when you:\n\n- Need to serve multiple fine-tuned models based on the same base model\n\n- Want to maximize deployment utilization\n\n- Can accept some performance tradeoff compared to single-LoRA deployment\n\n- Are managing multiple variants or experiments of the same model\n\n",
          "title": "Deploying Fine Tuned Models - Fireworks AI Docs",
          "url": "https://docs.fireworks.ai/fine-tuning/deploying-loras"
        }
      ],
      "type": "fetch_url_results"
    },
    {
      "queries": [
        "text-embedding-3-small reduced dimensions 512 768 performance tradeoff",
        "Supabase realtime subscriptions workout logging pattern"
      ],
      "results": [
        {
          "date": "2022-01-01",
          "id": 1,
          "last_updated": "2025-08-29T14:16:56.800466",
          "snippet": "",
          "source": "web",
          "title": "Vector embeddings - OpenAI API",
          "url": "https://platform.openai.com/docs/guides/embeddings"
        },
        {
          "date": "2026-02-12",
          "id": 2,
          "last_updated": "2026-02-13T10:33:21",
          "snippet": "",
          "source": "web",
          "title": "Subscribing to Database Changes | Supabase Docs",
          "url": "https://supabase.com/docs/guides/realtime/subscribing-to-database-changes"
        },
        {
          "date": "2025-11-03",
          "id": 3,
          "last_updated": "2025-11-20T14:00:52.414196",
          "snippet": "# text-embedding-3-small: High-Quality Embeddings at Scale\n\nOpenAI pulled off an impressive feat: they made embeddings both better AND 5\u00d7 cheaper, a model that outperforms its predecessor by 13% while costing just $0.02 per million tokens. This breakthrough, known as text-embedding-3-small, transforms text into 1536-dimensional vectors for semantic search, clustering, and RAG applications, an exponential increase in cost-efficiency for production AI systems that makes high-quality embeddings accessible at scale.\n\n\u2026\n\n## The Leap from ada-002 to 3-small\n\nMoving from\n\n__ada-002__ to text-embedding-3-small introduced a new economic reality for embeddings:\n\n- On multilingual retrieval tasks\n\n**(** **), performance jumped from 31.4% to 44.0%, a remarkable 13-point gain that makes the model significantly more effective for global applications** __MIRACL benchmark__\n\n\u2026\n\nThis flexibility offers practical benefits for real-world applications. Need faster similarity searches? Request 512-dimensional embeddings instead of the full 1536. Running low on vector database storage? Trim to 1024 dimensions while maintaining competitive accuracy. The remarkable aspect is that even a trimmed 1024D embedding from the large model beats ada-002's full output, demonstrating the superiority of the new architecture. This feature enables developers to dynamically balance accuracy, speed, and storage costs based on their specific use case requirements.\n\n\u2026\n\nHowever, trade-offs exist. For mission-critical applications demanding absolute best accuracy, text-embedding-3-large (3072D) offers marginally better performance at $0.13 per million tokens, still cheaper than the old ada-002. The large model scores 54.9% on MIRACL versus 3-small's 44.0%, which might justify the 6.5\u00d7 price increase for applications where every percentage point matters.\n\n\u2026\n\n## Efficiency Meets Performance\n\nText-embedding-3-small proves that AI progress doesn't require choosing between quality and cost. With 13-point gains on multilingual tasks and 80% lower pricing, it delivers what was once a pipe dream: genuinely better results for dramatically less money. The model's flexible dimensions and broad integration support make it production-ready out of the box.",
          "source": "web",
          "title": "text-embedding-3-small: High-Quality Embeddings at Scale",
          "url": "https://blog.promptlayer.com/text-embedding-3-small-high-quality-embeddings-at-scale/"
        },
        {
          "date": "2026-02-13",
          "id": 4,
          "last_updated": "2026-02-13T11:10:13",
          "snippet": "Realtime\n\nSend and receive messages to connected clients.\n\nSupabase provides a globally distributed Realtime service with the following features:\n\n- Broadcast: Send low-latency messages between clients. Perfect for real-time messaging, database changes, cursor tracking, game events, and custom notifications.\n\n- Presence: Track and synchronize user state across clients. Ideal for showing who's online, or active participants.\n\n- Postgres Changes: Listen to database changes in real-time.",
          "source": "web",
          "title": "Realtime | Supabase Docs",
          "url": "https://supabase.com/docs/guides/realtime"
        },
        {
          "id": 5,
          "last_updated": "2025-08-29T11:35:38.631131",
          "snippet": "",
          "source": "web",
          "title": "Text Embedding 3 Small - OpenAI Platform",
          "url": "https://platform.openai.com/docs/models/text-embedding-3-small"
        },
        {
          "date": "2025-08-05",
          "id": 6,
          "last_updated": "2026-02-13T09:49:31",
          "snippet": "## Setting Up Realtime Subscriptions\n\nImplementing realtime listeners follows a straightforward pattern:\n\n```\n\nconst subscription = supabase\n\n.channel('room1')\n\n.on(\n\n'postgres_changes',\n\n\n\nevent: 'INSERT',\n\nschema: 'public',\n\ntable: 'messages'\n\n},\n\n(payload) => console.log('New message:', payload.new)\n\n\n\n.subscribe()\n\n// Later, to unsubscribe\n\nsupabase.removeChannel(subscription)\n\n```\n\nFor more complex filtering:\n\n```\n\n.on('postgres_changes', {\n\nevent: 'UPDATE',\n\nschema: 'public',\n\ntable: 'inventory',\n\nfilter: 'item_id=eq.42'\n\n}, callback)\n\n```\n\n\u2026\n\nA: While Chat2DB doesn't directly show active subscriptions, its schema analysis and query monitoring help optimize the underlying tables driving realtime updates.\n\n**Q: What's the best pattern for handling initial data load with realtime updates?**\n\nA: First query the current state with a regular SELECT, then subscribe to changes to avoid missing updates between load and subscription.",
          "source": "web",
          "title": "Faq",
          "url": "https://chat2db.ai/resources/blog/supabase-realtime-guide"
        },
        {
          "date": "2026-01-08",
          "id": 7,
          "last_updated": "2026-02-12T16:47:40",
          "snippet": "",
          "source": "web",
          "title": "OpenAI's Text Embeddings v3 - Pinecone",
          "url": "https://www.pinecone.io/learn/openai-embeddings-v3/"
        },
        {
          "date": "2024-03-13",
          "id": 8,
          "last_updated": "2025-10-01T20:28:13.212499",
          "snippet": "",
          "source": "web",
          "title": "Best Practices or Patterns for implementing Realtime ...",
          "url": "https://github.com/orgs/supabase/discussions/21995"
        },
        {
          "date": "2025-12-22",
          "id": 9,
          "last_updated": "2025-12-23T03:18:59.885655",
          "snippet": "",
          "source": "web",
          "title": "How Many Dimensions Should Your Embeddings Have?",
          "url": "https://particula.tech/blog/embedding-dimensions-rag-vector-search"
        },
        {
          "date": "2025-12-22",
          "id": 10,
          "last_updated": "2026-01-14T22:04:06.522905",
          "snippet": "## 1. Understand Supabase Real-Time Subscriptions\n\nSupabase uses WebSockets to push database changes to clients in real time, enabling instant updates when data is inserted, updated, or deleted.\n\n**Best Practices:**\n\n\u2714 Enable real-time on specific tables in the Supabase dashboard to optimize performance.\n\n\u2714 Use selective event triggers (INSERT, UPDATE, DELETE) to reduce unnecessary broadcasts.\n\n\u2714 Test subscription latency in your target regions to ensure a smooth user experience.\n\n\u2026\n\n## 3. Subscribe to Real-Time Changes\n\nUse Supabase\u2019s on() method to listen for database changes and update your UI dynamically.\n\n**Example: Listening for new messages in a chat table:**\n\n**Best Practices:**\n\n\u2714 Unsubscribe from channels when components unmount to avoid memory leaks.\n\n\u2714 Handle payload errors gracefully to maintain a stable app.\n\n\u2714 Use filters to subscribe only to relevant data (e.g., specific user IDs).",
          "source": "web",
          "title": "Building Real-Time Apps with Supabase: A Step-by- ...",
          "url": "https://www.supadex.app/blog/building-real-time-apps-with-supabase-a-step-by-step-guide"
        },
        {
          "date": "2025-06-22",
          "id": 11,
          "last_updated": "2026-02-13T02:10:49",
          "snippet": "This model boost larger network and fixed embedding sizes, for example, 2,000 or 4,000\n{ts:342} dimensions, plus a context window up to 16,000 tokens. It's roughly three times the cost of three small but delivers 67\n{ts:353} 70% average score on the MTB benchmark outperforming smaller varants and even other point too.\n\n\u2026\n\nRemember, small\n{ts:389} vectors are cheaper and faster. Larger ones capture richer semantics. Let's look at two demos. For example, a\n{ts:398} product search that uses three small embedding. Imagine a reteller uses custom 512dimensional\n{ts:407} embeddings to power a lightning fast semantic search across thousands of squs delivering accurate results under 15",
          "source": "web",
          "title": "OpenAI Embeddings Explained: text-embedding-3 Small vs Large (with Code & Benchmarks)",
          "url": "https://www.youtube.com/watch?v=zIkHOrbpbCY"
        },
        {
          "date": "2025-06-01",
          "id": 12,
          "last_updated": "2026-02-02T09:16:43.057686",
          "snippet": "## SkillBakery Studio\n\n##### Jun 01, 2025 (0:06:02)\nIn this tutorial, you'll learn how to subscribe to realtime changes in your Supabase database. Watch your frontend react instantly as your data updates in real time \u2014 no need to refresh! Whether you're building a chat app, a dashboard, or collaborative tools, Supabase Realtime makes it easy to keep your UI in sync with your backend.\n\u2705 What you'll learn:\n\nSetting up Supabase Realtime\nSubscribing to table changes (INSERT, UPDATE, DELETE)\nUsing supabase-js for live data updates\nBuilding reactive apps with Supabase\n\n\ud83d\udca1 Perfect for developers building realtime experiences in web and mobile apps!\n\n\ud83d\udcac Have any use-cases in mind? Drop them in the comments!\n\ud83d\udc4d Like, Subscribe & Hit the Bell \ud83d\udd14 for more dev content.\n\n\u2026\n\n{ts:216} ensures  the  local  UI  reflects  any \n updates  from  SSR  or  loader  data.  After\n{ts:220} \n that  we  have  the  next  use  effect  which \n\n{ts:223} listens  for  realtime  inserts.  And  here \n you  will  notice  that  we  are  creating  a\n{ts:229} \n realtime  subscription  to  listen  for  new",
          "source": "web",
          "title": "10-Realtime Database Updates with Supabase | Live Subscriptions Tutorial",
          "url": "https://www.youtube.com/watch?v=F7F71ng9JYM"
        },
        {
          "date": "2024-12-17",
          "id": 13,
          "last_updated": "2026-02-12T10:49:56",
          "snippet": "as you can notice the average performance is very similar and it is interesting to notice that the\n\n*text-embedding-3-large* can be used to return only 256 dimensions instead of the default 3072, while still performing very well. Great performance with 1/12 of the resource usage. That\u2019s quite a great deal!\n\n\u2026\n\n*text-embedding-3-large* model as with 4Kb of space (1024 dimensions each one using 4 bytes to store single-precision float value) as it will give pretty much the same performance of using 3072 dimensions that will instead use 12Kb of space. More importantly calculating dot product or, even more, the cosine distance, will require way less computation, which in turns means less CPU usage and less power drain for still practically the same results, as shown by this chart that is also referenced in the OpenAI article mentioned before:",
          "source": "web",
          "title": "Choose the right dimension count for your embedding models",
          "url": "https://devblogs.microsoft.com/azure-sql/embedding-models-and-dimensions-optimizing-the-performance-resource-usage-ratio/"
        },
        {
          "id": 14,
          "last_updated": "2026-02-09T09:49:41",
          "snippet": "",
          "source": "web",
          "title": "Realtime - Supaboost",
          "url": "https://www.supaboost.dev/docs/installation/supabase-realtime"
        },
        {
          "date": "2024-07-25",
          "id": 15,
          "last_updated": "2026-02-13T03:17:33",
          "snippet": "",
          "source": "web",
          "title": "The guide to text-embedding-3-small | OpenAI - Zilliz",
          "url": "https://zilliz.com/ai-models/text-embedding-3-small"
        }
      ],
      "type": "search_results"
    },
    {
      "content": [
        {
          "text": "Now I have enough information to write a comprehensive technical document. Let me compile everything.\n\n---\n\n# RAG-Powered Hyrox Coaching System \u2014 Full Architecture & Implementation Guide\n\n## Table of Contents\n1. [System Architecture Overview](#1-system-architecture-overview)\n2. [PGVector Optimization for RAG](#2-pgvector-optimization-for-rag)\n3. [Supabase Schema & RAG Patterns](#3-supabase-schema--rag-patterns)\n4. [Fireworks AI / Kimi K2 Integration](#4-fireworks-ai--kimi-k2-integration)\n5. [Coaching Conversation Design](#5-coaching-conversation-design)\n6. [Training Data Collection for Fine-Tuning](#6-training-data-collection-for-fine-tuning)\n7. [End-to-End Walkthrough](#7-end-to-end-walkthrough)\n\n---\n\n## 1. System Architecture Overview\n\n### High-Level Data Flow\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        MOBILE CLIENT                             \u2502\n\u2502  (React Native / Flutter \u2014 \"Coach in Your Pocket\")               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502  Supabase Auth JWT                    \u2502  Realtime WS\n           \u25bc                                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    SUPABASE PLATFORM                              \u2502\n\u2502                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Edge Function   \u2502  \u2502  Edge Function   \u2502  \u2502   Realtime      \u2502  \u2502\n\u2502  \u2502  /chat           \u2502  \u2502  /ingest         \u2502  \u2502   (postgres_    \u2502  \u2502\n\u2502  \u2502                  \u2502  \u2502                  \u2502  \u2502    changes)     \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502                     \u2502                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              SUPABASE POSTGRESQL + PGVECTOR                 \u2502  \u2502\n\u2502  \u2502                                                             \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502  \u2502\n\u2502  \u2502  \u2502 athletes   \u2502 \u2502 conversations\u2502 \u2502 knowledge_chunks   \u2502    \u2502  \u2502\n\u2502  \u2502  \u2502 profiles   \u2502 \u2502 messages     \u2502 \u2502 (vector + FTS)     \u2502    \u2502  \u2502\n\u2502  \u2502  \u2502 workouts   \u2502 \u2502 sessions     \u2502 \u2502                    \u2502    \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                                       \u2502\n           \u25bc                                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   OpenAI API        \u2502              \u2502   Fireworks AI API        \u2502\n\u2502   text-embedding-   \u2502              \u2502   Kimi K2 Thinking        \u2502\n\u2502   3-small           \u2502              \u2502   (+ LoRA Phase 2)        \u2502\n\u2502   (embeddings only) \u2502              \u2502   accounts/fireworks/     \u2502\n\u2502                     \u2502              \u2502   models/kimi-k2-         \u2502\n\u2502                     \u2502              \u2502   instruct-0905           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Request Flow: User Message \u2192 Coach Response\n\n```\nUser sends message\n       \u2502\n       \u25bc\n\u250c\u2500 Edge Function /chat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Validate JWT via Supabase Auth                          \u2502\n\u2502  2. Load athlete profile (RPC \u2192 profiles table)             \u2502\n\u2502  3. Load conversation memory (last N messages + summary)    \u2502\n\u2502  4. Generate embedding via OpenAI text-embedding-3-small    \u2502\n\u2502  5. Hybrid search: semantic + FTS \u2192 knowledge_chunks        \u2502\n\u2502  6. Re-rank retrieved chunks (cross-encoder or RRF)         \u2502\n\u2502  7. Assemble prompt: system + profile + memory + context    \u2502\n\u2502  8. Stream to Fireworks AI Kimi K2 Thinking                 \u2502\n\u2502  9. Persist user msg + assistant msg to messages table       \u2502\n\u2502 10. If structured output detected \u2192 persist to workouts     \u2502\n\u2502 11. Stream response tokens back to client                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## 2. PGVector Optimization for RAG\n\n### 2.1 Chunking Strategies for Coaching/Fitness Content\n\nHyrox coaching content is heterogeneous: race rules, training plans, exercise technique guides, nutrition advice, pacing tables, and athlete Q&A. A single chunking strategy won't work.\n\n| Content Type | Chunking Strategy | Chunk Size | Overlap | Rationale |\n|---|---|---|---|---|\n| Training plans (weekly blocks) | **Hierarchical** \u2014 section \u2192 day \u2192 exercise | 400\u2013600 tokens | 50 tokens | Preserve plan structure; each day is self-contained |\n| Exercise technique guides | **Semantic** \u2014 split on topic shifts | 300\u2013500 tokens | 75 tokens | Keep setup, cues, and common mistakes together |\n| Race rules / station specs | **Document-based** \u2014 one chunk per station | 200\u2013400 tokens | 0 | Each station is a standalone lookup unit |\n| Nutrition / recovery articles | **Recursive character** with paragraph boundaries | 500\u2013800 tokens | 100 tokens | Longer narrative; preserve paragraph coherence |\n| FAQ / Q&A pairs | **Question-answer pairs** \u2014 one chunk per pair | 100\u2013300 tokens | 0 | Each Q&A is an atomic retrieval unit |\n| Pacing tables / data | **Table-aware** \u2014 keep full table in one chunk | Variable | 0 | Tables lose meaning when split |\n\n**Implementation \u2014 Python chunking pipeline:**\n\n```python\n# chunking.py\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import List\nimport tiktoken\n\nclass ContentType(Enum):\n    TRAINING_PLAN = \"training_plan\"\n    TECHNIQUE_GUIDE = \"technique_guide\"\n    RACE_RULES = \"race_rules\"\n    NUTRITION = \"nutrition\"\n    FAQ = \"faq\"\n    PACING_TABLE = \"pacing_table\"\n\n@dataclass\nclass Chunk:\n    content: str\n    content_type: str\n    source_document: str\n    section_title: str\n    metadata: dict  # e.g., {\"station\": \"sled_push\", \"division\": \"open\"}\n    token_count: int\n\nENCODING = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n\ndef count_tokens(text: str) -> int:\n    return len(ENCODING.encode(text))\n\ndef recursive_split(\n    text: str,\n    max_tokens: int = 500,\n    overlap_tokens: int = 75,\n    separators: list = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n) -> List[str]:\n    \"\"\"Recursively split text respecting natural boundaries.\"\"\"\n    if count_tokens(text) <= max_tokens:\n        return [text.strip()] if text.strip() else []\n\n    sep = separators[0] if separators else \" \"\n    parts = text.split(sep)\n\n    chunks = []\n    current = \"\"\n    for part in parts:\n        candidate = current + sep + part if current else part\n        if count_tokens(candidate) > max_tokens:\n            if current:\n                chunks.append(current.strip())\n                # Create overlap by keeping tail of current chunk\n                overlap_text = _get_tail(current, overlap_tokens)\n                current = overlap_text + sep + part if overlap_text else part\n            else:\n                # Single part exceeds limit; try next separator\n                if len(separators) > 1:\n                    sub_chunks = recursive_split(part, max_tokens, overlap_tokens, separators[1:])\n                    chunks.extend(sub_chunks)\n                    current = \"\"\n                else:\n                    chunks.append(part[:max_tokens * 4].strip())  # Hard truncate\n                    current = \"\"\n        else:\n            current = candidate\n    if current.strip():\n        chunks.append(current.strip())\n    return chunks\n\ndef _get_tail(text: str, target_tokens: int) -> str:\n    \"\"\"Get the last N tokens of text for overlap.\"\"\"\n    tokens = ENCODING.encode(text)\n    if len(tokens) <= target_tokens:\n        return text\n    return ENCODING.decode(tokens[-target_tokens:])\n\ndef chunk_document(\n    text: str,\n    content_type: ContentType,\n    source: str,\n    section: str = \"\",\n    metadata: dict = None\n) -> List[Chunk]:\n    \"\"\"Route to appropriate chunking strategy based on content type.\"\"\"\n    metadata = metadata or {}\n    config = {\n        ContentType.TRAINING_PLAN:   {\"max_tokens\": 500, \"overlap\": 50},\n        ContentType.TECHNIQUE_GUIDE: {\"max_tokens\": 400, \"overlap\": 75},\n        ContentType.RACE_RULES:      {\"max_tokens\": 300, \"overlap\": 0},\n        ContentType.NUTRITION:       {\"max_tokens\": 600, \"overlap\": 100},\n        ContentType.FAQ:             {\"max_tokens\": 250, \"overlap\": 0},\n        ContentType.PACING_TABLE:    {\"max_tokens\": 800, \"overlap\": 0},\n    }\n    cfg = config[content_type]\n    raw_chunks = recursive_split(text, cfg[\"max_tokens\"], cfg[\"overlap\"])\n\n    return [\n        Chunk(\n            content=c,\n            content_type=content_type.value,\n            source_document=source,\n            section_title=section,\n            metadata=metadata,\n            token_count=count_tokens(c),\n        )\n        for c in raw_chunks if c.strip()\n    ]\n```\n\n**Reference:** Weaviate's chunking strategies guide recommends hierarchical chunking for structured documents and semantic chunking for narrative content ([source](https://weaviate.io/blog/chunking-strategies-for-rag)). Unstructured.io's \"by title\" strategy preserves section boundaries ([source](https://unstructured.io/blog/chunking-for-rag-best-practices)).\n\n### 2.2 Optimal Embedding Dimensions\n\n**Model:** `text-embedding-3-small` supports Matryoshka-style dimension reduction. The native output is **1536 dimensions**, but you can request 256, 512, 1024, or 1536 at generation time.\n\n| Dimensions | Storage/Vector | HNSW Index RAM | Retrieval Quality (MTEB avg) | Recommendation |\n|---|---|---|---|---|\n| 256 | 1 KB | Low | ~90% of full | Too lossy for nuanced coaching queries |\n| 512 | 2 KB | Medium | ~95% of full | Good for high-volume, cost-sensitive |\n| **1024** | **4 KB** | **Medium-High** | **~98% of full** | **Best balance for our use case** |\n| 1536 | 6 KB | High | 100% (baseline) | Marginal gain over 1024 not worth 50% more storage |\n\n**Recommendation: Use 1024 dimensions.** Fitness coaching queries are domain-specific and nuanced (e.g., \"my sled push slows down after station 5\" vs \"sled push technique\"). We need high semantic resolution but can sacrifice the last 2% for a 33% storage reduction. OpenAI's Matryoshka architecture preserves semantic ordering in early dimensions ([source](https://particula.tech/blog/embedding-dimensions-rag-vector-search)).\n\n```python\n# Generate embedding with reduced dimensions\nimport openai\n\nclient = openai.OpenAI()\n\ndef generate_embedding(text: str, dimensions: int = 1024) -> list[float]:\n    response = client.embeddings.create(\n        model=\"text-embedding-3-small\",\n        input=text,\n        dimensions=dimensions\n    )\n    return response.data[0].embedding\n```\n\n### 2.3 HNSW vs IVFFlat Index Selection\n\n| Factor | HNSW | IVFFlat |\n|---|---|---|\n| **Algorithm** | Graph-based (navigable small world) | Clustering-based (inverted file) |\n| **Query Speed** | Faster (especially at high recall) | Slower |\n| **Build Time** | Slower (hours for >1M vectors) | Faster |\n| **Memory Usage** | High (graph in RAM) | Lower |\n| **Recall@10** | 95\u201399% typical | 85\u201395% typical |\n| **Data Requirement** | Works on empty tables | Needs representative data first |\n| **Update Cost** | Moderate (incremental inserts OK) | Requires periodic `REINDEX` |\n\n**Decision: HNSW for our scale.** We'll have ~10K\u2013100K chunks (not millions). HNSW gives superior recall without significant memory pressure at this scale. IVFFlat's clustering requires existing data before building indexes and degrades more with data drift \u2014 problematic as we continuously add coaching content.\n\n```sql\n-- HNSW index for cosine distance (matches our embedding pipeline)\nCREATE INDEX idx_knowledge_chunks_embedding ON knowledge_chunks\nUSING hnsw (embedding vector_cosine_ops)\nWITH (m = 16, ef_construction = 128);\n\n-- Query-time parameter for recall/speed tradeoff\nSET hnsw.ef_search = 100;  -- Higher = better recall, slower\n```\n\n**HNSW Parameters Explained:**\n- `m = 16`: Number of connections per layer. 16 is the recommended default for 1024-dim vectors. Higher m = better recall but more memory.\n- `ef_construction = 128`: Build-time search width. 128 gives good quality without excessive build time.\n- `ef_search = 100`: Query-time search width. At our scale, 100 gives >98% recall with sub-10ms latency.\n\n**Reference:** AWS deep dive on pgvector indexing recommends HNSW for datasets under 1M vectors where recall is critical ([source](https://aws.amazon.com/blogs/database/optimize-generative-ai-applications-with-pgvector-indexing-a-deep-dive-into-ivfflat-and-hnsw-techniques/)). Pixion's comparison confirms HNSW is data-agnostic and doesn't require pre-existing data for clustering ([source](https://pixion.co/blog/choosing-your-index-with-pg-vector-flat-vs-hnsw-vs-ivfflat)).\n\n### 2.4 Hybrid Search: Semantic + Keyword + Metadata Filtering\n\nPure semantic search can miss exact terminology (e.g., \"SkiErg\" vs \"ski machine\"). Hybrid search with Reciprocal Rank Fusion (RRF) captures both semantic meaning and exact keyword matches.\n\n**The complete hybrid search function with metadata filtering:**\n\n```sql\nCREATE OR REPLACE FUNCTION hybrid_search(\n    query_text text,\n    query_embedding vector(1024),\n    match_count int DEFAULT 10,\n    content_type_filter text DEFAULT NULL,\n    metadata_filter jsonb DEFAULT NULL,\n    full_text_weight float DEFAULT 1.0,\n    semantic_weight float DEFAULT 1.5,  -- Slightly favor semantic\n    rrf_k int DEFAULT 50\n)\nRETURNS TABLE (\n    id bigint,\n    content text,\n    content_type text,\n    source_document text,\n    section_title text,\n    metadata jsonb,\n    fts_rank bigint,\n    semantic_rank bigint,\n    rrf_score float\n)\nLANGUAGE sql STABLE\nAS $$\nWITH full_text AS (\n    SELECT\n        kc.id,\n        ROW_NUMBER() OVER (\n            ORDER BY ts_rank_cd(kc.fts, websearch_to_tsquery('english', query_text)) DESC\n        ) AS rank_ix\n    FROM knowledge_chunks kc\n    WHERE\n        kc.fts @@ websearch_to_tsquery('english', query_text)\n        AND (content_type_filter IS NULL OR kc.content_type = content_type_filter)\n        AND (metadata_filter IS NULL OR kc.metadata @> metadata_filter)\n    ORDER BY rank_ix\n    LIMIT LEAST(match_count, 30) * 2\n),\nsemantic AS (\n    SELECT\n        kc.id,\n        ROW_NUMBER() OVER (\n            ORDER BY kc.embedding <=> query_embedding\n        ) AS rank_ix\n    FROM knowledge_chunks kc\n    WHERE\n        (content_type_filter IS NULL OR kc.content_type = content_type_filter)\n        AND (metadata_filter IS NULL OR kc.metadata @> metadata_filter)\n    ORDER BY rank_ix\n    LIMIT LEAST(match_count, 30) * 2\n)\nSELECT\n    kc.id,\n    kc.content,\n    kc.content_type,\n    kc.source_document,\n    kc.section_title,\n    kc.metadata,\n    full_text.rank_ix AS fts_rank,\n    semantic.rank_ix AS semantic_rank,\n    (\n        COALESCE(1.0 / (rrf_k + full_text.rank_ix), 0.0) * full_text_weight +\n        COALESCE(1.0 / (rrf_k + semantic.rank_ix), 0.0) * semantic_weight\n    ) AS rrf_score\nFROM\n    full_text\n    FULL OUTER JOIN semantic ON full_text.id = semantic.id\n    JOIN knowledge_chunks kc ON COALESCE(full_text.id, semantic.id) = kc.id\nORDER BY rrf_score DESC\nLIMIT LEAST(match_count, 30);\n$$;\n```\n\n**Reference:** Supabase's official hybrid search documentation uses the same RRF approach with configurable weights ([source](https://supabase.com/docs/guides/ai/hybrid-search)). Instaclustr's pgvector hybrid search tutorial confirms RRF as the standard fusion method ([source](https://www.instaclustr.com/education/vector-database/pgvector-hybrid-search-benefits-use-cases-and-quick-tutorial/)).\n\n### 2.5 Re-ranking Strategies\n\nAfter initial hybrid retrieval, re-ranking improves precision before feeding context to the LLM. For Phase 1 we use a lightweight approach; Phase 2 adds a cross-encoder.\n\n**Phase 1 \u2014 Heuristic Re-ranking (no external API):**\n\n```python\ndef rerank_chunks(chunks: list[dict], query: str, athlete_profile: dict) -> list[dict]:\n    \"\"\"Heuristic re-ranking based on relevance signals.\"\"\"\n    for chunk in chunks:\n        score = chunk[\"rrf_score\"]\n\n        # Boost chunks matching athlete's division\n        if athlete_profile.get(\"division\") and \\\n           chunk[\"metadata\"].get(\"division\") == athlete_profile[\"division\"]:\n            score *= 1.3\n\n        # Boost chunks matching athlete's weak stations\n        weak_stations = athlete_profile.get(\"weak_stations\", [])\n        if chunk[\"metadata\"].get(\"station\") in weak_stations:\n            score *= 1.25\n\n        # Boost recent content (training plans evolve)\n        if chunk[\"metadata\"].get(\"year\", 2024) >= 2025:\n            score *= 1.1\n\n        # Penalize very short chunks (likely incomplete context)\n        if len(chunk[\"content\"]) < 100:\n            score *= 0.7\n\n        chunk[\"final_score\"] = score\n\n    return sorted(chunks, key=lambda x: x[\"final_score\"], reverse=True)\n```\n\n**Phase 2 \u2014 Cross-Encoder Re-ranking** (when you need higher precision):\nUse a cross-encoder like `cross-encoder/ms-marco-MiniLM-L-6-v2` run locally or via a lightweight inference endpoint. This scores each (query, chunk) pair directly, giving significantly better ranking than bi-encoder similarity alone.\n\n### 2.6 Context Window Assembly\n\nKimi K2 Thinking has a **256K context window** \u2014 enormous, but we shouldn't fill it. More context \u2260 better answers. The \"lost in the middle\" problem means LLMs attend more to the beginning and end of context.\n\n**Token Budget Strategy:**\n\n| Component | Token Budget | Notes |\n|---|---|---|\n| System prompt | ~800 tokens | Coaching persona + rules |\n| Athlete profile | ~300 tokens | Structured summary |\n| Conversation memory | ~2,000 tokens | Last 3\u20135 messages + rolling summary |\n| Retrieved knowledge | ~3,000\u20135,000 tokens | 6\u201310 re-ranked chunks |\n| User's current message | ~200 tokens | Current turn |\n| **Reserved for output** | **~4,000 tokens** | Workout plans can be long |\n| **Total used** | **~8,300 tokens** | ~3% of 256K window |\n\nWe deliberately stay far under the 256K limit. Benefits:\n1. **Faster inference** \u2014 latency scales with input length\n2. **Lower cost** \u2014 input tokens are billed\n3. **Better attention** \u2014 model focuses on what matters\n4. **Headroom** \u2014 long multi-turn conversations can grow\n\n```python\ndef assemble_context(\n    system_prompt: str,\n    athlete_profile: dict,\n    conversation_history: list[dict],\n    retrieved_chunks: list[dict],\n    user_message: str,\n    max_context_tokens: int = 12_000  # Stay well under 256K\n) -> list[dict]:\n    \"\"\"Assemble the final message array for the LLM.\"\"\"\n\n    messages = []\n\n    # 1. System prompt (always first)\n    messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n    # 2. Athlete profile injected as system context\n    profile_text = format_athlete_profile(athlete_profile)\n    messages.append({\n        \"role\": \"system\",\n        \"content\": f\"## Current Athlete Profile\\n{profile_text}\"\n    })\n\n    # 3. Retrieved knowledge (placed early for attention)\n    knowledge_text = format_retrieved_chunks(retrieved_chunks)\n    messages.append({\n        \"role\": \"system\",\n        \"content\": f\"## Relevant Coaching Knowledge\\n{knowledge_text}\"\n    })\n\n    # 4. Conversation history (summarized older + recent verbatim)\n    for msg in conversation_history:\n        messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n\n    # 5. Current user message\n    messages.append({\"role\": \"user\", \"content\": user_message})\n\n    return messages\n\n\ndef format_athlete_profile(profile: dict) -> str:\n    return f\"\"\"- Name: {profile['display_name']}\n- Division: {profile['division']} ({profile['gender']})\n- Experience: {profile['experience_level']}\n- Last race time: {profile.get('last_race_time', 'N/A')}\n- Goal time: {profile.get('goal_time', 'Not set')}\n- Weak stations: {', '.join(profile.get('weak_stations', ['Unknown']))}\n- Training days/week: {profile.get('training_days_per_week', 4)}\n- Injuries/limitations: {profile.get('injuries', 'None reported')}\"\"\"\n\n\ndef format_retrieved_chunks(chunks: list[dict]) -> str:\n    formatted = []\n    for i, chunk in enumerate(chunks, 1):\n        formatted.append(\n            f\"[Source {i}: {chunk['source_document']} \u2014 {chunk['section_title']}]\\n\"\n            f\"{chunk['content']}\\n\"\n        )\n    return \"\\n---\\n\".join(formatted)\n```\n\n---\n\n## 3. Supabase Schema & RAG Patterns\n\n### 3.1 Complete SQL Schema\n\n```sql\n-- ============================================================\n-- Enable required extensions\n-- ============================================================\nCREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA extensions;\nCREATE EXTENSION IF NOT EXISTS pg_trgm;   -- For fuzzy text matching\n\n-- ============================================================\n-- ATHLETES / PROFILES\n-- ============================================================\nCREATE TABLE public.athlete_profiles (\n    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),\n    user_id uuid NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,\n    display_name text NOT NULL,\n    gender text CHECK (gender IN ('male', 'female', 'non_binary')),\n    division text NOT NULL DEFAULT 'open'\n        CHECK (division IN ('open', 'pro', 'doubles', 'relay')),\n    experience_level text DEFAULT 'beginner'\n        CHECK (experience_level IN ('beginner', 'intermediate', 'advanced', 'elite')),\n    date_of_birth date,\n    weight_kg numeric(5,1),\n    height_cm numeric(5,1),\n    last_race_time interval,          -- e.g., '01:32:15'\n    goal_time interval,\n    weak_stations text[] DEFAULT '{}', -- e.g., {'sled_push', 'wall_balls'}\n    training_days_per_week int DEFAULT 4 CHECK (training_days_per_week BETWEEN 1 AND 7),\n    injuries text DEFAULT '',\n    preferences jsonb DEFAULT '{}'::jsonb,  -- {\"metric\": true, \"language\": \"en\"}\n    created_at timestamptz NOT NULL DEFAULT now(),\n    updated_at timestamptz NOT NULL DEFAULT now(),\n    UNIQUE (user_id)\n);\n\nCREATE INDEX idx_profiles_user_id ON athlete_profiles(user_id);\n\n-- ============================================================\n-- KNOWLEDGE BASE (RAG chunks)\n-- ============================================================\nCREATE TABLE public.knowledge_chunks (\n    id bigint PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n    content text NOT NULL,\n    content_type text NOT NULL\n        CHECK (content_type IN (\n            'training_plan', 'technique_guide', 'race_rules',\n            'nutrition', 'faq', 'pacing_table', 'general'\n        )),\n    source_document text NOT NULL,      -- e.g., \"hyrox_rulebook_2025.pdf\"\n    section_title text DEFAULT '',\n    metadata jsonb DEFAULT '{}'::jsonb,  -- {\"station\": \"sled_push\", \"division\": \"open\", \"year\": 2025}\n    token_count int NOT NULL DEFAULT 0,\n    embedding vector(1024),              -- text-embedding-3-small @ 1024 dims\n    fts tsvector GENERATED ALWAYS AS (\n        to_tsvector('english', content)\n    ) STORED,\n    created_at timestamptz NOT NULL DEFAULT now()\n);\n\n-- HNSW index for semantic search (cosine distance)\nCREATE INDEX idx_knowledge_embedding_hnsw ON knowledge_chunks\n    USING hnsw (embedding vector_cosine_ops)\n    WITH (m = 16, ef_construction = 128);\n\n-- GIN index for full-text search\nCREATE INDEX idx_knowledge_fts ON knowledge_chunks USING gin(fts);\n\n-- GIN index on JSONB metadata for filtered searches\nCREATE INDEX idx_knowledge_metadata ON knowledge_chunks USING gin(metadata jsonb_path_ops);\n\n-- Partial index for content type filtering\nCREATE INDEX idx_knowledge_content_type ON knowledge_chunks(content_type);\n\n-- ============================================================\n-- CONVERSATION SESSIONS & MESSAGES\n-- ============================================================\nCREATE TABLE public.conversation_sessions (\n    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),\n    user_id uuid NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,\n    title text DEFAULT 'New Conversation',\n    summary text DEFAULT '',              -- Rolling summary of conversation\n    message_count int DEFAULT 0,\n    is_active boolean DEFAULT true,\n    created_at timestamptz NOT NULL DEFAULT now(),\n    updated_at timestamptz NOT NULL DEFAULT now()\n);\n\nCREATE INDEX idx_sessions_user_active ON conversation_sessions(user_id, is_active);\n\nCREATE TABLE public.conversation_messages (\n    id bigint PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n    session_id uuid NOT NULL REFERENCES conversation_sessions(id) ON DELETE CASCADE,\n    user_id uuid NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,\n    role text NOT NULL CHECK (role IN ('user', 'assistant', 'system')),\n    content text NOT NULL,\n    reasoning_content text,             -- K2 Thinking chain (for debugging/training)\n    retrieved_chunk_ids bigint[],       -- Which knowledge chunks were used\n    tool_calls jsonb,                   -- If function calling was used\n    token_count_input int,\n    token_count_output int,\n    latency_ms int,\n    model_id text DEFAULT 'kimi-k2-instruct-0905',\n    created_at timestamptz NOT NULL DEFAULT now(),\n    -- Training data quality signals\n    user_rating smallint CHECK (user_rating BETWEEN 1 AND 5),\n    was_helpful boolean,\n    follow_through boolean              -- Did user actually do the workout?\n);\n\nCREATE INDEX idx_messages_session ON conversation_messages(session_id, created_at);\nCREATE INDEX idx_messages_user ON conversation_messages(user_id, created_at);\n\n-- ============================================================\n-- WORKOUT LOGGING\n-- ============================================================\nCREATE TABLE public.workout_logs (\n    id bigint PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n    user_id uuid NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,\n    session_id uuid REFERENCES conversation_sessions(id),  -- Which chat generated this\n    workout_date date NOT NULL DEFAULT CURRENT_DATE,\n    workout_type text NOT NULL\n        CHECK (workout_type IN (\n            'hyrox_simulation', 'run', 'strength', 'hybrid',\n            'skierg', 'row', 'technique', 'recovery', 'race'\n        )),\n    title text NOT NULL,\n    prescribed_plan jsonb NOT NULL,     -- The full workout as structured JSON\n    completed boolean DEFAULT false,\n    actual_results jsonb DEFAULT '{}'::jsonb,  -- What they actually did\n    rpe smallint CHECK (rpe BETWEEN 1 AND 10),  -- Rate of Perceived Exertion\n    notes text DEFAULT '',\n    duration_minutes int,\n    created_at timestamptz NOT NULL DEFAULT now()\n);\n\nCREATE INDEX idx_workouts_user_date ON workout_logs(user_id, workout_date DESC);\n\n-- ============================================================\n-- RACE RESULTS\n-- ============================================================\nCREATE TABLE public.race_results (\n    id bigint PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n    user_id uuid NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,\n    race_date date NOT NULL,\n    location text NOT NULL,\n    division text NOT NULL,\n    overall_time interval NOT NULL,\n    run_splits interval[],              -- Array of 8 run times\n    station_times jsonb NOT NULL,       -- {\"skierg\": \"4:32\", \"sled_push\": \"2:15\", ...}\n    overall_rank int,\n    division_rank int,\n    notes text DEFAULT '',\n    created_at timestamptz NOT NULL DEFAULT now()\n);\n\nCREATE INDEX idx_races_user ON race_results(user_id, race_date DESC);\n\n-- ============================================================\n-- DOCUMENT UPLOADS (for user-submitted content)\n-- ============================================================\nCREATE TABLE public.document_uploads (\n    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),\n    user_id uuid REFERENCES auth.users(id),  -- NULL for admin-uploaded global content\n    file_name text NOT NULL,\n    file_path text NOT NULL,                  -- Supabase Storage path\n    file_type text NOT NULL,\n    processing_status text DEFAULT 'pending'\n        CHECK (processing_status IN ('pending', 'processing', 'completed', 'failed')),\n    chunk_count int DEFAULT 0,\n    created_at timestamptz NOT NULL DEFAULT now()\n);\n```\n\n### 3.2 Row Level Security (RLS) Patterns\n\n```sql\n-- Enable RLS on all user-facing tables\nALTER TABLE athlete_profiles ENABLE ROW LEVEL SECURITY;\nALTER TABLE conversation_sessions ENABLE ROW LEVEL SECURITY;\nALTER TABLE conversation_messages ENABLE ROW LEVEL SECURITY;\nALTER TABLE workout_logs ENABLE ROW LEVEL SECURITY;\nALTER TABLE race_results ENABLE ROW LEVEL SECURITY;\nALTER TABLE document_uploads ENABLE ROW LEVEL SECURITY;\n\n-- Knowledge chunks: readable by all authenticated users (shared knowledge base)\nALTER TABLE knowledge_chunks ENABLE ROW LEVEL SECURITY;\n\nCREATE POLICY \"Knowledge chunks are readable by all authenticated users\"\n    ON knowledge_chunks FOR SELECT TO authenticated\n    USING (true);\n\nCREATE POLICY \"Only service role can insert/update/delete knowledge chunks\"\n    ON knowledge_chunks FOR ALL TO service_role\n    USING (true) WITH CHECK (true);\n\n-- Athlete profiles: users can only CRUD their own\nCREATE POLICY \"Users can view their own profile\"\n    ON athlete_profiles FOR SELECT TO authenticated\n    USING (user_id = (SELECT auth.uid()));\n\nCREATE POLICY \"Users can update their own profile\"\n    ON athlete_profiles FOR UPDATE TO authenticated\n    USING (user_id = (SELECT auth.uid()))\n    WITH CHECK (user_id = (SELECT auth.uid()));\n\nCREATE POLICY \"Users can insert their own profile\"\n    ON athlete_profiles FOR INSERT TO authenticated\n    WITH CHECK (user_id = (SELECT auth.uid()));\n\n-- Conversation sessions: users own their sessions\nCREATE POLICY \"Users can manage their own sessions\"\n    ON conversation_sessions FOR ALL TO authenticated\n    USING (user_id = (SELECT auth.uid()))\n    WITH CHECK (user_id = (SELECT auth.uid()));\n\n-- Messages: users can read their own, insert via edge function\nCREATE POLICY \"Users can read their own messages\"\n    ON conversation_messages FOR SELECT TO authenticated\n    USING (user_id = (SELECT auth.uid()));\n\nCREATE POLICY \"Service role can insert messages\"\n    ON conversation_messages FOR INSERT TO service_role\n    WITH CHECK (true);\n\n-- Workout logs: full CRUD for own data\nCREATE POLICY \"Users manage their own workouts\"\n    ON workout_logs FOR ALL TO authenticated\n    USING (user_id = (SELECT auth.uid()))\n    WITH CHECK (user_id = (SELECT auth.uid()));\n\n-- Race results: same pattern\nCREATE POLICY \"Users manage their own race results\"\n    ON race_results FOR ALL TO authenticated\n    USING (user_id = (SELECT auth.uid()))\n    WITH CHECK (user_id = (SELECT auth.uid()));\n```\n\n**Reference:** Supabase's RAG with Permissions guide demonstrates this exact pattern \u2014 RLS on `document_sections` filtered by ownership through a parent `documents` table ([source](https://supabase.com/docs/guides/ai/rag-with-permissions)).\n\n### 3.3 RPC Functions for Similarity Search\n\n```sql\n-- Simple semantic-only search (for quick lookups)\nCREATE OR REPLACE FUNCTION semantic_search(\n    query_embedding vector(1024),\n    match_count int DEFAULT 5,\n    match_threshold float DEFAULT 0.7\n)\nRETURNS TABLE (\n    id bigint,\n    content text,\n    content_type text,\n    source_document text,\n    section_title text,\n    metadata jsonb,\n    similarity float\n)\nLANGUAGE sql STABLE\nAS $$\n    SELECT\n        kc.id,\n        kc.content,\n        kc.content_type,\n        kc.source_document,\n        kc.section_title,\n        kc.metadata,\n        1 - (kc.embedding <=> query_embedding) AS similarity\n    FROM knowledge_chunks kc\n    WHERE 1 - (kc.embedding <=> query_embedding) > match_threshold\n    ORDER BY kc.embedding <=> query_embedding\n    LIMIT match_count;\n$$;\n```\n\n### 3.4 Edge Function: Embedding Pipeline (Ingest)\n\n```typescript\n// supabase/functions/ingest/index.ts\nimport { createClient } from \"npm:@supabase/supabase-js@2\";\nimport OpenAI from \"npm:openai\";\n\nconst supabase = createClient(\n  Deno.env.get(\"SUPABASE_URL\")!,\n  Deno.env.get(\"SUPABASE_SERVICE_ROLE_KEY\")!\n);\n\nconst openai = new OpenAI({ apiKey: Deno.env.get(\"OPENAI_API_KEY\")! });\n\ninterface IngestRequest {\n  chunks: Array<{\n    content: string;\n    content_type: string;\n    source_document: string;\n    section_title?: string;\n    metadata?: Record<string, unknown>;\n  }>;\n}\n\nDeno.serve(async (req) => {\n  // Verify admin/service role auth\n  const authHeader = req.headers.get(\"Authorization\");\n  if (!authHeader?.includes(Deno.env.get(\"ADMIN_SECRET\")!)) {\n    return new Response(\"Unauthorized\", { status: 401 });\n  }\n\n  const { chunks }: IngestRequest = await req.json();\n\n  // Batch embed (OpenAI supports up to 2048 inputs per request)\n  const BATCH_SIZE = 100;\n  const results = [];\n\n  for (let i = 0; i < chunks.length; i += BATCH_SIZE) {\n    const batch = chunks.slice(i, i + BATCH_SIZE);\n    const texts = batch.map((c) => c.content);\n\n    const embeddingResponse = await openai.embeddings.create({\n      model: \"text-embedding-3-small\",\n      input: texts,\n      dimensions: 1024,\n    });\n\n    for (let j = 0; j < batch.length; j++) {\n      const { data: inserted, error } = await supabase\n        .from(\"knowledge_chunks\")\n        .insert({\n          content: batch[j].content,\n          content_type: batch[j].content_type,\n          source_document: batch[j].source_document,\n          section_title: batch[j].section_title || \"\",\n          metadata: batch[j].metadata || {},\n          token_count: Math.ceil(batch[j].content.length / 4), // Rough estimate\n          embedding: embeddingResponse.data[j].embedding,\n        })\n        .select(\"id\")\n        .single();\n\n      if (error) {\n        results.push({ success: false, error: error.message });\n      } else {\n        results.push({ success: true, id: inserted.id });\n      }\n    }\n  }\n\n  return new Response(JSON.stringify({ inserted: results.length, results }), {\n    headers: { \"Content-Type\": \"application/json\" },\n  });\n});\n```\n\n### 3.5 Realtime Subscriptions for Workout Logging\n\nEnable realtime on the `workout_logs` table in the Supabase dashboard, then subscribe on the client:\n\n```typescript\n// Client-side: React Native / mobile\nimport { createClient } from \"@supabase/supabase-js\";\n\nconst supabase = createClient(SUPABASE_URL, SUPABASE_ANON_KEY);\n\n// Subscribe to workout updates for the logged-in user\nfunction subscribeToWorkouts(userId: string, onUpdate: (workout: any) => void) {\n  const channel = supabase\n    .channel(\"workout-updates\")\n    .on(\n      \"postgres_changes\",\n      {\n        event: \"*\",  // INSERT, UPDATE, DELETE\n        schema: \"public\",\n        table: \"workout_logs\",\n        filter: `user_id=eq.${userId}`,\n      },\n      (payload) => {\n        console.log(\"Workout change:\", payload.eventType, payload.new);\n        onUpdate(payload.new);\n      }\n    )\n    .subscribe();\n\n  // Return unsubscribe function\n  return () => supabase.removeChannel(channel);\n}\n```\n\n**Reference:** Supabase Realtime docs describe the `postgres_changes` channel with per-row filters using the `filter` parameter ([source](https://supabase.com/docs/guides/realtime/subscribing-to-database-changes)).\n\n---\n\n## 4. Fireworks AI / Kimi K2 Integration\n\n### 4.1 Model IDs and Configuration\n\n| Model | Fireworks Model ID | Context | Use Case |\n|---|---|---|---|\n| Kimi K2 Instruct | `accounts/fireworks/models/kimi-k2-instruct-0905` | 131K | Standard coaching (Phase 1 default) |\n| Kimi K2 Thinking | `accounts/fireworks/models/kimi-k2-thinking` | 262K | Complex analysis, race planning |\n\n**Pricing (as of Feb 2025):** Kimi K2 Instruct: $0.60/1M input, $2.50/1M output. The thinking variant may have different pricing for reasoning tokens.\n\n### 4.2 Core API Integration \u2014 Python Client\n\n```python\n# fireworks_client.py\nimport json\nfrom typing import AsyncGenerator, Optional\nfrom fireworks import Fireworks\n\nFIREWORKS_MODEL = \"accounts/fireworks/models/kimi-k2-instruct-0905\"\nFIREWORKS_MODEL_THINKING = \"accounts/fireworks/models/kimi-k2-thinking\"\n\nclient = Fireworks()\n\n# --- Non-Streaming (for structured outputs) ---\ndef coaching_completion(\n    messages: list[dict],\n    tools: Optional[list[dict]] = None,\n    temperature: float = 0.7,\n    max_tokens: int = 4096,\n    use_thinking: bool = False,\n) -> dict:\n    \"\"\"Synchronous completion for structured outputs.\"\"\"\n    model = FIREWORKS_MODEL_THINKING if use_thinking else FIREWORKS_MODEL\n\n    kwargs = {\n        \"model\": model,\n        \"messages\": messages,\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens,\n    }\n    if tools:\n        kwargs[\"tools\"] = tools\n        kwargs[\"tool_choice\"] = \"auto\"\n\n    response = client.chat.completions.create(**kwargs)\n    return {\n        \"content\": response.choices[0].message.content,\n        \"tool_calls\": response.choices[0].message.tool_calls,\n        \"reasoning_content\": getattr(response.choices[0].message, \"reasoning_content\", None),\n        \"usage\": {\n            \"input_tokens\": response.usage.prompt_tokens,\n            \"output_tokens\": response.usage.completion_tokens,\n        },\n    }\n\n\n# --- Streaming (for chat UX) ---\nasync def coaching_stream(\n    messages: list[dict],\n    tools: Optional[list[dict]] = None,\n    temperature: float = 0.7,\n    max_tokens: int = 4096,\n) -> AsyncGenerator[dict, None]:\n    \"\"\"Stream response tokens for real-time chat UX.\"\"\"\n    stream = client.chat.completions.create(\n        model=FIREWORKS_MODEL,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        stream=True,\n        tools=tools,\n        tool_choice=\"auto\" if tools else None,\n    )\n\n    tool_calls_buffer = {}\n    full_content = \"\"\n\n    for chunk in stream:\n        delta = chunk.choices[0].delta\n\n        # Handle streaming text content\n        if delta.content:\n            full_content += delta.content\n            yield {\"type\": \"content\", \"text\": delta.content}\n\n        # Handle streaming tool calls\n        if delta.tool_calls:\n            for tc in delta.tool_calls:\n                idx = tc.index\n                if idx not in tool_calls_buffer:\n                    tool_calls_buffer[idx] = {\"id\": \"\", \"name\": \"\", \"arguments\": \"\"}\n                if tc.id:\n                    tool_calls_buffer[idx][\"id\"] = tc.id\n                if tc.function and tc.function.name:\n                    tool_calls_buffer[idx][\"name\"] = tc.function.name\n                if tc.function and tc.function.arguments:\n                    tool_calls_buffer[idx][\"arguments\"] += tc.function.arguments\n\n        # Detect end of stream\n        if chunk.choices[0].finish_reason:\n            yield {\n                \"type\": \"done\",\n                \"finish_reason\": chunk.choices[0].finish_reason,\n                \"full_content\": full_content,\n                \"tool_calls\": list(tool_calls_buffer.values()) if tool_calls_buffer else None,\n            }\n```\n\n### 4.3 Function Calling \u2014 Coaching Tools\n\nDefine tools the model can call to take action (log workouts, query athlete data, look up race schedules):\n\n```python\nCOACHING_TOOLS = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"generate_workout_plan\",\n            \"description\": \"Generate a structured workout plan for the athlete. Call this when the user asks for a workout, training plan, or session prescription.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"workout_type\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"hyrox_simulation\", \"run\", \"strength\", \"hybrid\", \"skierg\", \"row\", \"technique\", \"recovery\"],\n                        \"description\": \"Type of workout session\"\n                    },\n                    \"title\": {\n                        \"type\": \"string\",\n                        \"description\": \"Short descriptive title for the workout\"\n                    },\n                    \"duration_minutes\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Estimated total duration in minutes\"\n                    },\n                    \"warmup\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"Warmup exercises\"\n                    },\n                    \"main_block\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"exercise\": {\"type\": \"string\"},\n                                \"sets\": {\"type\": \"integer\"},\n                                \"reps_or_duration\": {\"type\": \"string\"},\n                                \"load\": {\"type\": \"string\"},\n                                \"rest\": {\"type\": \"string\"},\n                                \"notes\": {\"type\": \"string\"}\n                            },\n                            \"required\": [\"exercise\", \"sets\", \"reps_or_duration\"]\n                        },\n                        \"description\": \"Main workout exercises\"\n                    },\n                    \"cooldown\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"Cooldown exercises\"\n                    },\n                    \"coaching_notes\": {\n                        \"type\": \"string\",\n                        \"description\": \"Key coaching cues and focus points for this session\"\n                    }\n                },\n                \"required\": [\"workout_type\", \"title\", \"main_block\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"log_race_result\",\n            \"description\": \"Record a Hyrox race result when the athlete shares their race data.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"race_date\": {\"type\": \"string\", \"description\": \"Date of the race (YYYY-MM-DD)\"},\n                    \"location\": {\"type\": \"string\", \"description\": \"Race city/venue\"},\n                    \"division\": {\"type\": \"string\", \"enum\": [\"open\", \"pro\", \"doubles\", \"relay\"]},\n                    \"overall_time\": {\"type\": \"string\", \"description\": \"Total race time (HH:MM:SS)\"},\n                    \"station_times\": {\n                        \"type\": \"object\",\n                        \"description\": \"Time for each station: skierg, sled_push, sled_pull, burpee_broad_jump, row, farmers_carry, sandbag_lunges, wall_balls\"\n                    }\n                },\n                \"required\": [\"race_date\", \"location\", \"division\", \"overall_time\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_progress\",\n            \"description\": \"Analyze the athlete's training progress over a time period. Call this when they ask about trends, improvements, or readiness.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"period_days\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Number of days to look back for analysis\",\n                        \"default\": 30\n                    },\n                    \"focus_area\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"overall\", \"running\", \"stations\", \"consistency\", \"volume\"],\n                        \"description\": \"What aspect of training to analyze\"\n                    }\n                },\n                \"required\": [\"period_days\"]\n            }\n        }\n    }\n]\n```\n\n**Reference:** Fireworks AI function calling docs confirm Kimi K2 Instruct supports `tool_choice`, parallel tool calls, and streaming tool calls ([source](https://docs.fireworks.ai/guides/function-calling)).\n\n### 4.4 LoRA Fine-Tuning Workflow (Phase 2)\n\n**Step 1: Prepare dataset in JSONL format**\n\n```json\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are Coach Rox, an expert Hyrox coach...\"}, {\"role\": \"user\", \"content\": \"My sled push is really slow. I finished last in my wave at the sled station. What should I focus on?\"}, {\"role\": \"assistant\", \"content\": \"I hear you \u2014 the sled push is one of the most technique-dependent stations in Hyrox. Let's break this down...\\n\\n**Immediate fixes for your next training session:**\\n\\n1. **Body angle**: Get LOW. Your torso should be at roughly 45\u00b0 to the ground. Most people stand too upright, which means you're pushing against gravity instead of into the sled.\\n\\n2. **Short, choppy steps**: Think 'quick feet' not long strides. Each step should be a powerful drive, not a lunge.\\n\\n3. **Hand position**: Arms fully extended, locked out. Push through your shoulders, not your elbows.\\n\\n**Training prescription for this week:**\\nI'd recommend 3 sled sessions:\\n- Monday: 6 \u00d7 25m heavy sled push (heavier than race weight) with 90s rest\\n- Wednesday: 4 \u00d7 50m at race weight, focusing purely on body angle\\n- Friday: 3 \u00d7 50m timed efforts with 3min rest\\n\\nWhat weight are you currently pushing in training?\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are Coach Rox, an expert Hyrox coach...\"}, {\"role\": \"user\", \"content\": \"Can you give me a workout for tomorrow? I have about 60 minutes and want to work on my running and wall balls.\"}, {\"role\": \"assistant\", \"content\": null, \"tool_calls\": [{\"type\": \"function\", \"function\": {\"name\": \"generate_workout_plan\", \"arguments\": \"{\\\"workout_type\\\": \\\"hybrid\\\", \\\"title\\\": \\\"Run + Wall Ball Combo\\\", \\\"duration_minutes\\\": 60, \\\"warmup\\\": [\\\"5 min easy jog\\\", \\\"2 \u00d7 10 air squats\\\", \\\"2 \u00d7 10 wall ball at light weight\\\", \\\"Dynamic stretches: leg swings, hip circles\\\"], \\\"main_block\\\": [{\\\"exercise\\\": \\\"Run\\\", \\\"sets\\\": 1, \\\"reps_or_duration\\\": \\\"3 \u00d7 1km @ race pace\\\", \\\"rest\\\": \\\"2 min between\\\", \\\"notes\\\": \\\"Focus on controlled breathing\\\"}, {\\\"exercise\\\": \\\"Wall Balls\\\", \\\"sets\\\": 5, \\\"reps_or_duration\\\": \\\"15 reps\\\", \\\"load\\\": \\\"Race weight (6kg W / 9kg M)\\\", \\\"rest\\\": \\\"60s\\\", \\\"notes\\\": \\\"Full squat depth, smooth throw rhythm\\\"}, {\\\"exercise\\\": \\\"Run + Wall Ball Superset\\\", \\\"sets\\\": 3, \\\"reps_or_duration\\\": \\\"400m run immediately into 15 wall balls\\\", \\\"rest\\\": \\\"90s\\\", \\\"notes\\\": \\\"Simulate race transitions\\\"}], \\\"cooldown\\\": [\\\"5 min easy walk\\\", \\\"Quad stretch 30s each side\\\", \\\"Shoulder stretch 30s each side\\\"], \\\"coaching_notes\\\": \\\"The key focus today is the transition from running into wall balls. That's where your heart rate will spike. Practice settling into a rhythm within the first 3 reps.\\\"}\"}}]}]}\n```\n\n**Step 2: Upload and fine-tune**\n\n```bash\n# Install Fireworks CLI\npip install fireworks-ai\n\n# Create dataset\nfirectl create dataset hyrox-coaching-v1\n\n# Upload JSONL\nfirectl upload-dataset hyrox-coaching-v1 ./training_data/coaching_conversations_v1.jsonl\n\n# Launch fine-tuning job\nfirectl sftj create \\\n    --base-model accounts/fireworks/models/kimi-k2-instruct-0905 \\\n    --dataset hyrox-coaching-v1 \\\n    --output-model hyrox-coach-lora-v1 \\\n    --epochs 2.0 \\\n    --lora-rank 16 \\\n    --learning-rate 0.0001 \\\n    --wandb-entity my-org \\\n    --wandb-api-key $WANDB_KEY \\\n    --wandb-project \"hyrox-coach\"\n\n# Monitor progress\nfirectl sftj get hyrox-coaching-v1\n```\n\n**Step 3: Deploy**\n\n```bash\n# Single LoRA deployment (best performance)\nfirectl create deployment \"accounts/YOUR_ACCOUNT/models/hyrox-coach-lora-v1\"\n\n# OR Multi-LoRA for A/B testing (same base model cost)\nfirectl create deployment \\\n    \"accounts/fireworks/models/kimi-k2-instruct-0905\" \\\n    --enable-addons\n\nfirectl load-lora hyrox-coach-lora-v1 --deployment $DEPLOYMENT_ID\nfirectl load-lora hyrox-coach-lora-v2 --deployment $DEPLOYMENT_ID\n```\n\n**Reference:** Fireworks AI SFT docs specify JSONL format with `messages` array, optional `reasoning_content` for thinking models, `weight` fields, and LoRA rank up to 64 ([source](https://docs.fireworks.ai/fine-tuning/fine-tuning-models)). Multi-LoRA deployment serves hundreds of adapters at base model cost ([source](https://docs.fireworks.ai/fine-tuning/deploying-loras)).\n\n---\n\n## 5. Coaching Conversation Design\n\n### 5.1 System Prompt Architecture\n\n```python\nSYSTEM_PROMPT = \"\"\"You are **Coach Rox**, an expert Hyrox fitness coach and training advisor. You combine deep knowledge of Hyrox race format, exercise science, and practical coaching experience to help athletes prepare for and improve at Hyrox competitions.\n\n## Your Persona\n- Encouraging but honest \u2014 you celebrate progress AND call out when athletes are cutting corners\n- You speak like a real coach: direct, clear, sometimes using humor to keep things light\n- You never give medical advice; for injuries, always recommend consulting a physiotherapist\n- You're obsessed with PACING \u2014 the #1 mistake in Hyrox is going out too fast\n\n## Hyrox Race Format (Your Core Knowledge)\nEvery Hyrox race = 8 \u00d7 (1km run + 1 workout station), in this fixed order:\n1. SkiErg (1,000m) \u2192 upper body endurance\n2. Sled Push (50m) \u2192 leg drive + technique\n3. Sled Pull (50m) \u2192 grip + posterior chain\n4. Burpee Broad Jumps (80m) \u2192 explosive endurance\n5. Row (1,000m) \u2192 full-body cardio\n6. Farmer's Carry (200m) \u2192 grip + core stability\n7. Sandbag Lunges (100m) \u2192 leg endurance + balance\n8. Wall Balls (75 reps M / 50 reps F) \u2192 full-body finisher\n\nDivisions: Open (standard weights), Pro (heavier), Doubles, Relay.\n\n## How to Use Retrieved Context\n- The coaching knowledge provided below comes from verified sources (official Hyrox rulebooks, certified coaching programs, sports science research)\n- ALWAYS prefer retrieved context over your general knowledge when they conflict\n- Cite the source naturally: \"According to the Hyrox training guide...\" or \"Research shows...\"\n- If the retrieved context doesn't cover the question, say so honestly and answer from your general knowledge with a disclaimer\n\n## How to Use the Athlete Profile\n- ALWAYS personalize advice to their division, experience level, weak stations, and goals\n- Reference their recent workouts and race results when relevant\n- Scale workout prescriptions to their training days/week and current fitness\n- Account for any injuries or limitations\n\n## Structured Outputs\n- When prescribing workouts, use the generate_workout_plan tool to create structured, loggable plans\n- When the athlete shares race results, use log_race_result to record them\n- Format workout descriptions clearly with sets, reps, rest periods, and coaching cues\n- Include RPE targets (1-10 scale) for intensity guidance\n\n## Conversation Rules\n- Keep responses focused and actionable \u2014 athletes want to know WHAT to do, not just theory\n- Ask clarifying questions before prescribing (e.g., \"What equipment do you have access to?\")\n- Break complex topics into digestible pieces\n- Use markdown formatting for readability: bold key terms, use bullet lists for plans\n- End coaching responses with a clear next step or question to maintain dialogue\n\"\"\"\n```\n\n### 5.2 Conversation Memory Strategy\n\nRather than naively stuffing the entire conversation history into context, we use a **tiered memory system**:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                MEMORY ARCHITECTURE                  \u2502\n\u2502                                                     \u2502\n\u2502  Tier 1: ROLLING SUMMARY (persisted in DB)          \u2502\n\u2502  \u251c\u2500\u2500 Summarized every 10 messages by LLM            \u2502\n\u2502  \u251c\u2500\u2500 Contains: key facts, decisions, preferences    \u2502\n\u2502  \u251c\u2500\u2500 ~200-400 tokens                                \u2502\n\u2502  \u2514\u2500\u2500 Stored in conversation_sessions.summary        \u2502\n\u2502                                                     \u2502\n\u2502  Tier 2: RECENT MESSAGES (last 6 messages)          \u2502\n\u2502  \u251c\u2500\u2500 Full verbatim messages                         \u2502\n\u2502  \u251c\u2500\u2500 Preserves exact context of current topic       \u2502\n\u2502  \u2514\u2500\u2500 ~1000-2000 tokens                              \u2502\n\u2502                                                     \u2502\n\u2502  Tier 3: ATHLETE PROFILE (always loaded)            \u2502\n\u2502  \u251c\u2500\u2500 Structured data from athlete_profiles table    \u2502\n\u2502  \u251c\u2500\u2500 Updated as we learn new information            \u2502\n\u2502  \u2514\u2500\u2500 ~300 tokens                                    \u2502\n\u2502                                                     \u2502\n\u2502  Tier 4: CROSS-SESSION FACTS (future enhancement)   \u2502\n\u2502  \u251c\u2500\u2500 Key facts extracted and persisted separately   \u2502\n\u2502  \u2514\u2500\u2500 \"Athlete prefers morning workouts\"             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Summarization function (runs every 10 messages):**\n\n```python\nasync def update_conversation_summary(\n    session_id: str,\n    existing_summary: str,\n    recent_messages: list[dict]\n) -> str:\n    \"\"\"Use LLM to update the rolling conversation summary.\"\"\"\n    messages_text = \"\\n\".join(\n        f\"{m['role'].upper()}: {m['content'][:500]}\"  # Truncate long messages\n        for m in recent_messages\n    )\n\n    summarize_prompt = f\"\"\"You are summarizing a coaching conversation for future reference.\n\nEXISTING SUMMARY:\n{existing_summary or \"No previous summary.\"}\n\nNEW MESSAGES TO INCORPORATE:\n{messages_text}\n\nCreate an updated summary that captures:\n1. Key athlete information learned (fitness level, preferences, constraints)\n2. Training decisions made (what workouts were prescribed, what was agreed upon)\n3. Progress discussed (improvements, setbacks, race results)\n4. Open items or follow-ups promised\n\nKeep the summary concise (under 300 words). Focus on facts the coach needs to remember.\"\"\"\n\n    result = coaching_completion(\n        messages=[{\"role\": \"user\", \"content\": summarize_prompt}],\n        temperature=0.3,\n        max_tokens=500,\n    )\n    return result[\"content\"]\n```\n\n### 5.3 Progress Analysis from Logged Data\n\nWhen the `analyze_progress` tool is called, we query the athlete's data and format it for the LLM:\n\n```python\nasync def execute_progress_analysis(\n    user_id: str, period_days: int, focus_area: str\n) -> str:\n    \"\"\"Query workout logs and race results, format for LLM analysis.\"\"\"\n    from datetime import datetime, timedelta\n\n    cutoff = datetime.now() - timedelta(days=period_days)\n\n    # Query workout logs\n    workouts = await supabase.from_(\"workout_logs\") \\\n        .select(\"*\") \\\n        .eq(\"user_id\", user_id) \\\n        .gte(\"workout_date\", cutoff.isoformat()) \\\n        .order(\"workout_date\", desc=True) \\\n        .execute()\n\n    # Query race results\n    races = await supabase.from_(\"race_results\") \\\n        .select(\"*\") \\\n        .eq(\"user_id\", user_id) \\\n        .gte(\"race_date\", cutoff.isoformat()) \\\n        .execute()\n\n    # Compute summary statistics\n    total_workouts = len(workouts.data)\n    completed = sum(1 for w in workouts.data if w[\"completed\"])\n    completion_rate = (completed / total_workouts * 100) if total_workouts > 0 else 0\n    avg_rpe = sum(w[\"rpe\"] for w in workouts.data if w.get(\"rpe\")) / \\\n              max(1, sum(1 for w in workouts.data if w.get(\"rpe\")))\n\n    workout_types = {}\n    for w in workouts.data:\n        wt = w[\"workout_type\"]\n        workout_types[wt] = workout_types.get(wt, 0) + 1\n\n    analysis_context = f\"\"\"## Training Data Analysis ({period_days} days)\n\n**Summary:**\n- Total workouts logged: {total_workouts}\n- Completed: {completed} ({completion_rate:.0f}%)\n- Average RPE: {avg_rpe:.1f}/10\n- Workout type distribution: {json.dumps(workout_types)}\n\n**Recent workouts (last 5):**\n\"\"\"\n    for w in workouts.data[:5]:\n        analysis_context += f\"- {w['workout_date']}: {w['title']} ({w['workout_type']}) \"\n        analysis_context += f\"\u2014 {'\u2705 completed' if w['completed'] else '\u274c incomplete'}\"\n        if w.get(\"rpe\"):\n            analysis_context += f\" \u2014 RPE {w['rpe']}/10\"\n        analysis_context += \"\\n\"\n\n    if races.data:\n        analysis_context += \"\\n**Race Results:**\\n\"\n        for r in races.data:\n            analysis_context += f\"- {r['race_date']} ({r['location']}): {r['overall_time']} \"\n            analysis_context += f\"({r['division']})\\n\"\n            if r.get(\"station_times\"):\n                analysis_context += f\"  Station times: {json.dumps(r['station_times'])}\\n\"\n\n    return analysis_context\n```\n\n---\n\n## 6. Training Data Collection for Fine-Tuning\n\n### 6.1 Structuring Conversation Logs\n\nEvery conversation is automatically structured for future LoRA training. The `conversation_messages` table already captures the fields Fireworks AI needs.\n\n**Export script to JSONL:**\n\n```python\n# export_training_data.py\nimport json\nfrom supabase import create_client\n\nsupabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)\n\ndef export_training_data(\n    min_rating: int = 4,          # Only high-quality conversations\n    min_messages: int = 4,         # At least 2 full turns\n    output_path: str = \"training_data.jsonl\"\n):\n    \"\"\"Export high-quality conversations as JSONL for Fireworks LoRA training.\"\"\"\n\n    # Fetch sessions with positive feedback\n    sessions = supabase.from_(\"conversation_sessions\") \\\n        .select(\"id, summary\") \\\n        .gte(\"message_count\", min_messages) \\\n        .execute()\n\n    exported = 0\n    with open(output_path, \"w\") as f:\n        for session in sessions.data:\n            # Get messages for this session\n            messages = supabase.from_(\"conversation_messages\") \\\n                .select(\"role, content, reasoning_content, tool_calls, user_rating, was_helpful\") \\\n                .eq(\"session_id\", session[\"id\"]) \\\n                .order(\"created_at\") \\\n                .execute()\n\n            msgs = messages.data\n            if not msgs:\n                continue\n\n            # Filter: only export if average rating >= min_rating\n            ratings = [m[\"user_rating\"] for m in msgs if m.get(\"user_rating\")]\n            if ratings and (sum(ratings) / len(ratings)) < min_rating:\n                continue\n\n            # Check for follow-through signal\n            has_follow_through = any(m.get(\"follow_through\") for m in msgs)\n\n            # Build JSONL training example\n            training_messages = [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n            ]\n\n            for msg in msgs:\n                if msg[\"role\"] == \"system\":\n                    continue  # Skip injected system messages\n\n                entry = {\"role",
          "type": "output_text"
        }
      ],
      "id": "msg_ddc1a5cc-3d55-42a9-a078-441da65f2f02",
      "role": "assistant",
      "status": "completed",
      "type": "message"
    }
  ],
  "status": "completed",
  "usage": {
    "cost": {
      "currency": "USD",
      "input_cost": 0.23594,
      "output_cost": 0.4096,
      "tool_calls_cost": 0.0315,
      "total_cost": 0.67704
    },
    "input_tokens": 47188,
    "output_tokens": 16384,
    "tool_calls_details": {
      "fetch_url": {
        "invocation": 3
      },
      "search_web": {
        "invocation": 6
      }
    },
    "total_tokens": 63572
  }
}