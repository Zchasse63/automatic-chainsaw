{
  "created_at": 1771015246,
  "id": "resp_8c10062d-f4a5-493e-95e4-3a58bc2293e4",
  "model": "anthropic/claude-opus-4-6",
  "object": "response",
  "output": [
    {
      "content": [
        {
          "text": "\n\n# Serverless LoRA Fine-Tuning & Inference: Platform Comparison (Mid-2025)\n\n## Executive Summary\n\nYou trained a LoRA on Kimi K2 Thinking via Fireworks AI and discovered it can't be served serverlessly \u2014 it requires dedicated deployment on 32\u00d7 H100s. This analysis identifies which platforms and models actually support the full end-to-end workflow: **LoRA fine-tuning \u2192 serverless inference** (no dedicated GPU required).\n\n**The short answer: Together AI is the only platform with a mature, documented serverless multi-LoRA inference system for large models. Fireworks AI's serverless LoRA is currently suspended. Nebius, Modal, and Replicate don't offer comparable serverless LoRA text inference.**\n\n---\n\n## Platform-by-Platform Analysis\n\n### 1. Together AI \u2705 \u2014 Best Option\n\nTogether AI is the clear leader here. They explicitly launched \"Serverless Multi-LoRA\" as a product, with dedicated documentation, and a specific list of supported base models.\n\n#### Models Supporting BOTH Fine-Tuning AND Serverless LoRA Inference\n\nThe serverless LoRA base model list (from their adapter upload docs) is **distinct from and smaller than** their fine-tuning models list. Only these base models support serverless LoRA serving:\n\n| Base Model | API String | Quantization | Size |\n|---|---|---|---|\n| **Llama 4 Maverick Instruct** | `meta-llama/Llama-4-Maverick-17B-128E-Instruct` | FP8 | 17B\u00d7128E MoE (~400B total) |\n| **Llama 3.1 8B Instruct** | `meta-llama/Meta-Llama-3.1-8B-Instruct-Reference` | BF16 | 8B |\n| **Llama 3.1 70B Instruct** | `meta-llama/Meta-Llama-3.1-70B-Instruct-Reference` | BF16 | 70B |\n| **Qwen 2.5 72B Instruct** | `Qwen/Qwen2.5-72B-Instruct` | FP8 | 72B |\n| **Qwen3 235B A22B Instruct 2507** | `Qwen/Qwen3-235B-A22B-Instruct-2507-tput` | FP8 | 235B MoE (22B active) |\n\n> \u26a0\ufe0f **Critical caveat**: You must use the `-Reference` variants (not `-Turbo`) for LoRA fine-tuning. Using Turbo variants will cause a \"No lora_model specified\" error.\n\n#### Maximum Model Size for Serverless LoRA\n- **Qwen3-235B-A22B** (235B total, 22B active parameters) \u2014 the largest supported\n- **Llama 4 Maverick** (~400B total parameters as MoE) \u2014 also massive but only 17B active per expert\n\n#### Pricing\n\n**Fine-tuning (LoRA SFT):**\n\n| Model Size | LoRA SFT per 1M tokens | LoRA DPO per 1M tokens |\n|---|---|---|\n| Up to 16B | $0.48 | $1.20 |\n| 17B\u201369B | $1.50 | $3.75 |\n| 70B\u2013100B | $2.90 | $7.25 |\n| Qwen3-235B-A22B | $6.00 | $15.00 |\n\n**Serverless Inference (LoRA adapters served at base model price):**\n\n| Model | Input / 1M tokens | Output / 1M tokens |\n|---|---|---|\n| Llama 3.1 8B | $0.18 | $0.18 |\n| Llama 3.1 70B | $0.88 | $0.88 |\n| Llama 3.3 70B Turbo* | $0.88 | $0.88 |\n| Llama 4 Maverick | $0.27 | $0.85 |\n| Qwen 2.5 72B Instruct | ~$0.30** | ~$0.30** |\n| Qwen3 235B A22B Instruct 2507 | $0.20 | $0.60 |\n\n*\\*Turbo is the serverless chat model but NOT the LoRA base \u2014 you fine-tune on Reference but inference pricing matches the serverless tier.*\n*\\*\\*Qwen 2.5 72B pricing not explicitly listed on the pricing page snippet; estimate based on tier.*\n\n**Key cost insight**: LoRA inference is billed at the **same per-token price as the base model**. No additional hosting fee. No idle cost.\n\n#### Your 729 Training Examples \u2014 Cost Estimate\n\nAssuming ~500 tokens per example average, that's ~365K tokens per epoch. With 3\u20135 epochs:\n- **Llama 3.1 70B LoRA SFT**: ~1.1M\u20131.8M tokens \u00d7 $2.90/1M = **$3.19\u2013$5.22**\n- **Qwen3-235B-A22B LoRA SFT**: ~1.1M\u20131.8M tokens \u00d7 $6.00/1M = **$6.60\u2013$10.80**\n\nThese are extremely affordable fine-tuning costs.\n\n#### Features\n- \u2705 OpenAI-compatible chat completions API\n- \u2705 Upload adapters from HuggingFace or S3\n- \u2705 Streaming supported\n- \u2705 Multi-LoRA (hundreds of adapters on one base model)\n- \u2705 Cross-LoRA continuous batching, adapter prefetching\n- \u2705 FP8 Turbo + speculative decoding for speed\n- \u26a0\ufe0f Cold start latency: 5\u201310 seconds on first request; sub-second thereafter\n- \u26a0\ufe0f Pre-Dec 2024 adapters require re-upload/migration\n\n#### Limitations\n- The serverless LoRA base model list is **small** (only 5 models currently)\n- **DeepSeek R1/V3 are NOT on the serverless LoRA list** (they support LoRA fine-tuning but require dedicated endpoints for inference)\n- **Kimi K2 is NOT on the serverless LoRA list** (same situation)\n- No Mistral Large, no Llama 3.1 405B on serverless LoRA\n- Maintains ~90% of base model throughput (10% overhead from runtime adapter loading)\n\n---\n\n### 2. Fireworks AI \u26a0\ufe0f \u2014 Serverless LoRA Currently Suspended\n\n#### Status\nFireworks conceptually supports serverless LoRA \u2014 their docs mention \"LoRA addons must be deployed on a **serverless or dedicated** deployment\" and the Concepts page references \"serverless LoRA addons.\" However:\n\n> **\u26a0\ufe0f \"Serverless LoRA is not currently supported (February 2026). Please check our Documentation Page for the latest updates.\"** \u2014 Fireworks AI blog\n\nThe updated model overview page now states: *\"LoRA addons must be deployed on a **dedicated** deployment\"* \u2014 the word \"serverless\" has been removed.\n\n#### What This Means\n- You **can** fine-tune LoRA on Fireworks (including Kimi K2 Thinking, DeepSeek R1/V3, etc.)\n- You **cannot** serve that LoRA serverlessly \u2014 you must spin up a dedicated on-demand deployment\n- For Kimi K2 Thinking, that means paying for 32\u00d7 H100s at $4.00/hr each = **$128/hour**\n\n#### Pricing (For Reference)\n\n**Fine-tuning:**\n\n| Base Model Size | LoRA SFT / 1M tokens | LoRA DPO / 1M tokens |\n|---|---|---|\n| Up to 16B | $0.50 | $1.00 |\n| 16.1B\u201380B | $3.00 | $6.00 |\n| 80B\u2013300B | $6.00 | $12.00 |\n| >300B (DeepSeek V3, Kimi K2) | $10.00 | $20.00 |\n\n**Serverless Inference (base models only, no LoRA):**\n\n| Model | Price / 1M tokens |\n|---|---|\n| <4B params | $0.10 |\n| 4B\u201316B params | $0.20 |\n| >16B params | $0.90 |\n| DeepSeek V3 family | $0.56 input / $1.68 output |\n| Kimi K2 | $0.60 input / $2.50 output |\n\n**Dedicated GPU Deployments (required for LoRA):**\n\n| GPU | $/hour |\n|---|---|\n| A100 80GB | $2.90 |\n| H100 80GB | $4.00 |\n| H200 141GB | $6.00 |\n| B200 180GB | $9.00 |\n\n#### Features\n- \u2705 OpenAI-compatible API\n- \u2705 Up to 100 LoRA adapters per dedicated deployment\n- \u2705 LoRA and qLoRA support\n- \u2705 Extensive model library for fine-tuning\n- \u274c **No serverless LoRA inference currently**\n- \u274c Dedicated deployments for large models are very expensive\n\n---\n\n### 3. Nebius AI Studio \u26a0\ufe0f \u2014 LoRA Fine-Tuning Yes, Serverless LoRA Inference Unclear\n\n#### Status\nNebius offers LoRA fine-tuning via Token Factory and mentions \"auto-deploy serverlessly on our infrastructure.\" However, the documentation is vague about whether fine-tuned LoRA adapters are served serverlessly with dynamic adapter swapping (like Together AI) or simply deployed as a new serverless endpoint.\n\n#### What's Available\n- **30+ models** for fine-tuning including Llama 3 series (1B\u201370B), Qwen series (1.5B\u201372B), DeepSeek Coder\n- LoRA supported on all models; full fine-tuning only for models under 20B\n- OpenAI-compatible API\n- Token-based pricing (specific rates not clearly documented in gathered data)\n- One-click deployment of fine-tuned models\n\n#### Limitations\n- **No documented serverless multi-LoRA architecture** (no equivalent of Together's cross-LoRA batching)\n- **No specific list of \"serverless LoRA-compatible base models\"** published\n- Less transparent pricing compared to Together AI and Fireworks\n- Smaller ecosystem and community compared to the other two\n\n#### Verdict\nNebius may work for your use case but the lack of clear documentation about serverless LoRA inference makes it a riskier choice. You'd need to contact their team to confirm the exact workflow and pricing.\n\n---\n\n### 4. Modal \u274c \u2014 Not Applicable\n\nModal is an infrastructure platform for running custom code on GPUs. The research found a tutorial about loading LoRA adapters with Stable Diffusion XL for image generation \u2014 this is a **DIY approach**, not managed serverless LoRA inference. You would need to:\n- Write your own serving code\n- Manage GPU allocation and scaling\n- Handle adapter loading yourself\n\nNot a viable alternative for your use case.\n\n---\n\n### 5. Replicate \u274c \u2014 Image LoRA Only\n\nReplicate's LoRA support is focused on **image generation models** (primarily Flux fine-tunes). The documentation discusses combining multiple LoRA styles for image generation. There is no evidence of serverless LoRA support for large language models in the research data.\n\n---\n\n## Comparison Matrix: 70B+ Models for Serverless LoRA\n\n| Model | Together AI | Fireworks AI | Nebius |\n|---|---|---|---|\n| **Llama 3.1 70B Instruct** | \u2705 Serverless LoRA | \u274c Dedicated only | \u2753 Unclear |\n| **Llama 3.1 405B** | \u274c Not on LoRA list | \u274c Dedicated only | \u274c Not mentioned |\n| **Llama 4 Maverick (~400B MoE)** | \u2705 Serverless LoRA | \u274c Dedicated only | \u274c Not mentioned |\n| **DeepSeek R1/V3** | \u274c Fine-tune yes, serverless LoRA no | \u274c Dedicated only | \u274c Not mentioned |\n| **Qwen 2.5 72B Instruct** | \u2705 Serverless LoRA | \u274c Dedicated only | \u2753 Unclear |\n| **Qwen3 235B A22B** | \u2705 Serverless LoRA | \u274c Dedicated only | \u274c Not mentioned |\n| **Kimi K2 Thinking** | \u274c Fine-tune yes, serverless LoRA no | \u274c Dedicated only | \u274c Not available |\n| **Mistral Large/Mixtral** | \u274c Not on LoRA list | \u274c Dedicated only | \u274c Not mentioned |\n\n---\n\n## \ud83c\udfaf Recommendation\n\n### Best Option: **Together AI + Qwen3-235B-A22B-Instruct-2507**\n\n**Why this combination:**\n\n1. **Largest reasoning model with serverless LoRA support**: At 235B total parameters (22B active), Qwen3-235B is the most capable model on Together's serverless LoRA list \u2014 significantly more powerful than Llama 3.1 70B, and a genuine reasoning model.\n\n2. **Extremely affordable for your dataset**: 729 examples \u00d7 ~500 tokens \u00d7 5 epochs = ~1.8M training tokens \u00d7 $6.00/1M = **~$10.80 to fine-tune**. Even if you iterate 5 times, that's only ~$54.\n\n3. **Cheapest inference for a model this size**: $0.20 input / $0.60 output per 1M tokens \u2014 cheaper than DeepSeek V3 on any platform, and you get your LoRA customization on top at **no additional cost**.\n\n4. **Zero idle cost**: Pay-per-token only. No GPU rental when not in use.\n\n5. **Immediate deployment**: Fine-tuning completes \u2192 model is instantly available for inference. No deployment step.\n\n6. **OpenAI-compatible API**: Drop-in replacement for your existing code.\n\n### Runner-Up: **Together AI + Llama 3.1 70B Instruct**\n\nIf you want the most battle-tested, widely-used model:\n- Fine-tuning: ~$5 for your dataset\n- Inference: $0.88/1M tokens (input and output)\n- Maximum ecosystem support and community knowledge\n- Proven LoRA fine-tuning results at 70B scale\n\n### Runner-Up: **Together AI + Llama 4 Maverick**\n\nIf you want Meta's latest MoE architecture:\n- Very cheap inference: $0.27 input / $0.85 output per 1M tokens\n- 1M+ context length\n- Newest model on the list\n- Less community fine-tuning data available\n\n### What to Do About Your Kimi K2 LoRA\n\nUnfortunately, your existing Kimi K2 Thinking LoRA trained on Fireworks **cannot be transferred** to Together AI \u2014 Kimi K2 is not on Together's serverless LoRA list either. You will need to **retrain on one of Together's supported base models**. Given your small dataset (729 examples), this is a <$15 expense and likely under an hour of training time.\n\n### Action Plan\n\n1. **Format your 729 JSONL examples** for the target model's chat template (Qwen3 or Llama 3.1 format)\n2. **Upload to Together AI**: `together files upload your-data.jsonl`\n3. **Launch LoRA fine-tuning**: `together fine-tuning create --training-file <file-id> --model Qwen/Qwen3-235B-A22B-Instruct-2507 --lora`\n4. **Run inference immediately** when training completes \u2014 no deployment needed\n5. **Send a warmup query** to minimize cold start latency for production use",
          "type": "output_text"
        }
      ],
      "id": "msg_943df6bf-ac2c-43d1-b876-fc2a7ac7f6eb",
      "role": "assistant",
      "status": "completed",
      "type": "message"
    }
  ],
  "status": "completed",
  "usage": {
    "cost": {
      "currency": "USD",
      "input_cost": 0.19534,
      "output_cost": 0.10093,
      "total_cost": 0.29627
    },
    "input_tokens": 39068,
    "output_tokens": 4037,
    "total_tokens": 43105
  }
}